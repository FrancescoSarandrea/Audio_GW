{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7942364c-9c1a-41a1-afdf-11256d5122c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a25abe3-e6ff-4b11-9c98-235121abbca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "import zipfile\n",
    "\n",
    "import math\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "import h5py as h5\n",
    "import os\n",
    "import time\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from PIL import Image\n",
    "from matplotlib import gridspec\n",
    "\n",
    "from torch import optim\n",
    "from scipy.stats import pearsonr\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b086ecaf-e191-415e-8f7e-6c4960cfdb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torch_smoothing_spline_interpolation import smoothing_spline_interpolate\n",
    "#from torch_smoothing_spline_interpolation import *\n",
    "from torch_spline_interpolation_beta import SplineInterpolate1D, SplineInterpolate2D\n",
    "from qtransform_beta import SingleQTransform\n",
    "from qtransform_beta import QScan\n",
    "#from ml4gw_qtransform import QScan, SingleQTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e35e73d7-991b-45cc-a465-a6a3efac52c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import gwpy\n",
    "    from gwpy.timeseries import TimeSeries\n",
    "except ModuleNotFoundError:\n",
    "    !pip install --quiet gwpy\n",
    "    import gwpy\n",
    "    from gwpy.timeseries import TimeSeries\n",
    "    \n",
    "from gwpy.signal import filter_design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5411c7ed-08c6-49e6-8037-34d92239905b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla T4\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    print(torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5b6e31d-d25a-40b1-9a0c-ee0c3cc4ee3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_timeseries(rows, columns, duration=10, sample_rate=500, num_waves_range=(10, 15), noise_amplitude=0.1):\n",
    "    \"\"\"\n",
    "    Generate a Pandas DataFrame with randomly generated smooth sine wave time series with added smooth random noise.\n",
    "    \n",
    "    Parameters:\n",
    "        - rows (int): Number of rows in the DataFrame.\n",
    "        - columns (int): Number of columns in the DataFrame.\n",
    "        - duration (float): Duration of the time series data in seconds (default is 6 seconds).\n",
    "        - sample_rate (int): Sampling rate of the time series data in samples per second (default is 500 samples per second).\n",
    "        - num_waves_range (tuple): Range for the random number of sine waves to be generated for each time series.\n",
    "                                   Format: (min_num_waves, max_num_waves) (default is (10, 15)).\n",
    "        - noise_amplitude (float): Amplitude of the smooth random noise added to the time series data (default is 0.1).\n",
    "    \n",
    "    Returns:\n",
    "        - DataFrame: Pandas DataFrame containing the generated time series data.\n",
    "    \"\"\"\n",
    "  \n",
    "   \n",
    "    \n",
    "    # Initialize an empty list to store individual DataFrames for each row\n",
    "    dfs = []\n",
    "    columns_list= [f'Channel_{i+1}' for i in range(columns)]\n",
    "    \n",
    "    # Generate time array\n",
    "    times = np.linspace(0, duration, duration * sample_rate)\n",
    "    \n",
    "    mu = duration /2\n",
    "    sigma = duration /4\n",
    "    \n",
    "    frequency0 = 20\n",
    "    \n",
    "    #*\n",
    "    \n",
    "    for index in range(rows):\n",
    "        df_dict={}\n",
    "        for col in range(columns):\n",
    "            # Initialize an array to store the generated wave data for this row\n",
    "            wave_data = np.zeros(len(times))\n",
    "            # Determine the number of sine waves to generate for this column randomly\n",
    "            num_waves = np.random.randint(*num_waves_range)\n",
    "            \n",
    "            # Generate each sine wave\n",
    "            for _ in range(num_waves):\n",
    "                           \n",
    "      \n",
    "    \n",
    "                #sigma = duration /np.random.uniform(1, 10)  # Standard deviation for the Gaussian\n",
    "                  # Standard deviation for the Gaussian\n",
    "\n",
    "                #mu = duration /np.random.uniform(0.1, 5)     # Mean (center) of the Gaussian\n",
    "                     # Mean (center) of the Gaussian\n",
    "\n",
    "                # Gaussian envelope\n",
    "                #envelope = np.exp(-0.5 * ((times - mu) / sigma) ** 2)\n",
    "                #frequency0 = np.random.uniform(1, 5)\n",
    "                envelope=np.sin(2 * np.pi * frequency0 * times)*np.exp(-0.5 * ((times - mu) / sigma) ** 2)\n",
    "                \n",
    "                \n",
    "                # Randomly generate parameters for the sine wave (amplitude, frequency, phase)\n",
    "                amplitude = np.random.uniform(0.5, 2.0)\n",
    "                #frequency = np.random.uniform(90, 130)\n",
    "                frequency = np.random.uniform(10, 30)\n",
    "                #frequency=0.2\n",
    "                #frequency0 = np.random.uniform(0.1, 2)\n",
    "                #frequency=frequency0*times**1.5\n",
    "                phase = np.random.uniform(0, 2*np.pi)\n",
    "                \n",
    "                # Generate the sine wave and add it to the wave_data\n",
    "                wave_data += envelope* np.sin(2 * np.pi * frequency * times + phase)\n",
    "                #wave_data += amplitude * np.sin(2 * np.pi * frequency * times + phase)\n",
    "\n",
    "                \n",
    "            \n",
    "            # Add smooth random noise to the wave data\n",
    "            smooth_noise = np.random.normal(0, noise_amplitude, len(times))\n",
    "            wave_data += smooth_noise\n",
    "            wave_data=wave_data/max(abs(wave_data))\n",
    "        \n",
    "            # Create a TimeSeries object from the wave data\n",
    "            ts = TimeSeries(wave_data, t0=0, dt=1/sample_rate)\n",
    "            df_dict[f'Channel_{col+1}']=[ts]\n",
    "            \n",
    "        #Create a DataFrame with the TimeSeries\n",
    "        df_row = pd.DataFrame(df_dict)\n",
    "        \n",
    "        # Append the DataFrame to the list\n",
    "        dfs.append(df_row)\n",
    "    \n",
    "    # Concatenate the list of DataFrames into a single DataFrame along rows axis\n",
    "    df = pd.concat(dfs, ignore_index=True, axis=0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e676f27b-d4ee-45dc-a023-84c4beb382f5",
   "metadata": {},
   "source": [
    "Function to construct the dataframe with real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0e20d48-67dd-4c51-9cf0-e7eb72b1b266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_dataframe(path,channel_list=None ,target_channel='V1:Hrec_hoft_16384Hz',n1_events=None, n2_events=None,n1_channels=None,n2_channels=None,print_=True,sr=False,low_freq=4,high_freq=50):\n",
    "    \n",
    "    if not n1_events:\n",
    "        n1_events=0\n",
    "    if not n2_events:\n",
    "        n2_events=len(listdir(path))\n",
    "        \n",
    "    if n2_events>len(listdir(path)):\n",
    "        n2_events=len(listdir(path))\n",
    "    \n",
    "    #print(f'PATH: {path}')    \n",
    "    lstdr=listdir(path)[n1_events:n2_events]\n",
    "    #print(f'LIST DIR: {lstdr}')\n",
    "    sample_file=listdir(path)[0]\n",
    "    \n",
    "    files = [f for f in lstdr]\n",
    "    # print(files)\n",
    "    df_list = []\n",
    "    event_data = []\n",
    "    \n",
    "    \n",
    "    if not channel_list:\n",
    "        n_all_channels=0\n",
    "        all_channels=[]\n",
    "        with h5.File(os.path.join(path, sample_file), 'r') as fout:\n",
    "            event_id = list(fout.keys())[0]\n",
    "            all_channels=list(fout[event_id])\n",
    "            n_all_channels=len(list(fout[event_id]))\n",
    "            #print(f'event id: {event_id}')\n",
    "\n",
    "\n",
    "        if not n1_channels:\n",
    "            n1_channels=0\n",
    "        if not n2_channels:\n",
    "            n2_channels=n_all_channels\n",
    "\n",
    "        if n2_channels>n_all_channels:\n",
    "            n2_channels=n_all_channels\n",
    "\n",
    "        channels=all_channels[n1_channels:n2_channels]\n",
    "    else:\n",
    "        channels=channel_list\n",
    "    try:\n",
    "        channels.remove(target_channel)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    for i, file in enumerate(files):\n",
    "        if print_:\n",
    "            print(f\"Added {i + 1}/{n2_events - n1_events} files to dataframe\", end='\\r')\n",
    "       \n",
    "           \n",
    "        try:\n",
    "            \n",
    "            with h5.File(os.path.join(path, file), 'r') as fout:\n",
    "                #print('file successfully opened')\n",
    "                event_id = list(fout.keys())[0]\n",
    "                dictionary = {'Event ID': event_id}\n",
    "                #dictionary={}\n",
    "                event_data.append(event_id)\n",
    "                \n",
    "                \n",
    "                tmsrs = TimeSeries(fout[event_id][target_channel],dt=1.0/fout[event_id][target_channel].attrs['sample_rate']) #ToDo: add other attrs to the TimeSeries (seee h5_gwpy.ipynb for reference)\n",
    "                \n",
    "                bp = filter_design.bandpass(low_freq, high_freq, tmsrs.sample_rate)\n",
    "                tmsrs = tmsrs.filter(bp, filtfilt=True)\n",
    "                \n",
    "                tmsrs=tmsrs[int(5.5*4096):int(10.5*4096)]\n",
    "                if sr:\n",
    "                    tmsrs=tmsrs.resample(sr)\n",
    "                    tmsrs=TimeSeries(np.array(tmsrs)/max(np.array(tmsrs)), dt=1/sr)\n",
    "                dictionary[target_channel] = [tmsrs]\n",
    "                #print(f'DICT: {dictionary}')\n",
    "        \n",
    "                for i,channel in enumerate(channels):\n",
    "                    #print(f\"Added {i + 1}/{n2_channels - n1_channels} files to dataframe\", end='\\r')\n",
    "                    try:\n",
    "                        tmsrs = TimeSeries(fout[event_id][channel],dt=1.0/fout[event_id][channel].attrs['sample_rate'])\n",
    "                        tmsrs = tmsrs.filter(bp, filtfilt=True)\n",
    "                        tmsrs=tmsrs[int(5.5*4096):int(10.5*4096)]\n",
    "                        if sr:\n",
    "                            tmsrs=tmsrs.resample(sr)\n",
    "                            tmsrs=TimeSeries(np.array(tmsrs)/max(np.array(tmsrs)), dt=1/sr)\n",
    "                        dictionary[channel] = [tmsrs]\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        tmsrs=np.nan\n",
    "                        dictionary[channel] = [tmsrs]\n",
    "                        #print(f'error in making timeseries:')\n",
    "                        #print(e)\n",
    "                        \n",
    "                \n",
    "                df_list.append(pd.DataFrame(dictionary))\n",
    "                    \n",
    "\n",
    "                \n",
    "        except Exception as e:\n",
    "        \n",
    "            if print_:\n",
    "                print(f'COULD NOT OPEN {os.path.join(path, file)}')\n",
    "                print(e)\n",
    "            \n",
    "        \n",
    "        \n",
    "    #print(f'DF LIST: {df_list.shape}')\n",
    "    \n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "    \n",
    "    #df_ids = pd.DataFrame({'Event ID': event_data})\n",
    "    #df = pd.concat([df_ids, df], axis=1)\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c891c1b0-07d0-4c50-94a4-965e405c3d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function which transates the two curves to maximise their pearson correlation\n",
    "def maximise_pearson(curve1, curve2, trans_step=5):\n",
    "    max_corr=0\n",
    "    delta_max_corr=0\n",
    "    max_delta=int(len(curve1)/trans_step)\n",
    "    for delta in np.arange(0,max_delta, trans_step):\n",
    "        cv1=curve1[:len(curve1)-delta]\n",
    "        cv2=curve2[delta:]\n",
    "        pearson_corr, _ = pearsonr(cv1,cv2)\n",
    "        if np.abs(pearson_corr)>np.abs(max_corr):\n",
    "            max_corr=pearson_corr\n",
    "            delta_max_corr=delta\n",
    "    cv1=curve1[:len(curve1)-delta_max_corr]\n",
    "    if max_corr<0:\n",
    "        cv2=-curve2[delta_max_corr:]\n",
    "    else:\n",
    "        cv2=curve2[delta_max_corr:]\n",
    "    return cv1, cv2\n",
    "\n",
    "# functions to ensure the semi-positivity of tensors\n",
    "def eliminate_negatives(x):\n",
    "    if x<0:\n",
    "        return 0\n",
    "    else:\n",
    "        return x\n",
    "def make_semipositive(tens):\n",
    "    tens_list=[]\n",
    "    for l in tens:\n",
    "        bool_list=[eliminate_negatives(x) for x in list(l)]\n",
    "        bool_tens=torch.tensor(bool_list, dtype=torch.float64)\n",
    "        tens_list.append(bool_tens)\n",
    "    tens_semipositive=torch.stack(tens_list).to(device)\n",
    "    return tens_semipositive\n",
    "\n",
    "def process_in_batches(tensor, batch_size, transform_function):\n",
    "    # Determine the total number of batches\n",
    "    total_batches = tensor.size(0) // batch_size\n",
    "    if tensor.size(0) % batch_size != 0:\n",
    "        total_batches += 1\n",
    "\n",
    "    # Placeholder list for storing results\n",
    "    results = []\n",
    "\n",
    "    # Process each batch\n",
    "    for i in range(total_batches):\n",
    "        # Calculate the start and end index for the current batch\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min(start_idx + batch_size, tensor.size(0))\n",
    "\n",
    "        # Extract the batch\n",
    "        batch_tensor = tensor[start_idx:end_idx]\n",
    "\n",
    "        # Apply the transformation function (e.g., qtransform)\n",
    "        transformed_batch = transform_function(batch_tensor)\n",
    "        #.to(torch.float64)\n",
    "\n",
    "        # Append the result to the results list\n",
    "        results.append(transformed_batch)\n",
    "\n",
    "    # Concatenate all the results along the first dimension\n",
    "    return torch.cat(results, dim=0)\n",
    "\n",
    "def gradnorm_adjustment(model, losses, alphas, scaling_factor=1.0):\n",
    "    # Compute gradient norms for each loss component\n",
    "    grads = []\n",
    "    \n",
    "    for loss in losses:\n",
    "        # Make sure the loss is a scalar (in case it is a batch loss)\n",
    "        if loss.dim() > 0:\n",
    "            loss = loss.mean()  # or loss.sum()\n",
    "\n",
    "        # Compute gradients of the loss w.r.t model parameters\n",
    "        gradients = torch.autograd.grad(loss, model.parameters(), retain_graph=True, create_graph=True,allow_unused=True)\n",
    "\n",
    "        # Compute the gradient norm (L2 norm) across all parameters\n",
    "        grad_norm = torch.sqrt(sum(torch.norm(g)**2 for g in gradients if g is not None))\n",
    "        grads.append(grad_norm)\n",
    "\n",
    "    # Compute the target gradient norm (average of current norms)\n",
    "    avg_grad_norm = torch.mean(torch.stack(grads))\n",
    "\n",
    "    # Compute the scaling factor to match the gradient norms\n",
    "    scaling_factors = [scaling_factor * (avg_grad_norm / g) for g in grads]\n",
    "\n",
    "    # Adjust the alphas by the scaling factors\n",
    "    adjusted_alphas = [alpha * sf.item() for alpha, sf in zip(alphas, scaling_factors)]\n",
    "\n",
    "    return adjusted_alphas\n",
    "\n",
    "def pearson_corr_batch(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the pairwise Pearson correlation coefficient between two batches of 1D time series.\n",
    "    \n",
    "    Parameters:\n",
    "    - x: A tensor of shape (batch_size, seq_length) containing the first batch of time series.\n",
    "    - y: A tensor of shape (batch_size, seq_length) containing the second batch of time series.\n",
    "    \n",
    "    Returns:\n",
    "    - A tensor of shape (batch_size,) containing the Pearson correlation coefficient for each pair.\n",
    "    \"\"\"\n",
    "    # Ensure the input tensors have the same shape\n",
    "    assert x.shape == y.shape, \"The two input tensors must have the same shape.\"\n",
    "    \n",
    "    # Compute the mean of each batch along the time dimension\n",
    "    mean_x = torch.mean(x, dim=1, keepdim=True)\n",
    "    mean_y = torch.mean(y, dim=1, keepdim=True)\n",
    "    \n",
    "    # Center the data by subtracting the mean\n",
    "    x_centered = x - mean_x\n",
    "    y_centered = y - mean_y\n",
    "    \n",
    "    # Compute the numerator (covariance) for each pair\n",
    "    covariance = torch.sum(x_centered * y_centered, dim=1)\n",
    "    \n",
    "    # Compute the denominator (standard deviation of each batch)\n",
    "    std_x = torch.sqrt(torch.sum(x_centered ** 2, dim=1))\n",
    "    std_y = torch.sqrt(torch.sum(y_centered ** 2, dim=1))\n",
    "    \n",
    "    # Compute Pearson correlation for each pair\n",
    "    correlation = covariance / (std_x * std_y)\n",
    "    \n",
    "    return correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d98c2c-c7af-4081-944e-3fc52c0486e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "Define the Single-Q Transform and extract only its Q-Tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "72865d43-d627-4b72-bb69-e142ea4338f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "qtransform = SingleQTransform(sample_rate=125, duration=5, q=5, frange=[5,40],spectrogram_shape=(70,250),qtiles_only=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6e3c14-6386-4faf-b324-ebac523a897c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#qtransform=QScan(sample_rate=500, duration=5, qrange=[5,30], frange=[5,40],spectrogram_shape=(70,250)).to(device)\n",
    "\n",
    "#qtransform = SingleQTransform(sample_rate=500, duration=5, q=5, frange=[5,40],spectrogram_shape=(70,250)).to(device)\n",
    "#qtransform = SingleQTransform(sample_rate=613, duration=10, q=5, frange=[5,40],spectrogram_shape=(250,48)).to(device)\n",
    "#qtransform = SingleQTransform(sample_rate=575, duration=10, q=5, frange=[5,40],spectrogram_shape=(1000,90)).to(device)\n",
    "\n",
    "#qtransform = SingleQTransform(duration=10, sample_rate=512, spectrogram_shape=(250,48), q=5, frange=[4,100]).to(device)\n",
    "#qtransform = SingleQTransform(duration=10, sample_rate=512, spectrogram_shape=(1000,190), q=5, frange=[5,50]).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7a4cc4-9131-4f88-89c7-5b62b55b307c",
   "metadata": {},
   "source": [
    "Run this cell if you want to construct a dataset with real glitches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "209c3634-10b4-4175-889f-dc75eb5ff663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COULD NOT OPEN /data/datasets/intertwin-dati-aux/gspy_O3a_Scattered_Light_V1/Shift_VS_Correlation.png\n",
      "Unable to synchronously open file (file signature not found)\n",
      "162.4858009815216iles to dataframe\n"
     ]
    }
   ],
   "source": [
    "path='/data/datasets/intertwin-dati-aux/gspy_O3a_Scattered_Light_V1'\n",
    "start_time=time.time()\n",
    "df_tms = construct_dataframe(path,target_channel='V1:Hrec_hoft_16384Hz', n1_events=0, n2_events=8000,channel_list=['V1:Hrec_hoft_16384Hz'],sr=125,low_freq=5,high_freq=40)\n",
    "print(time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971c7cf1-dc63-489c-994b-f685e1c7fdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tms=generate_dataset_timeseries(10, 1, duration=5, sample_rate=500, num_waves_range=(1,2), noise_amplitude=0.1)\n",
    "#df_qts=df_tms.apply(lambda x: x[0].q_transform(qrange=(5,5),frange=(5,50)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac63b972-3718-4f10-9c9f-f4c6336cc5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tms.to_pickle('DF_Training_SpecInv_SL_TimeSeries_8000.pkl')\n",
    "#df_qts.to_pickle('DF_Training_SpecInv_Spectrograms_10000.pkl')                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a748bbde-0e56-4411-8efb-e929a72feb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tms=pd.read_pickle('DF_Training_SpecInv_SL_TimeSeries_8000.pkl')\n",
    "#df_qts=pd.read_pickle('DF_Training_SpecInv_Spectrograms_10000.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "bd10193b-9d2e-4e40-8983-7ecc5f3286b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_list=[]\n",
    "for index, row in df_tms.iterrows():\n",
    "    # Extract the TimeSeries object\n",
    "    #timeseries = row['Channel_1']\n",
    "\n",
    "    timeseries = row['V1:Hrec_hoft_16384Hz']\n",
    "    \n",
    "    # Convert TimeSeries to numpy array\n",
    "    timeseries_array = timeseries.value  # .value gives the numpy array of TimeSeries data\n",
    "    \n",
    "    # Convert numpy array to PyTorch tensor\n",
    "    timeseries_tensor = torch.tensor(timeseries_array, dtype=torch.float64)\n",
    "    \n",
    "    # Append tensor to the list\n",
    "    tensor_list.append(timeseries_tensor)\n",
    "\n",
    "# If needed, you can stack these tensors into a single tensor\n",
    "# This will create a 2D tensor where each row corresponds to one TimeSeries\n",
    "y_train_tensor = torch.stack(tensor_list).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71a8fd1e-5c68-404d-93bb-6ea3634799f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7999, 2500])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "287ca782-7aeb-4e17-8217-543fa9686fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor=qtransform(y_train_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e641245e-777c-4d5b-8536-d73d00d4d228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7999, 11, 512])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f69942-4739-4fdb-a769-beb5bfbc8154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor_list=[]\n",
    "# n_inputs=X_train_tensor[0].shape[1]\n",
    "# for i in range(n_inputs):\n",
    "#     unpadded=[qt[:,i,:].unsqueeze(0) for qt in X_train_tensor]\n",
    "#     max_size = max(tensor.size(-1) for tensor in unpadded)\n",
    "#     padded=[]\n",
    "#     for qt in unpadded:\n",
    "#         length=qt.shape[-1]\n",
    "#         repeat_factor=int(max_size/length)    \n",
    "#         qt_longer=qt.repeat_interleave(repeat_factor, dim=-1)\n",
    "#         padded.append(qt_longer)\n",
    "#     padded_tensor=torch.stack(padded).squeeze(1,2)\n",
    "#     tensor_list.append(padded_tensor)\n",
    "#     padded_tensor=torch.stack(tensor_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5be13e-7ed2-4bca-b077-50cf0033a2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Define the function to repeat elements in the last dimension\n",
    "# def repeat_elements(tensor, repeat_factor):\n",
    "#     # Repeat elements in the last dimension consecutively\n",
    "#     repeated_tensor = tensor.repeat_interleave(repeat_factor, dim=-1)\n",
    "#     return repeated_tensor\n",
    "\n",
    "# # Step 2: Create padded tensors by repeating elements\n",
    "# repeated_tensors = []\n",
    "\n",
    "# for tensor in X_train_tensor:\n",
    "#     if tensor.size(-1) == 128:\n",
    "#         repeated_tensors.append(repeat_elements(tensor, 4))  # 128 -> 512 by repeating each element 4 times\n",
    "#     elif tensor.size(-1) == 256:\n",
    "#         repeated_tensors.append(repeat_elements(tensor, 2))  # 256 -> 512 by repeating each element 2 times\n",
    "#     else:\n",
    "#         repeated_tensors.append(tensor)  # 512 -> 512 (no change)\n",
    "\n",
    "# # Step 3: Concatenate along the first dimension\n",
    "# # Resulting shape: (11, batch_size, 512)\n",
    "# final_tensor = torch.cat(repeated_tensors, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef27a28-0e60-4069-a34f-73274be3fdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "qtiles=qtransform(y_train_tensor)\n",
    "n_inputs=qtiles[0].shape[1]\n",
    "padded_list=[]\n",
    "for i in range(n_inputs):\n",
    "    unpadded=[qt[:,i,:].unsqueeze(0)for qt in qtiles]\n",
    "    # Find the maximum size along the last dimension\n",
    "    max_size = max(tensor.size(-1) for tensor in unpadded)\n",
    "\n",
    "    # Pad tensors to the max_size along the last dimension and concatenate\n",
    "    padded_qtiles = [torch.nn.functional.pad(tensor, (0, max_size - tensor.size(-1))) for tensor in unpadded]\n",
    "    padded_list.append(torch.cat(padded_qtiles, dim=0).squeeze(1))\n",
    "X_train_tensor=torch.stack(padded_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc55dec4-151d-4546-96f9-c49853795825",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100  # Adjust this based on your GPU memory limits\n",
    "\n",
    "# Apply the function to tensor\n",
    "X_train_tensor = process_in_batches(y_train_tensor, batch_size, qtransform).to(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b096e613-7855-473a-a04c-add19b42d50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_tensor=df_qts.apply(lambda x: torch.tensor(x,dtype=torch.float64))\n",
    "#y_train_tensor = torch.tensor(df_tms.apply(lambda x: np.array(x[0]), axis=1),dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882d90dd-428f-4f14-9a8b-a6a91d9fe65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor=torch.stack(list(X_train_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171af275-755b-4ee1-ba5e-4b638ab17873",
   "metadata": {},
   "source": [
    "Normalise the spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7cdbb0-ea38-4a68-b151-c47ed387bb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_specs=X_train_tensor.shape[0]\n",
    "# Find the maximum value for each spectrogram (along the last two dimensions)\n",
    "max_vals = X_train_tensor.view(n_specs, -1).max(dim=1, keepdim=True)[0].view(-1, 1, 1)\n",
    "# Avoid division by zero by setting any 0 max values to 1\n",
    "max_vals[max_vals == 0] = 1\n",
    "\n",
    "# Normalize each spectrogram by its own maximum value\n",
    "X_train_tensor= X_train_tensor / max_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fe6a79-e609-4ecc-bd2b-919b5f0e4507",
   "metadata": {},
   "source": [
    "Cutting the beginning and the end of the glitches, delta_cut is a quantity in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef727b7-80b1-4a8f-bc34-67b5cac6b5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_cut=1\n",
    "sampling_rate=500\n",
    "duration=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200fcc2a-2c3a-4b3f-a11e-069126955154",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor=X_train_tensor[:,:,delta_cut*50:250-delta_cut*50]\n",
    "y_train_tensor=y_train_tensor[:, delta_cut*sampling_rate:(duration-delta_cut)*sampling_rate]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a712f1eb-f07e-4b18-824a-5140152deff7",
   "metadata": {},
   "source": [
    "Only run this cell if you are using the CNN or the ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd7d8cd-5c90-4cd2-a69d-5864245adfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "N, H, W= X_train_tensor.shape\n",
    "X_train_tensor = X_train_tensor.view(N, 1, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d0d458-94e4-444c-b407-dc4e6f856387",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_tms=generate_dataset_timeseries(2000, 1, duration=5, sample_rate=500, num_waves_range=(1,2), noise_amplitude=0.1)\n",
    "#df_test_tms=generate_dataset_timeseries(1600, 1, duration=10, sample_rate=575, num_waves_range=(1,2), noise_amplitude=0.1)\n",
    "#df_test_qts=df_test_tms.apply(lambda x: x[0].q_transform(qrange=(5,5),frange=(5,50)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0b7d9088-703b-40c9-bab1-2d20f5662886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 1600/1600 files to dataframe\r"
     ]
    }
   ],
   "source": [
    "df_test_tms = construct_dataframe(path,target_channel='V1:Hrec_hoft_16384Hz', n1_events=8000, n2_events=9600,channel_list=['V1:Hrec_hoft_16384Hz'],sr=125, low_freq=4,high_freq=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1de916c-f9a3-41ab-bb7c-f31839009ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_tms.to_pickle('DF_Test_SpecInv_SL_TimeSeries_1600.pkl')\n",
    "#df_test_qts.to_pickle('DF_Test_SpecInv_Spectrograms_1000.pkl')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "146bbf2a-9995-4cd5-81db-090f5a3dc726",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_tms=pd.read_pickle('DF_Test_SpecInv_SL_TimeSeries_1600.pkl')\n",
    "#df_test_qts=pd.read_pickle('DF_Test_SpecInv_Spectrograms_1000.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f83a84c-d934-4520-acef-0e36efb0ea10",
   "metadata": {},
   "source": [
    "Print some test timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47802e2-0952-4fb9-8ad4-5a851eac98ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    plt.plot(df_tms.iloc[i][0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01872193-ae07-45be-ae93-95b98068becc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(13):\n",
    "    plt.plot(df_test_tms.iloc[i][0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "75ca4546-23ba-4082-9ed6-e1e19e1c68e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_test_list=[]\n",
    "for index, row in df_test_tms.iterrows():\n",
    "    # Extract the TimeSeries object\n",
    "    #timeseries = row['Channel_1']\n",
    "    timeseries = row['V1:Hrec_hoft_16384Hz']\n",
    "    \n",
    "    # Convert TimeSeries to numpy array\n",
    "    timeseries_array = timeseries.value  # .value gives the numpy array of TimeSeries data\n",
    "    \n",
    "    # Convert numpy array to PyTorch tensor\n",
    "    timeseries_tensor = torch.tensor(timeseries_array, dtype=torch.float64)\n",
    "    \n",
    "    # Append tensor to the list\n",
    "    tensor_test_list.append(timeseries_tensor)\n",
    "\n",
    "# If needed, you can stack these tensors into a single tensor\n",
    "# This will create a 2D tensor where each row corresponds to one TimeSeries\n",
    "y_test_tensor = torch.stack(tensor_test_list).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5a09ef3a-8444-4e9e-a656-127b72744f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tensor=qtransform(y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de0b6e6-ee1f-4c83-95b7-958d1db68daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "qtiles=qtransform(y_test_tensor)\n",
    "n_inputs=qtiles[0].shape[1]\n",
    "padded_list=[]\n",
    "for i in range(n_inputs):\n",
    "    unpadded=[qt[:,i,:].unsqueeze(0)for qt in qtiles]\n",
    "    # Find the maximum size along the last dimension\n",
    "    max_size = max(tensor.size(-1) for tensor in unpadded)\n",
    "\n",
    "    # Pad tensors to the max_size along the last dimension and concatenate\n",
    "    padded_qtiles = [torch.nn.functional.pad(tensor, (0, max_size - tensor.size(-1))) for tensor in unpadded]\n",
    "    padded_list.append(torch.cat(padded_qtiles, dim=0).squeeze(1))\n",
    "X_test_tensor=torch.stack(padded_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a554dacd-1b89-4c0a-9567-472f880d6967",
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_size = 100  # Adjust this based on your GPU memory limits\n",
    "#X_test_tensor = process_in_batches(y_test_tensor, batch_size, qtransform).to(torch.float64)\n",
    "\n",
    "\n",
    "#X_test_tensor=torch.stack(list(X_test_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d76809e-4147-4ca9-91fa-6e97a0a85c3d",
   "metadata": {},
   "source": [
    "Normalise the spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249deeca-447b-4fe8-8033-bd336731fd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_specs=X_test_tensor.shape[0]\n",
    "# Find the maximum value for each spectrogram (along the last two dimensions)\n",
    "max_vals = X_test_tensor.view(n_specs, -1).max(dim=1, keepdim=True)[0].view(-1, 1, 1)\n",
    "# Avoid division by zero by setting any 0 max values to 1\n",
    "max_vals[max_vals == 0] = 1\n",
    "\n",
    "# Normalize each spectrogram by its own maximum value\n",
    "X_test_tensor= X_test_tensor / max_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92468096-bbb7-4d7f-be52-ef6730af23ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tensor=X_test_tensor[:,:,delta_cut*50:250-delta_cut*50]\n",
    "y_test_tensor=y_test_tensor[:, delta_cut*sampling_rate:(duration-delta_cut)*sampling_rate]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2be11ad-536f-4681-9dcb-83ebf5a4653b",
   "metadata": {},
   "source": [
    "Only run the cell below if you are using the CNN or the ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555a7b4e-a8a1-469e-95bf-d1c7754aee91",
   "metadata": {},
   "outputs": [],
   "source": [
    "N, H, W=X_test_tensor.shape\n",
    "X_test_tensor = X_test_tensor.view(N, 1, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2322d383-d97d-480b-b942-12dac6941d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram_train=X_test_tensor[0,0].detach().cpu().numpy().T\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(spectrogram_train.T, aspect='auto', origin='lower', cmap='viridis', vmin=0, vmax=10)\n",
    "#plt.colorbar(format='%+2.0f dB')\n",
    "plt.colorbar()\n",
    "plt.title('Spectrogram')\n",
    "plt.xlabel('Time bins')\n",
    "plt.ylabel('Frequency bins')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790c4b6d-310e-4650-a220-9b579933fe28",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b8b703-a7ab-4f76-9b4e-119087cfba6c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eed8643-d2f9-4666-8972-7fa0b64969b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += self.downsample(identity)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=5120):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self.make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self.make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self.make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self.make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride))\n",
    "        self.in_channels = out_channels\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(ResidualBlock, [2, 2, 2, 2])\n",
    "\n",
    "# Example usage\n",
    "# Assuming you have your spectrogram data stored in X_train and time series data stored in y_train\n",
    "\n",
    "# Preprocess spectrogram data if needed\n",
    "# Preprocess time series data if needed\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "# X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "# y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "# Create ResNet model\n",
    "model = ResNet18()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5977e2cc-fe37-42db-a432-5b672e9c5181",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf03ab6-070d-45cc-ba08-950410a5d9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Conv2DTo1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Conv2DTo1D, self).__init__()\n",
    "        \n",
    "        # Reduced number of filters\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=(3, 3), padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3, 3), padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), padding=1)\n",
    "        \n",
    "        # Fully connected layers with reduced size\n",
    "        self.fc1 = nn.Linear(32 * 31 * 6, 5000)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.constant_(m.weight.data, 1)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007bdb41-861e-4e4d-bf65-eefdf49cfa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Conv2DTo1D().cuda()\n",
    "model.apply(weights_init)\n",
    "model = model.double()  # Convert model parameters to float64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f8e1cf-9df5-4bc3-83d5-3d993804800e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## CNNLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b044c2f-4ad4-47cb-9f72-425315b2dca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "class CNNLSTM(nn.Module):\n",
    "    def __init__(self, input_channels=1, num_classes=5000, lstm_hidden_size=613):\n",
    "        super(CNNLSTM, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        #the number which is divided twice needs to be the time-size of the spectrogram\n",
    "        #self.lstm_input_size = 128 * (1000 // 2 // 2)  # Considering the pooling layers\n",
    "        self.lstm_input_size = 128 * (250 // 2 // 2)  # Considering the pooling layers\n",
    "\n",
    "        \n",
    "        self.lstm_hidden_size = lstm_hidden_size  # Set the hidden layer size\n",
    "        self.lstm = nn.LSTM(self.lstm_input_size, self.lstm_hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(self.lstm_hidden_size, num_classes)\n",
    "        \n",
    "        # Final tanh activation to constrain output to [-1, 1]\n",
    "        self.final_activation = nn.Tanh()\n",
    "        \n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        \n",
    "        \n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        \n",
    "        \n",
    "        batch_size, _, _, _ = x.size()\n",
    "        x = x.view(batch_size, -1, self.lstm_input_size)\n",
    "        \n",
    "        x, _ = self.lstm(x)\n",
    "        \n",
    "        x = self.fc(x[:, -1, :])\n",
    "        \n",
    "        # Apply final tanh activation to constrain output between -1 and 1\n",
    "        x = self.final_activation(x)\n",
    "        \n",
    "        \n",
    "        return x\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.constant_(m.weight.data, 1)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "model = CNNLSTM(input_channels=1, num_classes=2500, lstm_hidden_size=1024).to(device)\n",
    "model=model.cuda()\n",
    "model.apply(weights_init)\n",
    "model = model.double()  # Convert model parameters to float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70ffa70-78e4-43b9-ba4c-b814eda6f549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_CNNLSTM(model, data_train_loader, data_val_loader, loss_name, num_epochs=20):\n",
    "    \n",
    "    # Define optimizer and scheduler\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5, verbose=True)\n",
    "\n",
    "    patience = 5\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    train_l1_losses = []\n",
    "    train_sc_losses = []\n",
    "    val_l1_losses = []\n",
    "    val_sc_losses = []\n",
    "    alphas = [1.0, 1.0] \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    model_name='model_CCNLSTM'+'_'+str(loss_name)\n",
    "\n",
    "    checkpoint_path=os.getcwd()+'/'+model_name+'.pth'\n",
    "    \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        start_time=time.time()\n",
    "        total_train_loss = 0.0\n",
    "        train_l1_loss = 0.0\n",
    "        train_sc_loss = 0.0\n",
    "        \n",
    "        for spectrograms, target_timeseries in data_train_loader:\n",
    "            \n",
    "            model.train()\n",
    "            spectrograms = spectrograms.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            target_timeseries = target_timeseries.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad() \n",
    "            \n",
    "            # Forward pass\n",
    "            reconstructed_timeseries= model(spectrograms)\n",
    "            \n",
    "            # Compute loss\n",
    "            l1_loss, sc_loss = compute_loss(reconstructed_timeseries, target_timeseries)\n",
    "            \n",
    "            losses = [l1_loss, sc_loss]\n",
    "\n",
    "            # Compute the total loss with current alphas\n",
    "            loss = sum(alpha * loss for alpha, loss in zip(alphas, losses))\n",
    "\n",
    "            # Adjust alphas using GradNorm (with respect to model output)\n",
    "            adjusted_alphas = gradnorm_adjustment(model, losses, alphas)\n",
    "\n",
    "            # Recompute the total loss with adjusted alphas\n",
    "            loss = sum(alpha * loss for alpha, loss in zip(adjusted_alphas, losses))\n",
    "            \n",
    "            # Compute gradients for each loss component independently\n",
    "#            recon_loss.backward(retain_graph=True)\n",
    "#            recon_grad_norm = sum(p.grad.norm() for p in model.parameters() if p.grad is not None)\n",
    "#            model.zero_grad()\n",
    "\n",
    "#            latent_loss.backward(retain_graph=True)\n",
    "#            latent_grad_norm = sum(p.grad.norm() for p in model.parameters() if p.grad is not None)\n",
    "#            model.zero_grad()\n",
    "            \n",
    "            # Dynamically adjust alpha based on gradient norms\n",
    "#            alpha = recon_grad_norm / (latent_grad_norm + 1e-8)\n",
    "\n",
    "            # Compute the total loss and update the model\n",
    "#            loss = recon_loss + alpha * latent_loss    \n",
    "            \n",
    "            \n",
    "            # Backpropagation and optimization\n",
    "#            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_l1_loss+= l1_loss.item()\n",
    "            train_sc_loss+= sc_loss.item()\n",
    "            total_train_loss += loss.item()\n",
    "        avg_train_l1_loss = train_l1_loss / len(train_loader)\n",
    "        avg_train_sc_loss = train_sc_loss / len(train_loader)\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        \n",
    "        train_l1_losses.append(avg_train_l1_loss)\n",
    "        train_sc_losses.append(avg_train_sc_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        val_l1_loss = 0.0\n",
    "        val_sc_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for spectrograms, target_timeseries in data_val_loader:\n",
    "                model.train()\n",
    "                spectrograms = spectrograms.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "                target_timeseries = target_timeseries.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "                # Forward pass\n",
    "                reconstructed_timeseries= model(spectrograms)\n",
    "\n",
    "                # Compute loss\n",
    "                l1_loss, sc_loss = compute_loss(reconstructed_timeseries, target_timeseries)\n",
    "\n",
    "                loss = sum(alpha * loss for alpha, loss in zip(adjusted_alphas, [l1_loss, sc_loss]))\n",
    "                \n",
    "                \n",
    "                # Compute the total loss and update the model\n",
    "                #loss = recon_loss + alpha * latent_loss    \n",
    "                             \n",
    "                \n",
    "                val_l1_loss+= l1_loss.item()\n",
    "                val_sc_loss+= sc_loss.item()\n",
    "                total_val_loss += loss.item()\n",
    "            avg_val_l1_loss = val_l1_loss / len(val_loader)\n",
    "            avg_val_sc_loss = val_sc_loss / len(val_loader)\n",
    "            avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "            val_l1_losses.append(avg_val_l1_loss)\n",
    "            val_sc_losses.append(avg_val_sc_loss)\n",
    "\n",
    "        \n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}],Total Training Loss={avg_train_loss:.4f}, L2 Training Loss={avg_train_l1_loss:.4f}, SC Training Loss={avg_train_sc_loss:.4f}, Total Validation Loss={avg_val_loss:.4f}, L2 Validation Loss={avg_val_l1_loss:.4f}, SC Validation Loss={avg_val_sc_loss:.4f}, Time: {time.time()-start_time:.4f}')\n",
    "        \n",
    "#        if avg_val_latent_loss < best_val_loss:\n",
    "#            best_val_loss = avg_val_latent_loss\n",
    "#            patience_counter = 0\n",
    "#        else:\n",
    "#            patience_counter += 1\n",
    "#            if patience_counter >= patience:\n",
    "#                print(\"Early stopping\")\n",
    "#                break\n",
    "\n",
    "    \n",
    "        # Save checkpoint if validation loss improves\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss_name,\n",
    "            'val_loss': avg_val_loss,\n",
    "        }\n",
    "        \n",
    "        best_checkpoint_filename = checkpoint_path.format('best')\n",
    "        torch.save(checkpoint, best_checkpoint_filename)\n",
    "        best_val_loss = avg_val_loss\n",
    "\n",
    "\n",
    "    gs=gridspec.GridSpec(2,1)\n",
    "    ax1 = plt.subplot(gs[0,0])\n",
    "\n",
    "    # Plot the training and validation metrics\n",
    "    ax1.plot(train_l1_losses, label='Train L2 Loss')\n",
    "    ax1.plot(val_l1_losses, label='Validation L2 Loss')\n",
    "    ax1.legend()\n",
    "    \n",
    "    \n",
    "    ax2 = plt.subplot(gs[1,0])\n",
    "    \n",
    "    ax2.plot(train_sc_losses, label='Train SC Loss')\n",
    "    ax2.plot(val_sc_losses, label='Validation SC Loss')\n",
    "    ax2.legend()\n",
    "\n",
    "    #\n",
    "    #ax2.plot(val_pearson_correlations, label='Validation Pearson')\n",
    "    plt.title(model_name)\n",
    "    plt.savefig(model_name+'.png', format='png')\n",
    "    plt.show()        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac9a73e-043f-4c57-99ea-bcef952ecd58",
   "metadata": {},
   "source": [
    "Testing the generation with random initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eadf29-ab06-4b93-ab8c-d0cd07c47159",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    tms_real=TimeSeries(df_tms.iloc[i][0], dt=1/512)\n",
    "    #qplot_test=tms_real.q_transform(qrange=(12,12),frange=(5,15))\n",
    "    spectrogram_test=torch.tensor(df_qts.iloc[i], dtype=torch.float64)\n",
    "    H, W= spectrogram_test.shape\n",
    "    spectrogram_test = spectrogram_test.view(1, 1, H, W)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(spectrogram_test)\n",
    "    tms_generated=TimeSeries(output[0], dt=1/500)\n",
    "\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(tms_real, label='Original TimeSeries')\n",
    "    plt.plot(tms_generated, label='Generated Output', linestyle='--')\n",
    "    plt.title(\"Comparison of Original TimeSeries and Model Generated Output\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7fffd3-a137-43b8-ab7a-5b4e1a0f5202",
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_input=torch.randn(100,1,250,70).to(torch.float64).to(device)\n",
    "model(mock_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb7f259-7268-41cc-a6d3-8620c4ae6af1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## CNN Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fcf51e-ea64-4e7f-9f91-3adfc9a6ecfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_shape, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder: from 2D spectrogram to 1D latent space\n",
    "        self.encoder_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Dynamically calculate the flattened size after convolution\n",
    "        self.conv_output_shape, self.flattened_size = self._get_conv_output_shape(input_shape)\n",
    "        \n",
    "        # Linear layer to latent space\n",
    "        self.encoder_linear = nn.Linear(in_features=self.flattened_size, out_features=latent_dim)\n",
    "        \n",
    "        # Decoder: from 1D latent space back to 2D spectrogram\n",
    "        self.decoder_linear = nn.Linear(in_features=latent_dim, out_features=self.flattened_size)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Unflatten(1, self.conv_output_shape),  # Unflatten to match encoder output shape\n",
    "            nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=3, stride=2, padding=1, output_padding=(1, 0)),  # Adjusted output_padding\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=16, out_channels=1, kernel_size=3, stride=2, padding=1, output_padding=(1, 0)),  # Adjusted output_padding\n",
    "            nn.Sigmoid()  # Assuming input spectrogram is normalized between [0, 1]\n",
    "        )\n",
    "\n",
    "    def _get_conv_output_shape(self, input_shape):\n",
    "        # Create a mock input to calculate the shape and size after convolutional layers\n",
    "        mock_input = torch.zeros(1, 1, input_shape[0], input_shape[1])\n",
    "        conv_out = self.encoder_conv(mock_input)\n",
    "        conv_output_shape = conv_out.shape[1:]  # Shape after conv layers (channels, height, width)\n",
    "        flattened_size = int(torch.prod(torch.tensor(conv_output_shape)))  # Total flattened size\n",
    "        return conv_output_shape, flattened_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add channel dimension (batch, 1, height, width)\n",
    "        conv_out = self.encoder_conv(x)\n",
    "        flattened = torch.flatten(conv_out, start_dim=1)  # Flatten before the linear layer\n",
    "        latent = self.encoder_linear(flattened)  # 1D latent space\n",
    "        \n",
    "        # Apply tanh to restrict the latent space between -1 and 1\n",
    "        latent = torch.tanh(latent)\n",
    "\n",
    "        decoded = self.decoder_linear(latent)  # Map latent vector back to flattened conv size\n",
    "        reconstructed = self.decoder(decoded)  # Reconstruct the spectrogram\n",
    "        \n",
    "        # Optionally crop the output to ensure exact size\n",
    "        reconstructed = reconstructed[:, :, :input_shape[0], :input_shape[1]]  # Cropping to the desired size\n",
    "        \n",
    "        return reconstructed, latent\n",
    "       \n",
    "\n",
    "# Hyperparameters\n",
    "input_shape = (70, 250)  # Example shape for a 2D spectrogram (channels, frequency, time)\n",
    "latent_dim = 2500  # Fixed size of 1D latent space (you can adjust this)\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Initialize the model, optimizer, and loss function\n",
    "model = Autoencoder(input_shape=input_shape, latent_dim=latent_dim).to(device)\n",
    "\n",
    "# Apply weight initialization\n",
    "model.apply(initialize_weights)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "model = model.double()  # Convert model parameters to float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412ccffc-1c8f-442b-8ea6-29ceae19846a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(model, data_train_loader, data_val_loader, loss_name, num_epochs=20):\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5, verbose=True)\n",
    "\n",
    "    patience = 5\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    train_recon_losses = []\n",
    "    train_latent_losses = []\n",
    "    val_recon_losses = []\n",
    "    val_latent_losses = []\n",
    "    alphas = [1.0, 1.0] \n",
    "    \n",
    "    \n",
    "    model_name='model_CNN_AE'+'_'+str(loss_name)\n",
    "    checkpoint_path=os.getcwd()+'/'+model_name+'.pth'\n",
    "    \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        start_time=time.time()\n",
    "        total_train_loss = 0.0\n",
    "        train_recon_loss = 0.0\n",
    "        train_latent_loss = 0.0\n",
    "        \n",
    "        for spectrograms, target_timeseries in data_train_loader:\n",
    "            \n",
    "            model.train()\n",
    "            spectrograms = spectrograms.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            target_timeseries = target_timeseries.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad() \n",
    "            \n",
    "            # Forward pass\n",
    "            reconstructed_spectrograms, latent_rep = model(spectrograms)\n",
    "            \n",
    "            # Compute loss\n",
    "            recon_loss, latent_loss = loss_function_autoencoder(reconstructed_spectrograms, spectrograms, latent_rep, target_timeseries)\n",
    "            \n",
    "            losses = [recon_loss, latent_loss]\n",
    "\n",
    "            # Compute the total loss with current alphas\n",
    "            loss = sum(alpha * loss for alpha, loss in zip(alphas, losses))\n",
    "\n",
    "            # Adjust alphas using GradNorm (with respect to model output)\n",
    "            adjusted_alphas = gradnorm_adjustment(model, losses, alphas)\n",
    "\n",
    "            # Recompute the total loss with adjusted alphas\n",
    "            loss = sum(alpha * loss for alpha, loss in zip(adjusted_alphas, losses))\n",
    "            \n",
    "            # Compute gradients for each loss component independently\n",
    "#            recon_loss.backward(retain_graph=True)\n",
    "#            recon_grad_norm = sum(p.grad.norm() for p in model.parameters() if p.grad is not None)\n",
    "#            model.zero_grad()\n",
    "\n",
    "#            latent_loss.backward(retain_graph=True)\n",
    "#            latent_grad_norm = sum(p.grad.norm() for p in model.parameters() if p.grad is not None)\n",
    "#            model.zero_grad()\n",
    "            \n",
    "            # Dynamically adjust alpha based on gradient norms\n",
    "#            alpha = recon_grad_norm / (latent_grad_norm + 1e-8)\n",
    "\n",
    "            # Compute the total loss and update the model\n",
    "#            loss = recon_loss + alpha * latent_loss    \n",
    "            \n",
    "            \n",
    "            # Backpropagation and optimization\n",
    "#            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_recon_loss+= recon_loss.item()\n",
    "            train_latent_loss+= latent_loss.item()\n",
    "            total_train_loss += loss.item()\n",
    "        avg_train_recon_loss = train_recon_loss / len(train_loader)\n",
    "        avg_train_latent_loss = train_latent_loss / len(train_loader)\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        \n",
    "        train_recon_losses.append(avg_train_recon_loss)\n",
    "        train_latent_losses.append(avg_train_latent_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        val_recon_loss = 0.0\n",
    "        val_latent_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for spectrograms, target_timeseries in data_val_loader:\n",
    "                model.train()\n",
    "                spectrograms = spectrograms.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "                target_timeseries = target_timeseries.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "                # Forward pass\n",
    "                reconstructed_spectrograms, latent_rep = model(spectrograms)\n",
    "\n",
    "                # Compute loss\n",
    "                recon_loss, latent_loss = loss_function_autoencoder(reconstructed_spectrograms, spectrograms, latent_rep, target_timeseries)\n",
    "\n",
    "                loss = sum(alpha * loss for alpha, loss in zip(adjusted_alphas, [recon_loss, latent_loss]))\n",
    "                \n",
    "                \n",
    "                # Compute the total loss and update the model\n",
    "                #loss = recon_loss + alpha * latent_loss    \n",
    "                             \n",
    "                \n",
    "                val_recon_loss+= recon_loss.item()\n",
    "                val_latent_loss+= latent_loss.item()\n",
    "                total_val_loss += loss.item()\n",
    "            avg_val_recon_loss = val_recon_loss / len(val_loader)\n",
    "            avg_val_latent_loss = val_latent_loss / len(val_loader)\n",
    "            avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "            val_recon_losses.append(avg_val_recon_loss)\n",
    "            val_latent_losses.append(avg_val_latent_loss)\n",
    "\n",
    "        \n",
    "        scheduler.step(avg_val_latent_loss)\n",
    "        \n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}],Total Training Loss={avg_train_loss:.4f},Recon Training Loss={avg_train_recon_loss:.4f},Latent Training Loss={avg_train_latent_loss:.4f},Total Validation Loss={avg_val_loss:.4f},Recon Validation Loss={avg_val_recon_loss:.4f},Latent Validation Loss={avg_val_latent_loss:.4f}, Time: {time.time()-start_time:.4f}')\n",
    "        \n",
    "#        if avg_val_latent_loss < best_val_loss:\n",
    "#            best_val_loss = avg_val_latent_loss\n",
    "#            patience_counter = 0\n",
    "#        else:\n",
    "#            patience_counter += 1\n",
    "#            if patience_counter >= patience:\n",
    "#                print(\"Early stopping\")\n",
    "#                break\n",
    "\n",
    "    # Save checkpoint if validation loss improves\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss_name,\n",
    "            'val_loss': avg_val_loss,\n",
    "        }\n",
    "        \n",
    "        best_checkpoint_filename = checkpoint_path.format('best')\n",
    "        torch.save(checkpoint, best_checkpoint_filename)\n",
    "        best_val_loss = avg_val_loss\n",
    "\n",
    "\n",
    "\n",
    "    #save the model and the graph of the losses\n",
    "#    model_name='model_CNN_AE_'+str(epoch+1)+'e'\n",
    "#    torch.save(model.state_dict(), model_name+'.pth')\n",
    "\n",
    "\n",
    "    gs=gridspec.GridSpec(2,1)\n",
    "    ax1 = plt.subplot(gs[0,0])\n",
    "\n",
    "    # Plot the training and validation metrics\n",
    "    ax1.plot(train_recon_losses, label='Train Recon Loss')\n",
    "    ax1.plot(val_recon_losses, label='Validation Recon Loss')\n",
    "    ax1.legend()\n",
    "    \n",
    "    \n",
    "    ax2 = plt.subplot(gs[1,0])\n",
    "    \n",
    "    ax2.plot(train_latent_losses, label='Train Latent Loss')\n",
    "    ax2.plot(val_latent_losses, label='Validation Latent Loss')\n",
    "    ax2.legend()\n",
    "\n",
    "    #\n",
    "    #ax2.plot(val_pearson_correlations, label='Validation Pearson')\n",
    "    #plt.title(model_name)\n",
    "    plt.savefig(model_name+'.png', format='png')\n",
    "    plt.show()        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da481dc8-96b1-4af4-bf40-facfbff31969",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f2f60f88-8875-4b80-803f-84f995524952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2500])\n"
     ]
    }
   ],
   "source": [
    "# Patch Embedder with unified embedding for all patches\n",
    "class PatchEmbedder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PatchEmbedder, self).__init__()\n",
    "        # Define a single linear layer to embed all patches of size 512 into latent space\n",
    "        self.embed = nn.Linear(512, 128)  # Project 512-dim patch to 128-dim latent space\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Split the input tensor into predefined patches, each of size 512\n",
    "        patches = [\n",
    "            x[:, :1, :512],  # First patch of size 512\n",
    "            x[:, 1:2, :512],  # Second patch of size 512\n",
    "            x[:, 2:3, :512],  # Third patch of size 512\n",
    "            x[:, 3:4, :512],  # Fourth patch of size 512\n",
    "            x[:, 4:5, :512],  # Fifth patch of size 512\n",
    "            x[:, 5:6, :512],  # Sixth patch of size 512\n",
    "            x[:, 6:7, :512],  # Seventh patch of size 512\n",
    "            x[:, 7:8, :512],  # Eighth patch of size 512\n",
    "            x[:, 8:9, :512],  # Ninth patch of size 512\n",
    "            x[:, 9:10, :512],  # Tenth patch of size 512\n",
    "            x[:, 10:, :512],  # Eleventh patch of size 512\n",
    "        ]\n",
    "        \n",
    "        # Embed each patch into the latent space (128-dim)\n",
    "        embedded_patches = [self.embed(patch) for patch in patches]\n",
    "        \n",
    "        # Concatenate the embedded patches\n",
    "        embeddings = torch.cat(embedded_patches, dim=1)  # Shape: (batch_size, num_patches, latent_dim)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "# Transformer Model\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, num_patches=11, latent_dim=128, output_size=2500, num_heads=4, num_layers=6):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        # Patch embedder\n",
    "        self.patch_embedder = PatchEmbedder()\n",
    "        \n",
    "        # Transformer\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=latent_dim,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        # Linear layer to project to output size\n",
    "        self.fc = nn.Linear(num_patches * latent_dim, output_size)\n",
    "        # Final tanh activation to constrain output to [-1, 1]\n",
    "        self.final_activation = nn.Tanh()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Step 1: Embed the input patches into the latent space\n",
    "        embeddings = self.patch_embedder(x)\n",
    "        \n",
    "        # Step 2: Pass through Transformer\n",
    "        # Embeddings shape: (batch_size, num_patches, latent_dim)\n",
    "        transformer_output = self.transformer(embeddings, embeddings)\n",
    "        \n",
    "        # Step 3: Flatten the output and project to the target output size\n",
    "        flat_output = transformer_output.view(transformer_output.size(0), -1)  # Flatten\n",
    "        output = self.fc(flat_output)  # Project to 1D tensor of length 2500\n",
    "        \n",
    "        # Apply final tanh activation to constrain output between -1 and 1\n",
    "        output = self.final_activation(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Example usage\n",
    "batch_size = 1\n",
    "input_tensor = torch.randn(batch_size, 11, 512).to('cuda').double()  # Input of shape (1, 11, 512)\n",
    "\n",
    "model = TransformerModel(num_heads=4).to('cuda')\n",
    "initialize_weights(model)\n",
    "model=model.double()\n",
    "output_tensor = model(input_tensor)\n",
    "\n",
    "print(output_tensor.shape)  # Expected shape: (1, 2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0ad03630-447f-4c92-bc1f-63e7ca707c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 625])\n"
     ]
    }
   ],
   "source": [
    "class PatchEmbedder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PatchEmbedder, self).__init__()\n",
    "        # Define linear layers to embed patches into latent space\n",
    "        self.embed_128 = nn.Linear(128, 128)  # Project 128-dim patch to 128-dim latent space\n",
    "        self.embed_256 = nn.Linear(256, 128)  # Project 256-dim patch to 128-dim latent space\n",
    "        self.embed_512 = nn.Linear(512, 128)  # Project 512-dim patch to 128-dim latent space\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Split the input tensor into predefined patches\n",
    "        patches = [\n",
    "            x[:, :1, :128], x[:, 1:2, :128], x[:, 2:3, :128], x[:, 3:4, :128],\n",
    "            x[:, 4:5, :256], x[:, 5:6, :256], x[:, 6:7, :256], x[:, 7:8, :256],\n",
    "            x[:, 8:9, :512], x[:, 9:10, :512], x[:, 10:, :512]\n",
    "        ]\n",
    "        \n",
    "        # Embed each patch into the latent space\n",
    "        embedded_patches = [\n",
    "            self.embed_128(patches[i]) for i in range(4)\n",
    "        ] + [\n",
    "            self.embed_256(patches[i]) for i in range(4, 8)\n",
    "        ] + [\n",
    "            self.embed_512(patches[i]) for i in range(8, 11)\n",
    "        ]\n",
    "        \n",
    "        # Concatenate the embedded patches\n",
    "        embeddings = torch.cat(embedded_patches, dim=1)  # Resulting shape: (batch_size, num_patches, latent_dim)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, num_patches=11, latent_dim=128, output_size=2500, num_heads=4, num_layers=6):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        # Patch embedder\n",
    "        self.patch_embedder = PatchEmbedder()\n",
    "        \n",
    "        # Transformer\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=latent_dim,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        # Linear layer to project to output size\n",
    "        self.fc = nn.Linear(num_patches * latent_dim, output_size)\n",
    "        # Final tanh activation to constrain output to [-1, 1]\n",
    "        self.final_activation = nn.Tanh()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Step 1: Embed the input patches into the latent space\n",
    "        embeddings = self.patch_embedder(x)\n",
    "        \n",
    "        # Step 2: Pass through Transformer\n",
    "        # Embeddings shape: (batch_size, num_patches, latent_dim)\n",
    "        transformer_output = self.transformer(embeddings, embeddings)\n",
    "        \n",
    "        # Step 3: Flatten the output and project to the target output size\n",
    "        flat_output = transformer_output.view(transformer_output.size(0), -1)  # Flatten\n",
    "        output = self.fc(flat_output)  # Project to 1D tensor of length 2500\n",
    "        # Apply final tanh activation to constrain output between -1 and 1\n",
    "        output = self.final_activation(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    \n",
    "import torch.nn.init as init\n",
    "\n",
    "def initialize_weights(model):\n",
    "    \"\"\"\n",
    "    Initialize the weights of the model with appropriate schemes.\n",
    "    \"\"\"\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            # Xavier initialization for fully connected layers\n",
    "            init.xavier_normal_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.Embedding):\n",
    "            # Normal initialization for embedding layers (if you use any)\n",
    "            init.normal_(m.weight, mean=0, std=1)\n",
    "        elif isinstance(m, nn.Transformer):\n",
    "            # Default PyTorch initialization is typically sufficient for Transformer layers\n",
    "            for p in m.parameters():\n",
    "                if p.dim() > 1:\n",
    "                    init.xavier_uniform_(p)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            # Initialize LayerNorm layers to have weights of 1 and biases of 0\n",
    "            init.constant_(m.bias, 0)\n",
    "            init.constant_(m.weight, 1.0)    \n",
    "    \n",
    "    \n",
    "# Example usage\n",
    "batch_size = 1\n",
    "output_size=125*5\n",
    "input_tensor = torch.randn(batch_size, 11, 512).to('cuda').double()  # Input of shape (1, 11, 512)\n",
    "\n",
    "model = TransformerModel(num_heads=4, output_size=output_size).to('cuda')\n",
    "initialize_weights(model)\n",
    "model=model.double()\n",
    "output_tensor = model(input_tensor)\n",
    "\n",
    "print(output_tensor.shape)  # Expected shape: (1, 2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f54b02c-462f-4100-ae91-32482ec48e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Transformer2Dto1D(nn.Module):\n",
    "#     def __init__(self, input_dim, latent_dim, output_size=2500, num_heads=4, num_layers=2):\n",
    "#         super(Transformer2Dto1D, self).__init__()\n",
    "\n",
    "#         # Linear projection from 2D input (e.g., spectrogram) to a sequence of embeddings\n",
    "#         self.embedding = nn.Linear(input_dim, latent_dim)\n",
    "\n",
    "#         # Positional encoding is dynamically created for each input length\n",
    "#         self.positional_encoding = nn.Parameter(torch.zeros(1, 1000, latent_dim))  # Large enough max length\n",
    "\n",
    "#         # Transformer encoder layers\n",
    "#         encoder_layer = nn.TransformerEncoderLayer(\n",
    "#             d_model=latent_dim, nhead=num_heads, dim_feedforward=512\n",
    "#         )\n",
    "#         self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "#         # Project to the desired output size directly (2500 timesteps)\n",
    "#         self.project_to_output = nn.Linear(latent_dim, output_size)\n",
    "        \n",
    "#         # Final tanh activation to constrain output to [-1, 1]\n",
    "#         self.final_activation = nn.Tanh()\n",
    "        \n",
    "        \n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Input x: 2D tensor of shape (batch_size, num_features, num_timesteps)\n",
    "#         batch_size, num_features, num_timesteps = x.size()\n",
    "\n",
    "#         # Reshape input to (batch_size, num_timesteps, num_features)\n",
    "#         x = x.transpose(1, 2)\n",
    "\n",
    "#         # Project input features to latent_dim space\n",
    "#         x = self.embedding(x)\n",
    "\n",
    "#         # Generate positional encodings dynamically based on the input length\n",
    "#         pos_enc = self.positional_encoding[:, :num_timesteps, :]\n",
    "#         x = x + pos_enc\n",
    "\n",
    "#         # Pass through transformer encoder layers\n",
    "#         x = self.transformer_encoder(x)\n",
    "\n",
    "#         # Pooling layer: Global average pooling to reduce sequence length\n",
    "#         x = x.mean(dim=1)  # Pool across the sequence dimension\n",
    "\n",
    "#         # Project to 1D time series with 2500 steps\n",
    "#         x = self.project_to_output(x)\n",
    "        \n",
    "#         # Apply final tanh activation to constrain output between -1 and 1\n",
    "#         x = self.final_activation(x)\n",
    "\n",
    "#         return x\n",
    "\n",
    "# # Example usage:\n",
    "# batch_size = 16\n",
    "# num_features = 70  # Number of frequency bins (e.g., spectrogram height)\n",
    "# num_timesteps = 250  # Example size (variable)\n",
    "\n",
    "# latent_dim = 128  # Latent space size\n",
    "# output_size = 2500  # Target time series length\n",
    "# learning_rate=1e-4\n",
    "\n",
    "# # Initialize model\n",
    "# model = Transformer2Dto1D(input_dim=num_features, latent_dim=latent_dim, output_size=output_size,num_heads=16, num_layers=8).to('cuda')\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# model = model.double()\n",
    "# # Example variable-size input\n",
    "# input_data = torch.randn(batch_size, num_features, num_timesteps).to('cuda').double()\n",
    "\n",
    "# # Forward pass\n",
    "# output = model(input_data)\n",
    "# print(output.shape)  # Output shape: (batch_size, 2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d1445a-8f37-482b-9beb-91a7a1b3f10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transformer(model, data_train_loader, data_val_loader, loss_name, num_epochs=20):\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5, verbose=True)\n",
    "\n",
    "    patience = 5\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    train_l1_losses = []\n",
    "    train_sc_losses = []\n",
    "    val_l1_losses = []\n",
    "    val_sc_losses = []\n",
    "    alphas = [1.0, 1.0] \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    model_name='model_Transformer'+'_'+str(loss_name)\n",
    "\n",
    "    checkpoint_path=os.getcwd()+'/'+model_name+'.pth'\n",
    "    \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        start_time=time.time()\n",
    "        total_train_loss = 0.0\n",
    "        train_l1_loss = 0.0\n",
    "        train_sc_loss = 0.0\n",
    "        \n",
    "        for spectrograms, target_timeseries in data_train_loader:\n",
    "            \n",
    "            model.train()\n",
    "            spectrograms = spectrograms.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            target_timeseries = target_timeseries.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad() \n",
    "            \n",
    "            # Forward pass\n",
    "            reconstructed_timeseries= model(spectrograms)\n",
    "            \n",
    "            # Compute loss\n",
    "            l1_loss, sc_loss = compute_loss(reconstructed_timeseries, target_timeseries)\n",
    "            \n",
    "            losses = [l1_loss, sc_loss]\n",
    "\n",
    "            # Compute the total loss with current alphas\n",
    "            loss = sum(alpha * loss for alpha, loss in zip(alphas, losses))\n",
    "\n",
    "            # Adjust alphas using GradNorm (with respect to model output)\n",
    "            adjusted_alphas = gradnorm_adjustment(model, losses, alphas)\n",
    "\n",
    "            # Recompute the total loss with adjusted alphas\n",
    "            loss = sum(alpha * loss for alpha, loss in zip(adjusted_alphas, losses))\n",
    "            \n",
    "            # Compute gradients for each loss component independently\n",
    "#            recon_loss.backward(retain_graph=True)\n",
    "#            recon_grad_norm = sum(p.grad.norm() for p in model.parameters() if p.grad is not None)\n",
    "#            model.zero_grad()\n",
    "\n",
    "#            latent_loss.backward(retain_graph=True)\n",
    "#            latent_grad_norm = sum(p.grad.norm() for p in model.parameters() if p.grad is not None)\n",
    "#            model.zero_grad()\n",
    "            \n",
    "            # Dynamically adjust alpha based on gradient norms\n",
    "#            alpha = recon_grad_norm / (latent_grad_norm + 1e-8)\n",
    "\n",
    "            # Compute the total loss and update the model\n",
    "#            loss = recon_loss + alpha * latent_loss    \n",
    "            \n",
    "            \n",
    "            # Backpropagation and optimization\n",
    "#            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_l1_loss+= l1_loss.item()\n",
    "            train_sc_loss+= sc_loss.item()\n",
    "            total_train_loss += loss.item()\n",
    "        avg_train_l1_loss = train_l1_loss / len(train_loader)\n",
    "        avg_train_sc_loss = train_sc_loss / len(train_loader)\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        \n",
    "        train_l1_losses.append(avg_train_l1_loss)\n",
    "        train_sc_losses.append(avg_train_sc_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        val_l1_loss = 0.0\n",
    "        val_sc_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for spectrograms, target_timeseries in data_val_loader:\n",
    "                model.train()\n",
    "                spectrograms = spectrograms.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "                target_timeseries = target_timeseries.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "                # Forward pass\n",
    "                reconstructed_timeseries= model(spectrograms)\n",
    "\n",
    "                # Compute loss\n",
    "                l1_loss, sc_loss = compute_loss(reconstructed_timeseries, target_timeseries)\n",
    "\n",
    "                loss = sum(alpha * loss for alpha, loss in zip(adjusted_alphas, [l1_loss, sc_loss]))\n",
    "                \n",
    "                \n",
    "                # Compute the total loss and update the model\n",
    "                #loss = recon_loss + alpha * latent_loss    \n",
    "                             \n",
    "                \n",
    "                val_l1_loss+= l1_loss.item()\n",
    "                val_sc_loss+= sc_loss.item()\n",
    "                total_val_loss += loss.item()\n",
    "            avg_val_l1_loss = val_l1_loss / len(val_loader)\n",
    "            avg_val_sc_loss = val_sc_loss / len(val_loader)\n",
    "            avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "            val_l1_losses.append(avg_val_l1_loss)\n",
    "            val_sc_losses.append(avg_val_sc_loss)\n",
    "\n",
    "        \n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}],Total Training Loss={avg_train_loss:.4f},L2 Training Loss={avg_train_l1_loss:.4f},SC Training Loss={avg_train_sc_loss:.4f},Total Validation Loss={avg_val_loss:.4f},L2 Validation Loss={avg_val_l1_loss:.4f},SC Validation Loss={avg_val_sc_loss:.4f}, Time: {time.time()-start_time:.4f}')\n",
    "        \n",
    "#        if avg_val_latent_loss < best_val_loss:\n",
    "#            best_val_loss = avg_val_latent_loss\n",
    "#            patience_counter = 0\n",
    "#        else:\n",
    "#            patience_counter += 1\n",
    "#            if patience_counter >= patience:\n",
    "#                print(\"Early stopping\")\n",
    "#                break\n",
    "\n",
    "    \n",
    "        # Save checkpoint if validation loss improves\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss_name,\n",
    "            'val_loss': avg_val_loss,\n",
    "        }\n",
    "        \n",
    "        best_checkpoint_filename = checkpoint_path.format('best')\n",
    "        torch.save(checkpoint, best_checkpoint_filename)\n",
    "        best_val_loss = avg_val_loss\n",
    "\n",
    "\n",
    "    gs=gridspec.GridSpec(2,1)\n",
    "    ax1 = plt.subplot(gs[0,0])\n",
    "\n",
    "    # Plot the training and validation metrics\n",
    "    ax1.plot(train_l1_losses, label='Train L2 Loss')\n",
    "    ax1.plot(val_l1_losses, label='Validation L2 Loss')\n",
    "    ax1.legend()\n",
    "    \n",
    "    \n",
    "    ax2 = plt.subplot(gs[1,0])\n",
    "    \n",
    "    ax2.plot(train_sc_losses, label='Train SC Loss')\n",
    "    ax2.plot(val_sc_losses, label='Validation SC Loss')\n",
    "    ax2.legend()\n",
    "\n",
    "    #\n",
    "    #ax2.plot(val_pearson_correlations, label='Validation Pearson')\n",
    "    plt.title(model_name)\n",
    "    plt.savefig(model_name+'.png', format='png')\n",
    "    plt.show()        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7a042d-8991-4202-9149-84b3f6969dae",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1120536c-2182-4460-963c-e0a9d3a801d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_magnitude(stft_res):\n",
    "    real = stft_res[:, :, :, 0]\n",
    "    im = stft_res[:, :, :, 1]\n",
    "    return torch.sqrt(torch.pow(real, 2) + torch.pow(im, 2))\n",
    "def compute_stft(x):\n",
    "    _window = torch.hann_window(1024).to(device)\n",
    "    stft = torch.stft(x, 1024, win_length=1024, hop_length=64, window=_window, center=False, normalized=True,return_complex=False).to(device).transpose(1,2)\n",
    "    stft /= _window.pow(2).sum().sqrt()\n",
    "    return get_magnitude(stft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604a9c8b-ed24-4ef8-aade-818ce3d87bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneD_L1_loss(y, target):\n",
    "    # Normalize the time series\n",
    "    y_mean = y.mean(dim=1, keepdim=True)\n",
    "    target_mean = target.mean(dim=1, keepdim=True)\n",
    "\n",
    "    y_std = y.std(dim=1, keepdim=True)\n",
    "    target_std = target.std(dim=1, keepdim=True)\n",
    "\n",
    "    y_norm = (y - y_mean) / (y_std + 1e-19)\n",
    "    target_norm = (target - target_mean) / (target_std + 1e-19)\n",
    "\n",
    "    l1_loss=torch.sum(torch.abs(y_norm-target_norm))\n",
    "    return l1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405b02c5-c8b4-48e8-9061-a880bb118bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_loss(y, target):\n",
    "    # Normalize the time series\n",
    "    y_mean = y.mean(dim=1, keepdim=True)\n",
    "    target_mean = target.mean(dim=1, keepdim=True)\n",
    "\n",
    "    y_std = y.std(dim=1, keepdim=True)\n",
    "    target_std = target.std(dim=1, keepdim=True)\n",
    "\n",
    "    y_norm = (y - y_mean) / (y_std + 1e-19)\n",
    "    target_norm = (target - target_mean) / (target_std + 1e-19)\n",
    "\n",
    "    # Compute the correlation\n",
    "    correlation_matrix = torch.mm(y_norm, target_norm.T) / y.size(1)\n",
    "    diag_elements = correlation_matrix.diag()\n",
    "    correlation_loss = 1 - diag_elements.mean()\n",
    "    return correlation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dd79830f-a216-42b3-8ac7-3096597b7615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1532.1269, device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mock_y=torch.randn(100,2500).to(torch.float64).to(device)\n",
    "mock_target=torch.randn(100,2500).to(torch.float64).to(device)\n",
    "\n",
    "qt_y=qtransform(mock_y)\n",
    "qt_target=qtransform(mock_target)\n",
    "torch.norm(qt_y-qt_target)\n",
    "#qt_diff=torch.abs(qt_y-qt_targets)\n",
    "#torch.mean(qt_diff/(qt_y+1e-19))\n",
    "#compute_loss_stft(mock_y, mock_target, qtransform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a05d7be7-70a6-451a-83f3-6bbe5bdb985a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.5249, -0.8531,  0.0596],\n",
      "         [ 1.5927, -0.7633, -1.9240]]])\n",
      "tensor([[-0.5249, -0.8531,  0.0596,  1.5927, -0.7633, -1.9240]])\n",
      "tensor(-0.4022)\n"
     ]
    }
   ],
   "source": [
    "mock_data=torch.randn([100,11,512])\n",
    "print(mock_data)\n",
    "print(mock_data.reshape(1,-1))\n",
    "print(torch.mean(mock_data.reshape(1,-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "88c3c7d3-b2ea-49c0-a766-97e9393546cd",
   "metadata": {},
   "outputs": [],
   "source": [
    " #forza roma\n",
    "def compute_loss_stft(y, target, qtransform):\n",
    "        b = y.shape[0]\n",
    "        \n",
    "#        stft_y = compute_stft(y).reshape(b,-1)\n",
    "\n",
    "#        stft_target = compute_stft(target).reshape(b,-1)\n",
    "\n",
    "#        p1 = torch.norm(stft_target - stft_y, p=2, dim=1)\n",
    "\n",
    "#        p2 = torch.norm(stft_target, p=2, dim=1)\n",
    "\n",
    "#        stft_loss = torch.mean(p1/(p2+1e-19))\n",
    "        \n",
    "\n",
    "        qt_y = qtransform(y)\n",
    "        #.reshape(b,-1)\n",
    "        #p1=torch.norm(qt_y, dim=2)\n",
    "       \n",
    "        qt_target = qtransform(target)\n",
    "        #.reshape(b,-1)\n",
    "        #p2=torch.norm(qt_target, dim=2)\n",
    "        weights=torch.tensor([128,128,128,128,256,256,256,256,512,512,512])\n",
    "        weights=weights.reshape(1,11,1).to(device)\n",
    "        \n",
    "        qt_abs_diff=torch.abs(qt_y-qt_target)/weights\n",
    "        \n",
    "        \n",
    "        qt_loss=torch.mean(torch.sum(qt_abs_diff/(qt_target+1e-10), dim=2))\n",
    "        \n",
    "\n",
    "#        p1= torch.norm(qt_target - qt_y, p=2, dim=1)\n",
    "\n",
    "#        p2 = torch.norm(qt_target, p=2, dim=1)\n",
    "\n",
    "#        qt_loss = torch.mean(p1/(p2+1e-19))\n",
    "        \n",
    "        #qt_loss=torch.mean(torch.abs(qt_y-qt_target)/(qt_target+1e-19))\n",
    "#        qt_loss=torch.norm(qt_y-qt_target, p='fro')/torch.norm(qt_y, p='fro')\n",
    "        #qt_loss=torch.mean(torch.abs(qt_y-qt_target)/qt_target)\n",
    "        #qt_loss=nn.MSELoss()(qt_y, qt_target)/torch.mean(qt_target)\n",
    "        \n",
    "        # Pearson Correlation Loss\n",
    "        #correlation= pearson_corr_batch(y, target)\n",
    "        #correlation_loss =torch.mean(1-correlation)\n",
    "        \n",
    "        #L2 loss\n",
    "        # Compute the squared differences\n",
    "        #squared_diff = (y - target) ** 2\n",
    "        \n",
    "        # Sum the squared differences along the data dimension and take the square root to get the L2 distance\n",
    "        #l2_dist = torch.mean(torch.sqrt(squared_diff.sum(dim=1)))\n",
    "        \n",
    "        #l1_dist = torch.mean(torch.abs(y - target))\n",
    "        #l1_dist=torch.tensor(0)\n",
    "        \n",
    "        #l_tot=50*l1_dist+stft_loss\n",
    "        \n",
    "        #qt_loss=torch.tensor(0)\n",
    "        \n",
    "        #tot_loss=qt_loss+8*correlation_loss\n",
    "\n",
    "        return qt_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0174427-6df7-4a75-8093-b204d8205abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y, target):\n",
    "    b = y.shape[0]\n",
    "    \n",
    "    stft_y = compute_stft(y).reshape(b, -1)\n",
    "        \n",
    "    stft_target = compute_stft(target).reshape(b, -1)\n",
    "\n",
    "    p1 = torch.norm(stft_target - stft_y, p=2, dim=1)\n",
    "\n",
    "    p2 = torch.norm(stft_target, p=2, dim=1)\n",
    "\n",
    "    sc_loss = torch.mean(p1/(p2+1e-19))\n",
    "    \n",
    "    l2_loss=nn.MSELoss()(y, target)\n",
    "    \n",
    "    #tot_loss= l1_loss+sc_loss\n",
    "    \n",
    "    # Pearson Correlation Loss\n",
    "    #correlation= pearson_corr_batch(y, target)\n",
    "    #correlation_loss =torch.mean(1-correlation)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #log_loss=torch.sum(torch.abs(torch.log(p1+1e-19)-torch.log(p2+1e-19)))\n",
    "    \n",
    "    \n",
    "    #tot_loss= 55.94481680006932*sc_loss+0.5045089877544637*log_loss\n",
    "\n",
    "    return l2_loss,sc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff4f356-d4e5-44fd-b608-f4e13f261305",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogCoshLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogCoshLoss, self).__init__()\n",
    "\n",
    "    def forward(self, predicted, target):\n",
    "        # Compute the difference between predicted and target\n",
    "        diff = predicted - target\n",
    "        # Compute the log-cosh of the difference\n",
    "        loss = torch.log(torch.cosh(diff))\n",
    "        # Return the mean loss\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43be237f-5cc5-4f6b-80c2-da246db83a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function_autoencoder(reconstructed_spectrogram, original_spectrogram, latent_representation, target_timeseries):\n",
    "    # Reconstruction loss (2D spectrogram)\n",
    "    recon_loss = nn.MSELoss()(reconstructed_spectrogram, original_spectrogram)\n",
    "    \n",
    "    b = latent_representation.shape[0]\n",
    "        \n",
    "    stft_latent = compute_stft(latent_representation).reshape(b,-1)\n",
    "\n",
    "    stft_target = compute_stft(target_timeseries).reshape(b,-1)\n",
    "\n",
    "    p1 = torch.norm(stft_target - stft_latent, p=2, dim=1)\n",
    "\n",
    "    p2 = torch.norm(stft_target, p=2, dim=1)\n",
    "\n",
    "    latent_loss = torch.mean(p1/(p2+1e-19))\n",
    "\n",
    "    \n",
    "    # Latent space loss (1D time series)\n",
    "    #latent_loss = nn.MSELoss()(latent_representation, target_timeseries)\n",
    "    \n",
    "    \n",
    "    # Combine the losses\n",
    "    #total_loss = recon_loss + 4*latent_loss\n",
    "    \n",
    "    return recon_loss, latent_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ace294f-e64d-4c23-973c-8683f6067f91",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Check initial gradients to adjust the relative weights in the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee7c7da-459a-4040-bc7f-6cc4d8aea63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_input=torch.randn(100,1,250,70).to(torch.float64).to(device)\n",
    "mock_targets=torch.randn(100,5000).to(torch.float64).to(device)\n",
    "\n",
    "model.train()\n",
    "# Forward pass\n",
    "output = model(mock_input)\n",
    "\n",
    "# Compute both losses\n",
    "#total_loss,L1,L2 = compute_loss(output, mock_targets)\n",
    "total_loss,L1,L2 = loss_function_autoencoder(output, mock_targets)\n",
    "\n",
    "model.train()\n",
    "# Zero gradients before backprop\n",
    "#model.zero_grad()\n",
    "\n",
    "# Backpropagation\n",
    "#total_loss.backward()\n",
    "\n",
    "# Get gradients of the first layer (fc1) weights\n",
    "#c_gradients = model.fc.weight.grad\n",
    "\n",
    "# Print the gradients of the first layer\n",
    "#print gradients of the first layer (fc):\", fc_gradients)\n",
    "\n",
    "# Optionally: inspect the gradients of L1 and L2 separately\n",
    "\n",
    "\n",
    "#print(\"Gradients of L2:\", fc_gradients_L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a017c0-0fe0-4143-ace6-fde5efde109b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.zero_grad()  # Reset gradients\n",
    "L1.backward()  # Retain graph for the second backward pass\n",
    "fc_gradients_L1 = model.fc.weight.grad.clone()\n",
    "\n",
    "\n",
    "\n",
    "print(\"Gradients of L1:\", fc_gradients_L1)\n",
    "\n",
    "grad_L1_magnitude = torch.norm(fc_gradients_L1)\n",
    "print(grad_L1_magnitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03294a37-d18c-4d86-a280-98cbc011c5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "output = model(mock_input)\n",
    "\n",
    "# Compute both losses\n",
    "total_loss,L1,L2 = compute_loss(output, mock_targets)\n",
    "\n",
    "model.train()\n",
    "model.zero_grad()\n",
    "L2.backward() \n",
    "fc_gradients_L2 = model.fc.weight.grad.clone()\n",
    "\n",
    "print(\"Gradients of L2:\", fc_gradients_L2)\n",
    "\n",
    "grad_L2_magnitude = torch.norm(fc_gradients_L2)\n",
    "print(grad_L2_magnitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec6a545-a730-4b23-bd61-309b14d13c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_L2_magnitude/grad_L1_magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6acb0f7-40ab-44e5-a828-4642db909297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GradNorm function for adjusting loss weights based on gradient norms\n",
    "def gradnorm_adjustment(model, losses, alphas, total_loss, layer_name='fc1'):\n",
    "    # Zero gradients before computing them\n",
    "    model.zero_grad()\n",
    "    total_loss.backward(retain_graph=True)\n",
    "\n",
    "    # Compute gradient norms for each loss component\n",
    "    grads = []\n",
    "    for i, loss in enumerate(losses):\n",
    "        model.zero_grad()  # Clear previous grads\n",
    "        loss.backward(retain_graph=True)  # Compute gradient for this loss\n",
    "        grad_norm = torch.norm(getattr(model, layer_name).weight.grad, p=2)  # Norm of the first layer gradients\n",
    "        grads.append(grad_norm.item())  # Store the grad norm\n",
    "\n",
    "    # Compute the target gradient norm (average of current norms)\n",
    "    target_grad_norm = sum(grads) / len(grads)\n",
    "\n",
    "    # Calculate scale factors to balance gradients\n",
    "    scale_factors = [target_grad_norm / grad for grad in grads]\n",
    "\n",
    "    # Adjust alphas using scale factors\n",
    "    adjusted_alphas = [alpha * scale for alpha, scale in zip(alphas, scale_factors)]\n",
    "\n",
    "    return adjusted_alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f9ff37-3e74-4305-a3cb-8790c1159b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_input=torch.randn(100,1,250,70).to(torch.float64).to(device)\n",
    "mock_targets=torch.randn(100,5000).to(torch.float64).to(device)\n",
    "\n",
    "# Forward pass\n",
    "mock_output = model(mock_input)\n",
    "\n",
    "# Compute both losses\n",
    "sc_loss = compute_loss_stft(mock_output, mock_targets,qtransform)\n",
    "log_loss = compute_loss(mock_output, mock_targets)\n",
    "\n",
    "model.train()\n",
    "# Initialize loss weights (alphas)\n",
    "alphas = [1.0, 1.0]  # Starting with equal weighting for L1 and L2\n",
    "\n",
    "# Total loss\n",
    "losses = [sc_loss, log_loss]\n",
    "total_loss = sum(alpha * loss for alpha, loss in zip(alphas, losses))\n",
    "\n",
    "# Apply GradNorm to adjust the alphas (scaling factors for L1 and L2)\n",
    "adjusted_alphas = gradnorm_adjustment(model, losses, alphas, total_loss, layer_name='fc')\n",
    "print(\"Scaling Factor= \",adjusted_alphas[0]/adjusted_alphas[1]) \n",
    "# Recompute the total loss with adjusted alphas\n",
    "#total_loss = sum(alpha * loss for alpha, loss in zip(adjusted_alphas, losses))\n",
    "\n",
    "# Zero gradients and backward pass with the adjusted loss\n",
    "#model.zero_grad()\n",
    "#total_loss.backward()\n",
    "\n",
    "# Get gradients of the first layer (fc1) weights after adjustment\n",
    "#fc_gradients = model.fc.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a000527a-0cb4-481d-bea2-f00f6ced48cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Adjusted alphas (weights for L1 and L2):\", adjusted_alphas)\n",
    "print(\"Scaling Factor= \",adjusted_alphas[0]/adjusted_alphas[1]) \n",
    "print(\"Gradients of the first layer (fc1):\", fc_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cd3600-cac9-4f5d-a287-444be8226169",
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_input=torch.randn(100,70,250).to(torch.float64).to(device)\n",
    "mock_targets=torch.randn(100,2500).to(torch.float64).to(device)\n",
    "\n",
    "# Forward pass\n",
    "mock_output = model(mock_input)\n",
    "\n",
    "# Compute both losses\n",
    "l1_loss, sc_loss = compute_loss(mock_output, mock_targets)\n",
    "\n",
    "model.train()\n",
    "# Initialize loss weights (alphas)\n",
    "alphas = [1.0, 1.0]  # Starting with equal weighting for L1 and L2\n",
    "\n",
    "# Total loss\n",
    "losses = [l1_loss, sc_loss]\n",
    "total_loss = sum(alpha * loss for alpha, loss in zip(alphas, losses))\n",
    "\n",
    "# Apply GradNorm to adjust the alphas (scaling factors for L1 and L2)\n",
    "adjusted_alphas = gradnorm_adjustment(model, losses, alphas, total_loss, layer_name='fc')\n",
    "print(\"Scaling Factor= \",adjusted_alphas[0]/adjusted_alphas[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0797f679-775e-44e9-abcf-9c6962b641ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4c86baa0-d757-4eda-bb36-6462c3a752e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Prepare DataLoader for training and validation data\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ab678dc1-ebb4-4ca3-84a1-d97c906f31a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_name='SL_QTnormalised__zeroes_padding'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "cd18a620-0a83-4f6b-a5e4-ac547f5814fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Train Loss: 0.1143, Validation Loss: 0.0779 , Time: 28.0337\n",
      "Epoch [2/30], Train Loss: 0.0893, Validation Loss: 0.0770 , Time: 28.1338\n",
      "Epoch [3/30], Train Loss: 0.0837, Validation Loss: 0.0775 , Time: 28.4754\n",
      "Epoch [4/30], Train Loss: 0.0807, Validation Loss: 0.0768 , Time: 28.4999\n",
      "Epoch [5/30], Train Loss: 0.0788, Validation Loss: 0.0767 , Time: 28.6169\n",
      "Epoch [6/30], Train Loss: 0.0779, Validation Loss: 0.0759 , Time: 28.5377\n",
      "Epoch [7/30], Train Loss: 0.0773, Validation Loss: 0.0760 , Time: 28.4163\n",
      "Epoch [8/30], Train Loss: 0.0770, Validation Loss: 0.0754 , Time: 28.2126\n",
      "Epoch [9/30], Train Loss: 0.0767, Validation Loss: 0.0754 , Time: 28.2744\n",
      "Epoch [10/30], Train Loss: 0.0765, Validation Loss: 0.0757 , Time: 28.0897\n",
      "Epoch [11/30], Train Loss: 0.0764, Validation Loss: 0.0755 , Time: 28.0749\n",
      "Epoch [12/30], Train Loss: 0.0761, Validation Loss: 0.0756 , Time: 28.0582\n",
      "Epoch [13/30], Train Loss: 0.0761, Validation Loss: 0.0756 , Time: 28.0465\n",
      "Epoch [14/30], Train Loss: 0.0761, Validation Loss: 0.0753 , Time: 28.0557\n",
      "Epoch [15/30], Train Loss: 0.0758, Validation Loss: 0.0755 , Time: 28.1648\n",
      "Epoch [16/30], Train Loss: 0.0758, Validation Loss: 0.0754 , Time: 28.0203\n",
      "Epoch [17/30], Train Loss: 0.0757, Validation Loss: 0.0754 , Time: 28.0432\n",
      "Epoch [18/30], Train Loss: 0.0757, Validation Loss: 0.0756 , Time: 28.0267\n",
      "Epoch [19/30], Train Loss: 0.0757, Validation Loss: 0.0758 , Time: 28.0344\n",
      "Epoch 00020: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch [20/30], Train Loss: 0.0757, Validation Loss: 0.0753 , Time: 28.0280\n",
      "Epoch [21/30], Train Loss: 0.0753, Validation Loss: 0.0752 , Time: 28.0339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_123/3187287081.py:188: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig=plt.figure(figsize=(14, 7))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/30], Train Loss: 0.0753, Validation Loss: 0.0752 , Time: 28.1836\n",
      "Epoch [23/30], Train Loss: 0.0753, Validation Loss: 0.0751 , Time: 28.0178\n",
      "Epoch [24/30], Train Loss: 0.0753, Validation Loss: 0.0751 , Time: 28.1859\n",
      "Epoch [25/30], Train Loss: 0.0753, Validation Loss: 0.0751 , Time: 27.9890\n",
      "Epoch [26/30], Train Loss: 0.0753, Validation Loss: 0.0751 , Time: 28.1655\n",
      "Epoch [27/30], Train Loss: 0.0752, Validation Loss: 0.0749 , Time: 28.1136\n",
      "Epoch [28/30], Train Loss: 0.0753, Validation Loss: 0.0751 , Time: 28.1408\n",
      "Epoch [29/30], Train Loss: 0.0752, Validation Loss: 0.0749 , Time: 27.9860\n",
      "Epoch [30/30], Train Loss: 0.0752, Validation Loss: 0.0750 , Time: 27.9970\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIAAAAE6CAYAAABuwSnuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB1b0lEQVR4nO3dd3xTVeMG8Odmd09WWWVvKLNsKMgGGaKCm6WCvIq8KioKxfECjlfgp4C+ivDKS0VkqIAoQkFllCFTQFllCBa6m46kSc7vjzRp0qSl6SBN+nw/n/tJcu65956kp5fm4dxzJSGEABEREREREREReS2ZuxtARERERERERESViwEQEREREREREZGXYwBEREREREREROTlGAAREREREREREXk5BkBERERERERERF6OARARERERERERkZdjAERERERERERE5OUYABEREREREREReTkGQEREREREREREXo4BEBGRB5AkCf379y/XPhITEyFJEp544okKaZM3+eijj9CmTRv4+vpCkiQsWbLE3U0iqpKKO4888cQTkCQJiYmJbmmXLZ7ryJmy9IvVq1dDkiSsXr3arjwyMhKRkZEV2j4ioruBARAREZXI8gewK4sn+eqrrzBz5kyo1Wo899xzmD9/Prp37+7uZlV5eXl5eO+99xAdHY2goCCoVCrUqVMHnTt3xsyZM7F37167+pZ+VFlfyn/66Sc8+OCDaNCgATQaDUJCQtCtWze89dZbyMjIcNoWb+3TRERERM4o3N0AIiKq2qKiojB//ny7ssTERKxZswYNGzb0+P9l//bbbwEAW7duRUREhJtb4xm0Wi369u2LY8eOoXbt2rjvvvtQq1YtJCUl4Y8//sCKFSuQnp6Ofv36VXpbdDodpk6dirVr18LHxwfDhg1D8+bNodVqsXv3brz++uv48MMP8c033yA6OhqA9/dpd1i4cCFefvll1K1b191NIap0u3btcncTiIjKhAEQERGVKCoqClFRUXZle/bswZo1axAZGYnY2Fi3tKui3LhxAwAY/rhgyZIlOHbsGAYPHozvvvsOKpXKbv3Nmzdx4cKFu9KWp59+GmvXrkWnTp2wZcsW1K9f37pOCIGPPvoIzz33HIYOHYqjR4+icePGXt+n3aFOnTqoU6eOu5tBdFc0adLE3U0gIioTXgJGRFTAdn6AixcvYvz48QgLC0NAQAAGDx6M06dPAwCSkpIwZcoU1KlTBxqNBl27dsWePXsc9peeno6XX34ZzZs3t16SMnjwYOzcudPp8fV6Pd588000adIEarUajRo1wmuvvQadTldsmw0GA5YvX47u3bsjMDAQvr6+6NixIz788EOYTKYK+VxcZZmv6MaNG5g0aRLq1KkDuVxunUPhzz//xMsvv4wuXbqgRo0aUKvVaNiwIaZNm4arV6867G/Pnj2QJAmxsbE4fvw4RowYgeDgYPj6+qJv377Yt2+fwzYZGRlYsGAB2rRpg4CAAPj7+yMyMhL3338/jh49CgCIjY2FJEmIj4+3ttvZ5T47d+7EkCFDEBoaCo1Gg2bNmmHOnDlIT093OG7//v0hSRJ0Oh3mzZuHZs2aQaVSWUeUWNbn5+fjjTfeQJMmTaDRaNCiRQv85z//se7no48+Qtu2beHj44N69eohNja22J9nQkICxo8fj9q1a0OlUqF+/fp46qmnrMGWK+0rLctnPn36dIfwBzCHAX369HFpn2Xxyy+/YPXq1QgODsbWrVvtwh/A/DOdOXMmXnzxRaSnp+P5558v1/FszxGJiYmYMGECwsPDodFo0LlzZ+tosqLy8vKwcOFCtGvXDr6+vggMDESfPn3w5ZdflniMc+fOYfz48ahRowZkMhn27NlT4eepGzdu4I033kCvXr2sfSgiIgITJ07E77//XurPprg5gDZv3oyYmBjUrl0barUatWvXRu/evfHRRx857CM1NRWvvPIKWrVqBR8fHwQFBWHgwIH48ccfnR4zKysLs2fPRr169aDRaNCyZUu8//77d+XcZ/k5lLQUnTfm+vXrmDlzJho3bgy1Wo2wsDDce++9OHz4sMP+LeenPXv24L///S+6du0KPz8/u3lnbty4gRkzZiAyMhIqlQo1atTA2LFjne7PIi4uDjExMQgJCYFGo0GrVq3w1ltvOf13Zs+ePRg5ciTq1atn3X/Xrl3LHJDavqc1a9agY8eO8PHxQc2aNTF58mT8/fffDtscPXoUzz33HDp06GB3Dp49ezZSU1OdHqcs/eLChQu4//77ERISAj8/P/Ts2RNbt24ttr6zOYBs5wuKj49H//79ERAQgMDAQAwfPrzY36c///wT9913n92xt23bVuz8Q0RE5cERQERERSQmJiI6OhqtWrWyftHbvHkz+vfvj3379mHo0KEIDg7Ggw8+iLS0NMTFxWHo0KH4888/0aBBAwBAWloaevbsiXPnzqFbt24YN24ckpOT8dVXX2HIkCH48MMPMWPGDOsxhRB44IEH8M0336BJkyaYOXMm9Ho9Vq1ahZMnTzptZ35+PkaNGoUffvgBLVu2xEMPPQSNRoP4+Hj84x//wMGDB7F27dq78pkVlZKSgh49eiAgIADjx4+HEAI1a9YEAGzatAkrV65ETEwMevbsCZVKhdOnT+Ozzz7Dt99+i6NHj6JevXoO+zxy5Ajeeecd9OjRA1OnTsXVq1exceNGDBw4EMeOHUOrVq0AmD/LoUOH4uDBg+jRowemTZsGhUKBa9euYc+ePThw4AA6d+5snVR79erVuHLlisMlQQCwfPlyzJw5E35+fnjggQdQo0YNxMfH45133sG3336L/fv3IyQkxGG7++67D0eOHMGwYcMwZswY1KpVy279hAkTkJCQgOHDh0OpVOLrr7/Gk08+CZVKhSNHjmDdunUYOXIk7rnnHnz33XdYsGABfHx8MGfOHLv9fP7555g2bRo0Gg3uvfde1KtXD+fPn8enn36K7777DgcPHrT2SVfadyc1atQAYP7i4k6W0GzatGkljj556aWXsGTJEnz33Xe4ceNGuUd7XblyBd26dUPjxo3x6KOPIjU1FevXr8eYMWOwc+dODBw40FpXr9dj8ODB+OWXX9C6dWs888wzyMnJwYYNGzBx4kQcO3YMixcvdjjGhQsX0L17d7Ro0QKPPPIItFotAgICrOsr4jwFAD///DMWLVqEmJgY3HffffDz88P58+fx9ddf49tvv8W+ffscRkuV1ooVKzBjxgzUrl0b9957L8LDw3Hr1i2cPHkSq1evxjPPPGP3mfbv3x+JiYno27cvhg0bBq1Wi61bt2Lo0KFYuXIlnnzySWt9nU6HgQMH4vDhw+jQoQMefvhhpKen46233nKYf6oyBAcHOz1nAMCHH36IlJQU+Pr6Wst+++03DB48GKmpqRgyZIj134QtW7agd+/e2Lx5M4YPH+6wr/feew8//fQTRo0ahQEDBliD50uXLqF37964efMmBg4ciIkTJ+LatWvYsGEDtm3bhg0bNmD06NF2+5oyZQpWrVqF+vXr47777kNQUBAOHjyI119/Hbt27cKPP/4IpVIJANi+fTtGjhyJoKAg3Hvvvahbty5SU1Nx9uxZrFixolyj5D744AP8+OOPePDBBzF06FD8+uuv+Pzzz7Fnzx4kJCRYzy+A+Xd88+bN6NevH+655x4YjUYcOXIEH3zwAbZv347Dhw/b/V6UpV+cP38ePXr0QEpKCoYNG4aoqChcuHABY8aMcfozuZOtW7fim2++wbBhw/D000/jzJkz1raeOXPG7v2dO3cOvXr1QmpqKkaMGIH27dvj0qVLGDt2bJmOTUR0R4KIiIQQQly+fFkAEADEW2+9ZbfujTfeEABEUFCQeOqpp4TRaLSu+9///icAiFmzZlnLpk2bJgCI6dOn2+3n3LlzIiAgQCiVSnHp0iWHfXTv3l3k5uZay1NSUkTjxo0FANGvXz+7fc2fP18AEM8995wwGAzWcoPBICZPniwAiM2bNzu8v8cff7wsH4+d+Ph4p20SQlg/w0cffVTk5+c7rL9+/brIy8tzKN++fbuQyWTiqaeecnosAGL16tV261auXCkAiKefftpaduLECQFAjB492uEYRqNRpKam2pX169dPOPvn8PLly0KpVIrAwEDxxx9/2K176qmnBAAxdepUp/tq166duH37tsM+Leu7dOki0tLSrOUXL14USqVSBAUFicjISHH9+nXruvT0dBEeHi7Cw8PtPs8//vhDKJVK0axZM3Hjxg274+zatUvIZDKHz+BO7Sutbdu2CQBCpVKJp59+WnzzzTd2bXbm888/r7D+Z9GoUSMBQPz44493rNujRw8BQKxfv97p+pL6tIXtOSI2NtZu3Y4dOwQAMXToULvyt99+WwAQI0eOtPv5/f3336J+/foCgPjll1+cHuOVV14psQ3lPU8JIURSUpLIzMx0OM7Ro0eFr6+vGDJkiNPjF/05Pv744wKAuHz5srWsY8eOQqVSiaSkJIf9F+1//fr1E5Ikia+++squPC0tTXTo0EFoNBpx8+ZNa7nlcx03bpzd+7x06ZIICQmp8L5WWvPmzXNoV35+vmjSpInQaDR2P2shhPjrr79ERESEqFWrlt2533J+9/X1Fb/99pvDcQYNGiQAiEWLFtmV//LLL0Imk4mQkBC7n6vl92/8+PF2x7E91gcffGAtGzt2rAAgjh075nDssp47LMdRKpUO72nWrFkCgJg8ebJdeWJiot2/bxaWc//ChQvtysvSLyyf5ZIlS+zKt2zZYv1d+/zzz+3WNWzYUDRs2NCuzPIZy+Vy8dNPP9mte/nll53+vAYMGCAAiOXLl9uVb9++vdhjExGVBwMgIqICli82kZGRDn9wXrlyxfrHeNEvSwaDQSiVStG/f38hhBA6nU74+PgIf39/h7BBCCFeffVVAUAsWLDAWnbPPfcIAGL37t0O9S1/VNp+MTUajSIsLEzUqVPH6R/HaWlpQpIkMX78eIf3dzcCoOK+9N1J27ZtRaNGjZweq3fv3g719Xq9UCgUonPnztaykydPCgBi4sSJpTpmcQHQm2++KQCIuXPnOqxLSUkR/v7+QqPR2IVZln3ZBm/OjrVr1y6HdTExMQKA+OyzzxzWTZo0SQAQiYmJ1jLLF6Zt27Y5PdaYMWOETCYTGRkZpW6fKz788EMRHBxs/ZICQNSpU0c88sgjYt++fQ71KyMA8vHxEQDE2bNn71j3gQceEADE+++/73S9KwGQs3OEEEI0aNBAhIWF2ZU1adJESJLkECIKIcQnn3wiAIhJkyY5HKNWrVpOg9KKOk+VxsiRI4VarRZ6vd7h+KUJgDp16iR8fX2dngdtHT9+XAAQ999/v9P1li/iH374obWsadOmQiaTiQsXLjjUtwQNdzsAWrNmjQAgunXrJnJycqzllva/+OKLTrdbsmSJACC2bt1qLbMN+Iu6du2aACAaNmzoNGR/6KGHBACxZs0aa1lUVJRQKpV2wbOFwWAQYWFhokuXLtaycePGCQBO+21ZWd5T0ZBHCHPQHRQU5HBOLY7JZBKBgYEiJibGrtzVfmH5LBs1auT0d9pyznQlAHrkkUcc9nPp0iUBQNx3333WsqtXrwoAomnTpnZhlYXl7wIGQERUkXgJGBFREVFRUZDL5XZllktGmjdvbjfcHADkcjlq1qyJ69evAwD++OMP5Obmonfv3k4vD7rnnnvwr3/9C7/99pu17LfffoNMJkPv3r0d6lsuVbL1559/IiUlBc2aNcObb77p9H34+Pjg3LlzJb/ZShIZGWm95KsoIQT+97//YfXq1Thx4gTS0tJgNBqt653NKQMAXbp0cShTKpWoVasW0tLSrGWtW7dGx44dERcXh2vXruHee+9Fr1690KVLl2L37cyxY8cAADExMQ7rQkND0alTJ/z88884e/aswyUylrtNFadz584OZZY+VtK669evo2HDhgCAAwcOADDP03Ho0CGHbW7dugWTyYTz58877PNO7SuNZ555BpMmTcLOnTuxf/9+HDt2DPv378fatWuxdu1axMbGFnuJTEUrzW3aLXXy8vLKfTxn5wgAqF+/vvXnApjnIrl48SLq1auH5s2bO9S/5557AMDuXGDRoUMHqNVql9rgynnK1rZt27By5UocOXIEycnJMBgMduuTk5PLNMHzww8/jH/+859o06YNJkyYgL59+6JXr152l8AAhX05PT3d6aVFt2/fBgDr+SwrKwsXLlxA/fr1nU7G279/fyxYsMDl9pZHfHw8pk6dikaNGuG7776Dj4+PdZ3l/SUmJjp9f+fPnwdgfn8jRoywW+fsd9VyburTpw8UCsc/5e+55x6sW7cOv/32Gx577DHk5OTgxIkTCA8Px5IlS5y2X61W2/178fDDD2PTpk2Ijo7GhAkTrJfsOrs811XO7g4YFBSEqKgo7N271+6cmp+fj48//hhffvklzpw5g4yMDLu5fP766y/r87L0C8tn2bt3b6e/0/3793f5kkJn/1ZZ5iez/bfq+PHjAIAePXpAJnOclrV379746aefXDo2EdGdMAAiIioiKCjIoczyR7azdZb1+fn5AMwTEANA7dq1nda1fJGy1LM8Dw0Ntc6/YMvZflJSUgCYvziU9EVHq9UWu64yFffeAWD27NlYsmQJ6tSpgyFDhqBu3brWL0uW+XicKemztw2Q5HI5du3ahTfeeANff/01XnrpJQBAYGAgnnjiCfzrX/+Cn5/fHd9DWX6OFiW9/+LeS0l9zLLO0seAwj7w7rvvlngsZ33gTu0rLV9fX4wePdo614her8d//vMfPPfcc4iNjcXo0aPLPH9MadSuXRuXL1/GtWvX0KJFixLrWoKPouFDWZTUF22/nFalPmRZb9uHAGDZsmV47rnnEBISgkGDBqFBgwbw9fWFJEnYsmULTpw4UeJE9CWZPXs2wsPDsXz5cixduhQffPABJElCTEwM3n33XXTq1AlAYV/euXNnsZPkA4V92fJ5FTd3VUX179I6e/Ysxo0bBz8/P2zbts0h/La8vw0bNpS4n9L+rrrar9LS0iCEwO3bt0sdjI0bNw5bt27F+++/j88++wwrV64EYA43Fi1aZDfPlavu9HOz/X148MEHsXnzZjRu3BijR4+2TiYOmO9GaNs3y9IvKqMvlfS7aftv1Z2O7ercbEREpcEAiIiogln++HN2RxPAfIts23qW56mpqcjPz3cIgZztx7Lt2LFjsWnTpgppd0UqbkTGrVu3sGzZMrRt2xb79+93GKUQFxdXIccPCQnBBx98gA8++AAXLlzA3r178fHHH2PZsmVIT0/HmjVr7rgP259jmzZtHNY7+zlalGZESnlZjpuRkYHAwECXtq2s9qlUKjzzzDPWCch3795dqQFQ7969cfnyZfz000/W0TTOpKenW+/+5myEVWUpy7nA4m70IYPBgPnz56N27dr47bffHEb52I5mKqvHHnsMjz32GNLT07F//35s3rwZq1atwuDBg3H27FnUqFHD+v6XLl2KZ5999o77tNRPSkpyur64z7sy3Lp1CyNGjEB2djZ++OEH62T0tizt/eabb3Dvvfe6tH9n/cDVfmV57Nixo9PRZsUZMWKE9b0lJCRg69atWLFiBUaMGGE38b6r7vRzs7T3yJEj2Lx5MwYOHIjvv//e7t9Gk8mEd955x277svQLd/Yly3m7uGMXV05EVB68DTwRUQVr0aIFfH19cfz4cbvh3haW245b/vfb8txkMuHXX391qO/s1s0tW7ZEcHAwDh486PA/+lXZpUuXYDKZMHjwYIfw5/r167h06VKFH7Np06aYMmUK9u7dC39/f2zevLlU23Xs2BGA888/PT0dx48ft95G2R26d+8OwHwr9KrG8rMVQlTqcaZOnQrAfKegkr4svfvuu8jLy0PLli3tfu8qW0BAAJo0aYK//vrLepmPLWfngrspOTkZ6enp6Nmzp0P4o9VqXQoL7iQ4OBjDhw/Hf/7zHzzxxBNISUmx9l1X+3JAQACaNm2Kv/76CxcvXnRY7+x3tjLk5uZi1KhRuHz5Mj799FOnl4sCFf+7ajk3/frrrw6X6wGO/crf3x9t2rTB77//Xuyt00vi5+eHAQMG4N///jdeffVV6HQ6fP/992Vuv7NLqjIyMhzOqRcuXAAAjB492uE/Rg4dOoTc3Fy7srL0C9vP0nZ0TknbVBTLsQ8cOOD0FvXO/h4gIiovBkBERBVMpVLh4Ycfhlarxbx58+zWXbx4EcuWLYNSqcSjjz5qLZ80aRIAYO7cuXZzlKSmpuKtt95yOIZCocA//vEP3Lx5E88++6zDH8KA+X+Bz5w5U1Fvq0JERkYCcPxjW6vVYtq0aU6/zLjq8uXL+P333x3K09LSoNPpoNFoSrWfRx55BEqlEv/3f/9n/SJi8frrryMzMxOPPPJIifO0VKaZM2dCqVTi+eefd3o7dr1eX2nh0MqVK3Hw4EGn686dO2e91KVPnz6VcnyLvn37Wm/DPnLkSKfz26xcuRKLFy+GXC4vdv6TyjR58mQIIfDiiy/a9fnk5GTr/F2TJ0++6+0CgJo1a8LX1xdHjhyxu/woPz8fzz33HJKTk8u1/x07djj9nb516xYAWH8Xu3Tpgj59+mDTpk1YtWqV032dOnXKuh1gPmeaTCbMmTPH7svz5cuXsWzZsnK1uzRMJhMeeeQRHDp0CPPnz8djjz1WbN3Ro0ejSZMm+Oijj7B9+3andQ4cOICcnJxSHbtevXoYNGgQEhMTHfp0QkIC1q1bh5CQEIwdO9ZaPnv2bOj1ekyePNl6K3lbaWlpdoHfrl27nP67YglaS3sedeaLL76wzr1jERsbi4yMDEycONF6TrX8e1E0hLl16xaeeeYZp/t2tV9YPsvLly/jww8/tFv3zTffuDz/jyvq16+P/v3748KFC/j444/t1u3YsYPz/xBRpeAlYERElWDRokX45Zdf8OGHH+Lw4cOIiYlBcnIyvvrqK2RlZeHDDz9Eo0aNrPUnTpyI9evX49tvv0Xbtm0xevRo5Ofn4+uvv0bXrl2d/m/m66+/jhMnTmDlypX47rvvMGDAANStWxe3bt3C+fPnsW/fPrz99tto3br13XzrJapduzYmTJiAL7/8ElFRURg8eDAyMjKwc+dOaDQaREVFWSfGLKsTJ05g7Nix6Ny5M9q2bYuIiAjcvn0b33zzDfLz8zFnzpxS7ScyMhJLlizBM888g06dOuGBBx5AjRo1sHfvXhw4cAAtW7bE4sWLy9XW8mjZsiVWrVqFyZMno02bNhg6dCiaN2+O/Px8XL16Fb/88gtq1KhRKROB79ixA9OnT0dkZCR69eqF+vXrQ6fT4fz58/jhhx+Qn5+PZ599Ft26dXPY9tdff8UTTzzhdL+dOnUq1SVAtj755BMYDAbExcWhRYsWGDZsGJo1a4bs7GzEx8fj9OnTkMlkWLZsGYYMGVKWt1suL7zwAr7//nt888036NChA4YPH46cnBxs2LABt27dwksvveR08ve7QSaT4dlnn8WiRYvQrl07jB49Gnq9HvHx8UhNTUVMTIx1NElZTJgwARqNBr1790ZkZCSEEPjll19w+PBhdOrUye6yvXXr1mHAgAGYMmUKli1bhujoaAQHB+P69es4efIkTp8+jQMHDljn1/nnP/+JLVu2YOPGjejUqROGDBmCjIwMrF+/Hn379sW3335b7s+nJF9//TU2bdqEsLAwAHA6ufOYMWMQFRUFpVKJTZs2YciQIRgxYgR69uyJqKgo+Pr64tq1azh8+DAuXbqEmzdvwtfXt1THX7lyJXr16oUXX3wRP/74I7p06YJr165hw4YNkMlk+Pzzz+1GWU6ePBlHjx7F8uXL0aRJEwwZMgQNGjRAamoqLl++jJ9//hmTJk2yzvXzz3/+E4mJiejfvz8iIyOhUqlw9OhR7N69Gw0aNMCECRPK/NkNHz4cvXr1wgMPPIA6derg119/xa+//orIyEgsWrTIWq9r167o1asXNm3ahJ49e6J3795ISkrC999/jxYtWlgnPbdVln7x0UcfoUePHpg1axZ+/PFHdOjQARcuXMDmzZsxatQofPfdd2V+r3fy0UcfoVevXpgxYwa2b9+O9u3b49KlS9i4cSNGjx6Nb775xukE0UREZebWe5AREVUhd7pNOkq4RbSzW8KmpaWJl156STRt2lSoVCoRFBQk7rnnHvHDDz843YdOpxMLFiwQjRo1EiqVSjRs2FC8+uqrIi8vr9hjm0wm8d///lcMGDBAhISECKVSKSIiIkSvXr3E22+/La5evVrq9+eKO90GvqRbaWdnZ4tXX31VNGnSRKjValGvXj0xY8YMkZyc7PSW7JZjzZ8/3+n+in72165dE6+88oro2bOnqFWrllCpVKJu3bpi6NChYvv27Q7bF3cbeIsffvhBDBo0SAQHBwuVSiWaNGkiXnzxRae3U77Tvkpa7+w22haW2xfHx8c7rDt58qR4/PHHRYMGDYRKpRIhISGiTZs24sknn3S43fyd2ldaf/zxh3jvvffE0KFDRZMmTYSvr69QqVSifv36YuzYseLbb7912MZyi+SSltGjR5e5TTt27BDjx48XdevWFUql0rrPli1biiNHjtxxe1duA1/c71Bxn29ubq54++23RZs2bYRGoxH+/v6iV69eYt26dS4fo6LPU/n5+eL9998XrVq1EhqNRtSqVUs88sgjIjEx0WmfdOU28CtWrBBjxowRjRo1Ej4+PiIkJERERUWJxYsXO9ymXgghMjMzxdtvvy06deok/Pz8hEajEZGRkWL48OHi448/Flqt1q5+RkaGeP7550VERIRQq9WiRYsW4r333hMXL16s9NvAl6Y/F719d1JSkpgzZ45o06aN8PHxEX5+fqJp06bivvvuE1988YXdLd1L+p23uH79unj66adFgwYNhFKpFGFhYWL06NHi0KFDxW7z3XffiREjRogaNWoIpVIpatWqJbp27Srmzp0rzp49a623fv16MWHCBNG0aVPh5+cnAgICRJs2bcSrr74qbt26VabPzPY9rV69WnTo0EFoNBoRHh4unnjiCXHjxg2HbVJSUsT06dNFw4YNhVqtFo0bNxavvPKKyM7OdtqfhShbvzh//ry47777RFBQkPD19RXdu3cXW7dutf6cXbkNfHG3bS/ud/Ps2bNi7NixDsd+9913BQCxZcuWYj5RIiLXSUJU8gX6RERERHfZX3/9hejoaOsIM8s8LETkHrGxsViwYAHi4+PRv39/dzenynv44Yexbt06nDt37o53OSQiKi2OKSQiIiKvU7duXWzduhUAMGzYMIc5R4iI3M1kMjm909iuXbuwfv16tGnThuEPEVUozgFEREREXikqKgpbt25FfHw89u3bh6ioqLtye3UiotLQ6/WoX78+YmJi0LJlSygUCvz+++/YuXMn1Go1li9f7u4mEpGXYQBERFRNbdmypVQTLkdGRhY7aS95h+PHj2PLli2lqutsstuq3I5+/fqhX79+ZW8UeTxXznWJiYl3rBccHIxZs2aVu12ejv+GlJ9SqcSMGTMQHx+Pw4cPQ6vVIjw8HPfffz9effVVdOjQwd1NJCIvwzmAiIiqqSeeeAJr1qy5Y71+/fo53IaXvMvq1asxadKkUtWtzD8bqko7yLu4cq4rzW2/GzZsWKqgyNvx3xAiIs/DAIiIiIiIiIiIyMtxEmgiIiIiIiIiIi9XLeYAMplM0Gq1UKlUnPyRiIiIiIiIiLyGEAJ6vR7+/v6QyYof51MtAiCtVosPPvjA3c0ot3r16uH69evubgZRidhPyROwn5InYD8lT8B+Sp6A/ZQ8QUX00+effx6BgYHFrq8WcwDl5eVh8eLFeP7556FWq93dnDI7evQoOnfu7O5mEJWI/ZQ8AfspeQL2U/IE7KfkCdhPyROUp5/qdDp88MEHmDNnDjQaTbH1qsUIIMtlX2q12qMDILlc7tHtp+qB/ZQ8AfspeQL2U/IE7KfkCdhPyRNURD+905Q3nASaiIiIiIiIiMjLMQAiIiIiIiIiIvJyDICIiIiIiIiIiLwcAyAiIiIiIiIiIi/HAIiIiIiIiIiIyMtVi7uAEREREREREZVEq9UiLy/P3c2gakqr1SI5OdnpOo1GA39//3IfgwEQERERERERVWtarRZfffUVDAaDu5tC1ZROp8PFixedrlMoFHjggQfKHQIxAPIAGTn5WLH3IkKy09Gtm4AkSe5uEhERERERkdfIy8uDwWBATEwMQkJC3N0cqoays7Ph5+fnUJ6Wlob4+Hjk5eUxAKoO1EoZPvv1Eh5vasC11Fw0CPN1d5OIiIiIiIi8TkhICMLDw93dDKqGKuoyr5JwEmgPoFHK0b5eMAAg4XKKextDRERERERERB6HAZCH6NYoFABw6HKqm1tCRERERERERJ6GAZCHsARAhxMZABERERERERGRaxgAeYjODUMgAUhMycGtTN6akIiIiIiIyJtJklSqZc+ePeU6TmxsbIXfaEiSJMycObNC90nlx0mgPUSgRokAjQKAwKHEVIxsH+HuJhEREREREXm9vHwjrqbmlHn7BqG+0CjlLm934MABu9dvvvkm4uPjsXv3brvy1q1bl7ltADB16lQMHTq0XPsgz8AAyIME+yoB6HHoMgMgIiIiIiKiu+Fqag4Gf/Bzmbf/8fm+aF4rwOXtunfvbve6Ro0akMlkDuVF5eTkwNe39HeOrlevHurVq+dy+8jz8BIwDxLsqwLAiaCJiIiIiIgI6N+/P9q2bYuff/4ZPXv2hK+vLyZPngwAWL9+PQYPHow6derAx8cHrVq1wssvv4zs7Gy7fTi7BCwyMhIjR47Ejh070KlTJ/j4+KBly5ZYtWpVhbU9NTUVM2bMQN26daFSqdC4cWPMnTsXOp3Ort6GDRsQHR2NoKAg+Pr6onHjxtb3CAAmkwlvvfUWWrRoAR8fHwQHB6N9+/ZYunRphbXVW3AEkAcxjwAC/kjKQkZOPoIKXhMREREREVH1dPPmTTzyyCN46aWX8K9//QsymXmcx/nz5zF8+HDMmjULfn5+OHfuHBYvXoxDhw45XEbmzIkTJ/DPf/4TL7/8MmrVqoVPP/0UU6ZMQdOmTdG3b99ytTkvLw8xMTG4ePEiFixYgPbt2+OXX37BwoULcfz4cWzbtg2A+TK4Bx98EA8++CBiY2Oh0Whw5coVu/a/8847iI2NxWuvvYa+ffsiPz8f586dQ3p6erna6I0YAHkQtUKOxjX8cOl2No5cScXAVrXc3SQiIiIiIiJyo9TUVGzYsAEDBgywK3/ttdesz4UQ6NWrF1q1aoV+/frh5MmTaN++fYn7TU5Oxr59+9CgQQMAQN++fbFr1y6sW7eu3AHQmjVrcPLkSXz11Ve4//77AQCDBg2Cv78/5syZg507d2LQoEHYv38/hBBYuXIlgoKCrNs/8cQT1uf79u1Du3btEBsbay0bMmRIudrnrXgJmIfpFmm+HTwvAyMiIiIiIqKQkBCH8AcALl26hIceegi1a9eGXC6HUqlEv379AABnz569436joqKs4Q8AaDQaNG/eHFeuXCl3m3fv3g0/Pz+MHz/ertwS7OzatQsA0LVrVwDAAw88gK+++gp//fWXw766deuGEydOYMaMGfjhhx+QmZlZ7vZ5KwZAHqZbI3MAlMAAiIiIiIiIqNqrU6eOQ5lWq0WfPn2QkJCAt956C3v27MHhw4exadMmAEBubu4d9xsWFuZQplarS7XtnaSkpKB27doOcw/VrFkTCoUCKSkpAMyjjrZs2QKDwYDHHnsM9erVQ9u2bREXF2fd5pVXXsF7772HgwcPYtiwYQgLC8PAgQNx5MiRcrfT2zAA8jCWAOj0XxnI0Rvc3BoiIiIiIiJyp6IhCmAeYXPjxg2sWrUKU6dORd++fdGlSxcEBLh+N7LKEBYWhqSkJAgh7Mpv3boFg8GA8PBwa9no0aOxa9cuZGRkYM+ePahXrx4eeughHDhwAACgUCgwe/Zs/Pbbb0hNTUVcXByuXbuGIUOGICcn566+r6qOAZCHqRfii4ggDQwmgWNX093dHCIiIiIiIqpiLKGQWq22K//444/d0RwHAwcOhFarxZYtW+zK//vf/1rXF6VWq9GvXz8sXrwYAHDs2DGHOsHBwRg/fjyeeeYZpKamIjExscLb7sk4CbQH6tYoFFuO38Chy6no1TT8zhsQERERERFRmTQI9cWPz5d90uMGob4V2JrS6dmzJ0JCQvD0009j/vz5UCqV+N///ocTJ07ctTZcvHgRX3/9tUN569at8dhjj+Gjjz7C448/jsTERLRr1w6//vor/vWvf2H48OG45557AADz5s3D9evXMXDgQNSrVw/p6elYunSp3XxGo0aNQtu2bdGlSxfUqFEDV65cwZIlS9CwYUM0a9bsrr1fT8AAyAN1tQmAiIiIiIiIqPJolHI0r1U1Lp0qrbCwMGzbtg3//Oc/8cgjj8DPzw+jR4/G+vXr0alTp7vShh07dmDHjh0O5fPnz0dsbCzi4+Mxd+5cvPvuu7h9+zbq1q2LF154AfPnz7fWjY6OxpEjRzBnzhzcvn0bwcHB6NKlC3bv3o02bdoAAGJiYrBx40Z8+umnyMzMRO3atTFo0CC8/vrrUCqVd+W9egoGQB4oumAeoGPX0qA3mKBS8Eo+IiIiIiIib7Z69WqsXr3armzPnj3F1u/Rowf279/vUF503p3Y2Fi7W6gDKPbSqZKOV9IxnAkNDcWKFSuwYsWKYuuMGDECI0aMKHE/s2fPxuzZs0vVruqOyYEHalLDH6F+KuTlm3Dqrwx3N4eIiIiIiIiIqjgGQB5IkiR0jQwBAF4GRkRERERERER3VKYASKvVYtasWYiIiIBGo0FUVBS+/PLLO26XlZWFl156CYMHD0aNGjUgSZLDULOy1K2OukaaLwM7nMgAiIiIiIiIiIhKVqYAaNy4cVizZg3mz5+P77//Hl27dsXEiROxbt26ErdLSUnBJ598Ap1OhzFjxlRY3eooulEYAHMAZDTd+fpKIiIiIiIiIqq+XJ4Eevv27di5cyfWrVuHiRMnAjDPun3lyhW8+OKLePDBByGXy51u27BhQ6SlpUGSJCQnJ+PTTz8t9jiu1K2OWtUJgL9agaw8A/74OwutIwLd3SQiIiIiIiIiqqJcHgG0efNm+Pv74/7777crnzRpEm7cuIGEhIRit5UkCZIkleo4rtStjhRyGTo1tMwDlOLm1hARERERERFRVeZyAHT69Gm0atUKCoX94KH27dtb19PdYbkd/OHENDe3hIiIiIiIiIiqMpcvAUtJSUHjxo0dykNDQ63r3U2n00Gn09m9NhgMbmxR5bBMBJ1wORVCCI6YIiIiIiIiIiKnXA6AAJQYNFSFEGLhwoVYsGCBXdmoUaNw9OjRYucn8gRpaWl2l9gZTQJPtjTCJHIQ/8t++KnL9OMkqlBF+ylRVcR+Sp6A/ZQ8AfspeYLS9FOtVgudTofs7GxoNJq71DKiQkajEVqt1qE8OzsbOp0OJ06cgL+/f7HblobLiUFYWJjTUT6pqebbkVtGArnTK6+8gtmzZ1tf63Q6LFu2DJ07d4ZarXZjy8onISEB0dHRdmVLThzAocRUNGtbHwO6NnBTy4gKOeunRFUN+yl5AvZT8gTsp+QJStNPk5OTcfHiRfj5+RX7JZuoMmm1Wqd9Ly8vD2q1Gh06dEB4eLjTbXU6HXbu3HnHY7g8B1C7du1w9uxZh0uqTp06BQBo27atq7uscGq1GoGBgXZL0TmLvEW3RoWXgREREREREREROeNyADR27FhotVps3LjRrnzNmjWIiIjg/wDcZV2tE0EzACIiIiIiIvIWY8eOhY+PD9LT04ut8/DDD0OpVCIpKanU+5UkCbGxsdbXe/bsgSRJ2LNnzx23feKJJxAZGVnqY9lavnw5Vq9e7VCemJgISZKcrqtssbGxkCQJycnJd/3Y7uDysJhhw4Zh0KBBmD59OjIzM9G0aVPExcVhx44dWLt2rXWOnb1792LgwIGYN28e5s2bZ93++++/R3Z2NrKysgAAZ86cwddffw0AGD58OHx9fctUt7rq3DAEMgm4lpqLmxm5qBPk4+4mEREREREReY/8PCDtctm3D2kEKF2fV2jKlCnYsmUL1q1bhxkzZjisz8jIwObNmzFy5EjUqlWrzM3r1KkTDhw4gNatW5d5H6WxfPlyhIeH44knnrArr1OnDg4cOIAmTZpU6vGpjJNAb9q0CXPnzsW8efOQmpqKli1bIi4uDhMmTLDWEULAaDTCZDLZbTt9+nRcuXLF+nrDhg3YsGEDAODy5ct2aaIrdasrf7UCbesG4eT1DBy6nIrRUXXd3SQiIiIiIiLvkXYZWN697NvPOAjUbOXyZsOGDUNERARWrVrlNACKi4tDbm4upkyZUva2AQgMDET37uV4f+WkVqvdevzqxOVLwADA398fS5cuxc2bN62zUduGPwDQv39/CCHshpYB5uFdQginS9FAx5W61ZnldvCHOA8QERERERGRV5DL5Xj88cdx9OhR65y7tj7//HPUqVMHw4YNw+3btzFjxgy0bt0a/v7+qFmzJgYMGIBffvnljscp7hKw1atXo0WLFlCr1WjVqhX++9//Ot1+wYIFiI6ORmhoKAIDA9GpUyd89tlnEEJY60RGRuL333/H3r17IUkSJEmyfqcv7hKwX3/9FQMHDkRAQAB8fX3Rs2dPbNu2zaGNkiQhPj4e06dPR3h4OMLCwjBu3DjcuHHjju+9tL799lv06NEDvr6+CAgIwKBBg3DgwAG7Ordv38aTTz6J+vXrQ61Wo0aNGujVqxd++ukna51jx45h5MiRqFmzJtRqNSIiIjBixAhcv369wtpakjIFQFS1dOM8QERERERERF5n8uTJkCQJq1atsis/c+YMDh06hMcffxxyudx6V+758+dj27Zt+Pzzz9G4cWP079+/VHP7FLV69WpMmjQJrVq1wsaNG/Haa6/hzTffxO7dux3qJiYm4qmnnsJXX32FTZs2Ydy4cfjHP/6BN99801pn8+bNaNy4MTp27IgDBw7gwIED2Lx5c7HH37t3LwYMGICMjAx89tlniIuLQ0BAAEaNGoX169c71J86dSqUSiXWrVuHd955B3v27MEjjzzi8vt2Zt26dRg9ejQCAwMRFxeHzz77DGlpaejfvz9+/fVXa71HH30UW7Zswbx58/Djjz/i008/xT333GO9i3p2djYGDRqEpKQkfPTRR9i5cyeWLFmCBg0aWKe9qWzeeWusasYyAujPJC1Ss/UI9VO5uUVERERERERUXk2bNkXfvn2xdu1avPPOO1AqlQBgDYQmT54MAGjRogWWL19u3c5oNGLIkCFITEzEsmXL0L9//1If02QyYe7cuejUqRM2b94MSZIAAL1790azZs0QERFhV//zzz+329ZyNdDSpUvx+uuvQ5IkdOzYET4+PqW+3Ozll19GSEgI9uzZY701+siRIxEVFYUXXngBDzzwgLVdADB06FAsW7bM+jo1NRUvvfQS/v77b9SuXbvU793ZZ/Hiiy+iXbt2+P777yGTmcfQDB8+HE2aNMGcOXOwb98+AMC+ffswdepUTJs2zbr96NGjrc/PnTuHlJQUfPbZZ3blDzzwAADzbeArG0cAeYFQPxWa1TT/UnAUEBERERERkfeYMmUKkpOT8e233wIADAYD1q5diz59+qBZs2bWeitXrkSnTp2g0WigUCigVCqxa9cunD171qXj/fHHH7hx4wYeeughu5ClYcOG6Nmzp0P93bt345577kFQUBDkcjmUSiXmzZuHlJQU3Lp1y+X3m52djYSEBIwfP94a/gDmS+IeffRRXL9+HX/88YfdNvfee6/d6/bt2wOA3ZzCZWH5LB599FFr+AOYp8W57777cPDgQeTk5AAAunXrhtWrV+Ott97CwYMHkZ+fb7evpk2bIiQkBHPmzMHKlStx5syZcrWtLBgAeQnr7eA5DxAREREREZHXGD9+PIKCgqwjbbZv346kpCS7yZ///e9/Y/r06YiOjsbGjRtx8OBBHD58GEOHDkVubq5Lx7NcsuRs5EzRskOHDmHw4MEAgP/85z/Yt28fDh8+jLlz5wKAy8cGgLS0NAghUKdOHYd1ltFHljZahIWF2b1Wq9VlPr4ty3GKa4vJZEJaWhoAYP369Xj88cfx6aefokePHggNDcVjjz2Gv//+GwAQFBSEvXv3IioqCq+++iratGmDiIgIzJ8/3yEsqiy8BMxLRDcKxbqEqzjEEUBERERERERew8fHBxMnTsR//vMf3Lx5E6tWrUJAQADuv/9+a521a9eif//+WLFihd22ZZlbxhKmWIILW0XLvvzySyiVSmzduhUaTeGt7rds2eLycS1CQkIgk8lw8+ZNh3WWiZ3Dw8PLvH9XWD6L4toik8kQEhJibdOSJUuwZMkSXL16Fd9++y1efvll3Lp1Czt27AAAtGvXDl9++SWEEDh58iRWr16NN954Az4+Ppg5c2alvx+OAPISlnmAfr+RCa3O4ObWEBERERERUUWZMmUKjEYj3n33XWzfvh0TJkyAr6+vdb0kSdZRLxYnT550uFNVabRo0QJ16tRBXFyc3Z28rly5gv3799vVlSQJCoUCcrncWpabm4svvvjCYb9qtbpUI3L8/PwQHR2NTZs22dU3mUxYu3Yt6tWrh+bNm7v8vsqiRYsWqFu3LtatW2f3WWRnZ2Pjxo3WO4MV1aBBA8ycORODBg3Cb7/95rBekiR06NABH3zwAYKDg53WqQwcAeQlIoJ9UC/EB9fTcvHblTT0bV7D3U0iIiIiIiLyfCGNgBkHy7d9OXXp0gXt27fHkiVLIISwu/wLME+Q/Oabb2L+/Pno168f/vjjD7zxxhto1KgRDAbXBgjIZDK8+eabmDp1KsaOHYtp06YhPT0dsbGxDpeAjRgxAv/+97/x0EMP4cknn0RKSgree+89hzAKKBz9sn79ejRu3BgajQbt2rVz2oaFCxdi0KBBiImJwQsvvACVSoXly5fj9OnTiIuLs5ubqCJ89913CAgIcCgfP3483nnnHTz88MMYOXIknnrqKeh0Orz77rtIT0/HokWLAAAZGRmIiYnBQw89hJYtWyIgIACHDx/Gjh07MG7cOADA1q1bsXz5cowZMwaNGzeGEAKbNm1Ceno6Bg0aVKHvpzgMgLxIt0ahuJ72Fw4npjIAIiIiIiIiqghKDVCzlbtbgSlTpuC5555D69atER0dbbdu7ty5yMnJwWeffYZ33nkHrVu3xsqVK7F58+Yy3QbeEjAtXrwY48aNQ2RkJF599VXs3bvXbn8DBgzAqlWrsHjxYowaNQp169bFtGnTULNmTYeQasGCBbh58yamTZuGrKwsNGzYEImJiU6P369fP+zevRvz58/HE088AZPJhA4dOuDbb7/FyJEjXX4/d2K5m1pRQgg89NBD8PPzw8KFC/Hggw9CLpeje/fuiI+Pt06KrdFoEB0djS+++AKJiYnIz89HgwYNMGfOHLz00ksAgGbNmiE4OBjvvPMObty4AZVKhRYtWmD16tV4/PHH78pdwCRhO47JS+l0OixatAgvv/yy0yTSUyQkJDj8otv68tBVvLzpFLo1CsVXT/W4iy0jKnSnfkpUFbCfkidgPyVPwH5KnqA0/TQ5ORmbNm3CuHHj7tr8MkS2tFqt3V3PLErTN0ubeXAOIC/SreBOYMevpUNnMLq5NURERERERERUVTAA8iKNwv0Q7q+C3mDCyesZ7m4OEREREREREVURDIC8iCRJ1lFAhy7zdvBEREREREREZMYAyMtYbgfPAIiIiIiIiIiILBgAeRnLCKCjV9JgNHn9/N5EREREREREVAoMgLxMy9qBCFAroNUZcPZmprubQ0RERERERERVAAMgLyOXSegSGQIASOBlYEREREREREQEQOHuBlDF69YoDPF/3Mbhy6mY0ruRu5tDRERERETkEdLS0tzdBKqmsrOzkZeX51BekX2SAZAX6tbIPALocGIqhBCQJMnNLSIiIiIiIqq6NBoNFAoF4uPj3d0UqqZ0Oh3UarXTdQqFAhqNptzHYADkhdrVDYZaIUNKth4Xb2ejaU1/dzeJiIiIiIioyvL398cDDzzgdAQG0d1w4sQJdOjQwek6jUYDf//yf69nAOSFVAoZOjYIxsFLqTh0OZUBEBERERER0R34+/tXyJdsorLw9/dHeHh4pR6Dk0B7qW6NwgAAhy6nuLklRERERERERORuDIC8VLfIUADA4UROYkZERERERERU3TEA8lKdGgZDIZPwV3ourqfluLs5RERERERERORGDIC8lK9KgTZ1gwCY7wZGRERERERERNUXAyAvFt3IfBnYocsMgIiIiIiIiIiqMwZAXswyDxADICIiIiIiIqLqjQGQF+sSGQIAuHg7G8lanZtbQ0RERERERETuwgDIiwX7qtCydgAA4DBHARERERERERFVWwyAvFxXy2VgnAiaiIiIiIiIqNpiAOTlunEiaCIiIiIiIqJqjwGQl7MEQGdvZiIzL9/NrSEiIiIiIiIid2AA5OVqBWrQMMwXJgEcvZLm7uYQERERERERkRswAKoGLPMAcSJoIiIiIiIiouqJAVA1wHmAiIiIiIiIiKo3BkDVQHRBAHTyegby8o1ubg0RERERERER3W0MgKqBBqG+qBmght5owvFr6e5uDhERERERERHdZQyAqgFJkngZGBEREREREVE1xgComrAEQIcTGQARERERERERVTcMgKoJSwB09EoaDEaTm1tDRERERERERHcTA6BqonnNAAT5KJGjN+L3G5nubg4RERERERER3UUMgKoJmUxC18gQAJwHiIiIiIiIiKi6YQBUjXSNLJgImvMAEREREREREVUrDICqEduJoE0m4ebWEBEREREREdHdwgCoGmlbNwg+SjnSc/Jx4bbW3c0hIiIiIiIiorukTAGQVqvFrFmzEBERAY1Gg6ioKHz55Zd33C4rKwsvvfQSBg8ejBo1akCSJMTGxlb4ccg5pVyGTg2DAQAJnAeIiIiIiIiIqNooUwA0btw4rFmzBvPnz8f333+Prl27YuLEiVi3bl2J26WkpOCTTz6BTqfDmDFjKu04VLxukWEAOBE0ERERERERUXWicHWD7du3Y+fOnVi3bh0mTpwIAIiJicGVK1fw4osv4sEHH4RcLne6bcOGDZGWlgZJkpCcnIxPP/20Uo5DxevayHwnsMOXUyGEgCRJbm4REREREREREVU2l0cAbd68Gf7+/rj//vvtyidNmoQbN24gISGh2G0lSSp14FCe41DxOtYPgVIu4e/MPFxLzXV3c4iIiIiIiIjoLnA5ADp9+jRatWoFhcJ+8FD79u2t6ytCeY6j0+mQmZlptxgMhgppl6fzUcnRrm4QAN4OnoiIiIiIiKi6cPkSsJSUFDRu3NihPDQ01Lq+IpTnOAsXLsSCBQvsykaNGoWjR4969GVjaWlpFTLyaVy9XHTyMSLr6hkkGP6qgJYRFaqofkpUmdhPyROwn5InYD8lT8B+Sp6gPP3UaDSWqp7LARCAEi/jqsg5Zcp6nFdeeQWzZ8+2vtbpdFi2bBk6d+4MtVpdYe272xISEhAdHV3u/WSfS8Jrq4+gUbgK8WPLvz8iWxXVT4kqE/speQL2U/IE7KfkCdhPyROUp5/qdDrs3LnzjvVcDoDCwsKcjr5JTTVfTmQZoVNe5TmOWq22C3p0Op3DpWTVWeeGoZAk4HJyNm5l5qFmoMbdTSIiIiIiIiKiSuTyHEDt2rXD2bNnHebUOXXqFACgbdu2FdKwu3Wc6ijIR4lWtQMBcB4gIiIiIiIiourA5QBo7Nix0Gq12Lhxo135mjVrEBERUWFD6+7Wcaqrbo3MI6gOX2YAREREREREROTtXL4uatiwYRg0aBCmT5+OzMxMNG3aFHFxcdixYwfWrl1rnWR57969GDhwIObNm4d58+ZZt//++++RnZ2NrKwsAMCZM2fw9ddfAwCGDx8OX19fl45DZdOtUShW709EAgMgIiIiIiIiIq9XpolxNm3ahLlz52LevHlITU1Fy5YtERcXhwkTJljrCCFgNBphMpnstp0+fTquXLlifb1hwwZs2LABAHD58mVERka6dBwqm66R5hFAfyRlISMnH0G+Sje3iIiIiIiIiIgqS5kCIH9/fyxduhRLly4ttk7//v0hhHAoT0xMrNDjUNnUCFCjcbgfLiVn48iVVAxsVcvdTSIiIiIiIiKiSuLyHEDkPSyjgDgRNBEREREREZF3YwBUjVkmgj7EeYCIiIiIiIiIvBoDoGrMEgCdup6BXL3Rza0hIiIiIiIiosrCAKgaqxfigzpBGhhMAseuprm7OURERERERERUSRgAVWOSJFlHAfF28ERERERERETeiwFQNWeZCPowJ4ImIiIiIiIi8loMgKq56IIRQL9dTYPeYHJza4iIiIiIiIioMjAAquaa1vRHiK8SefkmnL6R4e7mEBEREREREVElYABUzUmSZL0MjLeDJyIiIiIiIvJODIDIOhH0YQZARERERERERF6JARBZA6BDiakwmoSbW0NEREREREREFY0BEKF1nUD4qeTIyjPgj7+z3N0cIiIiIiIiIqpgDIAICrkMnRqGAODt4ImIiIiIiIi8EQMgAlB4O3hOBE1ERERERETkfRgAEQAU3gksMRVCcB4gIiIiIiIiIm/CAIgAAB3qB0Mll+F2lg6JKTnubg4RERERERERVSAGQAQA0Cjl6FA/CABvB09ERERERETkbRgAkZXldvAJDICIiIiIiIiIvAoDILKyzAPEO4EREREREREReRcGQGTVuWEIZBJwNTUHNzNy3d0cIiIiIiIiIqogDIDIKkCjROuIQAC8HTwRERERERGRN2EARHa6RYYB4GVgRERERERERN6EARDZsUwEzRFARERERERERN6DARDZ6RoZAgD4M0mLtGy9m1tDRERERERERBWBARDZCfNXo2lNfwC8DIyIiIiIiIjIWzAAIge8HTwRERERERGRd2EARA6iOQ8QERERERERkVdhAEQOuhYEQKdvZCJbZ3Bza4iIiIiIiIiovBgAkYO6wT6oG+wDo0ngt6tp7m4OEREREREREZUTAyByireDJyIiIiIiIvIeDIDIKQZARERERERERN6DARA5ZQmAjl1Lh85gdHNriIiIiIiIiKg8GACRU43D/RDur4LeYMKp6xnubg4RERERERERlQMDIHJKkiR0jTSPAkrgZWBEREREREREHo0BEBXLEgAdTmQAREREREREROTJGABRsSzzAB1JTIPRJNzcGiIiIiIiIiIqKwZAVKxWdQIRoFZAqzPg7M1MdzeHiIiIiIiIiMqIARAVSy6T0DkyBABvB09ERERERETkyRgAUYks8wAxACIiIiIiIiLyXAyAqETRjQonghaC8wAREREREREReSIGQFSidvWCoFbIkJKtx8Xb2e5uDhERERERERGVAQMgKpFaIUdU/WAAvB08ERERERERkadiAER3ZLkMjPMAEREREREREXmmMgVAWq0Ws2bNQkREBDQaDaKiovDll19W+LaHDh3CkCFDEBAQAH9/f8TExGDfvn1laTKVQ1cGQEREREREREQerUwB0Lhx47BmzRrMnz8f33//Pbp27YqJEydi3bp1Fbbt4cOH0bdvX+Tm5uKLL77AF198gby8PAwcOBAHDhwoS7OpjDo1CIFcJuGv9FxcT8txd3OIiIiIiIiIyEUKVzfYvn07du7ciXXr1mHixIkAgJiYGFy5cgUvvvgiHnzwQcjl8nJv+/rrryM4OBg7duyAr68vAOCee+5B48aN8cILL3Ak0F3kp1agbUQgTlzPwOHEVNQL8XV3k4iIiIiIiIjIBS6PANq8eTP8/f1x//3325VPmjQJN27cQEJCQoVsu2/fPvTv398a/gBAQEAA+vbti/379+PmzZuuNp3KoZv1MrA0N7eEiIiIiIiIiFzlcgB0+vRptGrVCgqF/eCh9u3bW9dXxLZ6vR5qtdphH5ayU6dOudp0KoeukZYAKMXNLSEiIiIiIiIiV7kcAKWkpCA0NNSh3FKWklJ8QODKtq1bt8bBgwdhMpmsZQaDwTpKqKTj6HQ6ZGZm2i0Gg+EO74xKYgmALt7ORrJW5+bWEBEREREREZErXJ4DCAAkSSrTOle2/cc//oEpU6Zg5syZmDt3LkwmExYsWIArV64AAGSy4rOrhQsXYsGCBXZlo0aNwtGjR4udn8gTpKWllXiJXWWb1V6CVmfAgQMHUTNQ47Z2UNXm7n5KVBrsp+QJ2E/JE7CfkidgPyVPUJ5+ajQaS1XP5QAoLCzM6eib1FTzLcKdjfApy7aTJ0/G7du38dZbb2HFihUAgB49euCFF17A4sWLUbdu3WKP88orr2D27NnW1zqdDsuWLUPnzp2dXlbmKRISEhAdHe2243938xTWnryKyeE1MG9Qa7e1g6o2d/dTotJgPyVPwH5KnoD9lDwB+yl5gvL0U51Oh507d96xnsuXgLVr1w5nz551uKTKMidP27ZtK2zbOXPmIDk5GadOnUJiYiL279+PtLQ0+Pn5oXPnzsUeR61WIzAw0G4pOu8Qua5bozAAwKFEzgNERERERERE5ElcDoDGjh0LrVaLjRs32pWvWbMGERERJSZWZdlWrVajbdu2aNiwIa5evYr169dj2rRp8PHxcbXpVE7dCuYBOnMjE/svJLu5NURERERERERUWi4Pixk2bBgGDRqE6dOnIzMzE02bNkVcXBx27NiBtWvXWufY2bt3LwYOHIh58+Zh3rx5Lm0LmO8ItnHjRnTp0gVqtRonTpzAokWL0KxZM7z55psV9PbJFbWDNBjUuhZ2nknCE58fxrKJHTG0bW13N4uIiIiIiIiI7sDlEUAAsGnTJjz66KOYN28ehg4dioSEBMTFxeHhhx+21hFCwGg02t3Fq7TbAoBKpcLu3bvx2GOPYcSIEVi5ciWefvpp7NmzB/7+/mVpNlWADx/qiKFtakNvNGHG/47iq8PX3N0kIiIiIiIiIrqDMk2M4+/vj6VLl2Lp0qXF1unfvz+EEGXaFgCaN2+OvXv3lqV5VInUCjk+ergTXt10CuuPXMNLG08iIzcf0/o2dnfTiIiIiIiIiKgYZRoBRNWbXCZh0X3t8FQ/c+jz9vazWLzjnNPAj4iIiIiIiIjcjwEQlYkkSXhlWCu8PKwlAGDFnot4dfNpGE0MgYiIiIiIiIiqGgZAVC5P92uCRePaQSYBcYeu4tm4Y9AZjO5uFhERERERERHZYABE5TahWwN89FAnqOQybDt1E1PXHEG2zuDuZhERERERERFRAQZAVCGGtauDVU90ha9Kjl/OJ+PhTxOQnqN3d7OIiIiIiIiICAyAqAL1bhaOddO6I9hXiePX0vHAxwfwd0aeu5tFREREREREVO0xAKIKFVU/GBue6oHagRr8maTF+JX7cTk5293NIiIiIiIiIqrWGABRhWtWKwAbnu6BRuF+uJ6Wi/tX7sfvNzLc3SwiIiIiIiKiaosBEFWK+qG++OqpHmhdJxDJWj0mfHwQhy6nurtZRERERERERNUSAyCqNDUC1Pjyqe7oFhmKLJ0Bj36WgN3nktzdLCIiIiIiIqJqhwEQVapAjRL/ndINA1vWhM5gwrT/HsWWY3+5u1lERERERERE1QoDIKp0GqUcKx/tjLEd68JoEpi1/jhW77vs7mYRERERERERVRsMgOiuUMpleP/+DniiZyQAIPa7M1jy058QQri3YURERERERETVAAMgumtkMgnzR7XG7EHNAQBLfjqP2G9/h8nEEIiIiIiIiIioMjEAortKkiQ8O7AZ3hjdBgCw5sAVzP7qOPKNJje3jIiIiIiIiMh7MQAit3isRySWToiCQiZhy/EbeOqLo8jVG93dLCIiIiIiIiKvxACI3GZ0VF3857Eu0Chl2H3uFh5blYCM3Hx3N4uIiIiIiIjI6zAAIreKaVkTX0yJRoBGgcOJaZjwyUHcyspzd7OIiIiIiIiIvAoDIHK7rpGhWP9kD4T7q3H2ZibuX3kA11Jz3N0sIiIiIiIiIq/BAIiqhNYRgfj66R6oF+KDKyk5GL9yP/74O8vdzSIiIiIiIiLyCgyAqMqIDPfDxuk90byWP5IydXjg4wP47Wqau5tFRERERERE5PEYAFGVUitQg6+e6oGODYKRkZuPh/+TgF/O33Z3s4iIiIiIiIg8GgMgqnKCfVX439Ro9GkWjtx8IyavPoxtJ2+6u1lEREREREREHosBEFVJvioFPnu8K0a0r4N8o8DMuN+wLuGqu5tFRERERERE5JEYAFGVpVLIsGxCRzwU3QBCAK9uPoXley5ACOHuphERERERERF5FAZAVKXJZRLeHtMWz8Q0AQC8s+MPLPz+HEMgIiIiIiIiIhcwAKIqT5IkvDikJV4b0QoA8MnPl/DS1ydhMJrc3DIiIiIiIiIiz8AAiDzG1D6N8c749pBJwIaj1zHtv0dwJSXb3c0iIiIiIiIiqvIYAJFHeaBLfax4pDNUChni/7iNAe/vxZyvT+Jaao67m0ZERERERERUZTEAIo8zpE1tbJreE/2a14DRJLD+yDXEvLcHr2w6hb/Sc93dPCIiIiIiIqIqhwEQeaS2dYOwZnI3bJzeE32ahcNgEog7dBX9343Ha1tO4WYGgyAiIiIiIiIiCwZA5NE6NwzBF1Oi8dVTPdCjcRjyjQJrD15Fv3f2IPbb35GUmefuJhIRERERERG5HQMg8grdGoUi7snuiJvWHd0ahUJvNGH1/kT0fSceb3x3BreyGAQRERERERFR9cUAiLxKjyZhWP9kd/xvajQ6NwyBzmDCqn2X0fedeLy97QyStTp3N5GIiIiIiIjormMARF5HkiT0ahqOr5/ugf9O7oao+sHIyzfhP79cRp/F8Vj0/TmkZuvd3UwiIiIiIiKiu4YBEHktSZLQt3kNbJ7RE59P6or29YKQm2/Eyr0X0Wfxbrz7wzmk5zAIIiIiIiIiIu/HAIi8niRJiGlRE9880wufPtYFbSICka034qP4i+i9OB7/3vknMnLz3d1MIiIiIiIiokrDAIiqDUmScE/rWtj6j974+NHOaFk7AFqdAct2nUfvxbux9KfzyMxjEERERERERETehwEQVTuSJGFIm9rY/mwfrHi4E5rX8kdWngEf/PQn+iyOx4e7z0OrM7i7mUREREREREQVhgEQVVsymYRh7epgx3N98X8TO6JJDT9k5ObjvR//RJ/Fu7Fiz0VkMwgiIiIiIiIiL8AAiKo9mUzCqA4R+PH5flg6IQqNw/2QlpOPxTvOoe878fjk54vI1Rvd3UwiIiIiIiKiMmMARFRALpMwOqoufny+L/79QAc0DPNFSrYe/9p+Dn3eicenv1xCXj6DICIiIiIiIvI8DICIilDIZRjXqR52ze6Hd8a3R/1QHyRrdXhr21n0fSceq/ddZhBEREREREREHoUBEFExFHIZHuhSH7v/2R+LxrVD3WAf3MrSIfa7M+j/7h58cSAROgODICIiIiIiIqr6yhQAabVazJo1CxEREdBoNIiKisKXX35Z4dseO3YMY8aMQUREBHx9fdGyZUu88cYbyMnJKUuzicpEKZdhQrcGiH+hP94a0xZ1gjT4OzMPr3/zO2Le3YPV+y7jVmaeu5tJREREREREVCxFWTYaN24cDh8+jEWLFqF58+ZYt24dJk6cCJPJhIceeqhCtj1z5gx69uyJFi1aYMmSJQgPD8fPP/+MN954A0ePHsU333xTlqYTlZlKIcMj3Rvi/i71sP7wNXwUfwE3MvIQ+90ZxH53Bh0bBGNw69oY3KYWmtTwd3dziYiIiIiIiKxcDoC2b9+OnTt3WoMbAIiJicGVK1fw4osv4sEHH4RcLi/3tuvWrUNeXh42btyIJk2aAAAGDBiAmzdv4pNPPkFaWhpCQkLK9KaJykOtkOOxHpF4oEt9rD98DZuP/YXj19Jx7Kp5WbzjHJrU8MPgNrUxuHUtdKgXDJlMcneziYiIiIiIqBpz+RKwzZs3w9/fH/fff79d+aRJk3Djxg0kJCRUyLZKpRIAEBQUZFc3ODgYMpkMKpXK1aYTVSiNUo7He0ZiyzO9kPDqQLw1pi36NAuHQibh4u1srNhzEWOX70ePRbvw2pZT+PnP29AbTO5uNhEREREREVVDLgdAp0+fRqtWraBQ2A8eat++vXV9RWz7+OOPIzg4GNOnT8elS5eQlZWFrVu34uOPP8YzzzwDPz+/Yo+j0+mQmZlptxgMBlffKlGp1QrU4JHuDfHFlGgcfX0Qlk6Iwoj2deCnkiMpU4e1B6/isVWH0PnNnXg27hi2nrwBrY59koiIiIiIiO4Oly8BS0lJQePGjR3KQ0NDresrYtvIyEgcOHAAY8eOtV4CBgDPPvsslixZUmIbFy5ciAULFtiVjRo1CkePHi328jRPkJaWVuIIK6o6agN4rAnwcKNgpOXocStLh+QsHXQGHZB3DcePXsPJ34BQPxVqBKhRI0ANtcJz+6Yt9lPyBOyn5AnYT8kTsJ+SJ2A/JU9Qnn5qNJbu7tRlmgRakoqfz6Skda5sm5iYiFGjRqFWrVr4+uuvUaNGDSQkJOCtt96CVqvFZ599Vux+XnnlFcyePdv6WqfTYdmyZejcuTPUanWJ7avKEhISEB0d7e5mUBmZTALHrqXjx9//xg+//43ElBwARgA5kKQcdGoQgsGta2Fwm9poFF78CLeqjv2UPAH7KXkC9lPyBOyn5AnYT8kTlKef6nQ67Ny58471XA6AwsLCnI7ySU1NBVA4mqe827788svIzMzE8ePHrZd79e3bF+Hh4Zg8eTIee+wx9OvXz+lx1Gq1XdCj0+kcLjsjuttkMgmdG4agc8MQvDysJS7c0uLHM0n48fe/ceJ6Bo5eScPRK2lY+P05NK/lb72jWLu6QXcMVomIiIiIiIhK4nIq0q5dO8TFxcFgMNiFKqdOnQIAtG3btkK2PX78OFq3bu0w10/Xrl0BmOcLKi4AIqrqJElCs1oBaFYrAM/ENMXNjFz8dCYJP55JwoGLKfgzSYs/ky7gw/gLqBOkwaDWtTC4dW1ENw6FUu7y1F1ERERERERUzbn8TXLs2LHQarXYuHGjXfmaNWsQERFR4pAlV7aNiIjA77//Dq1Wa1f3wIEDAIB69eq52nSiKqtOkA8e7RFpnkT6tUFY8mAUhrerDV+VHDcz8vDfA1fwyGcJ6PzmTsz68hi+P3UT2ZxEmoiIiIiIiErJ5RFAw4YNw6BBgzB9+nRkZmaiadOmiIuLw44dO7B27VrrJMt79+7FwIEDMW/ePMybN8+lbQFg1qxZGDNmDAYNGoTnn38e4eHhOHjwIBYuXIjWrVtj2LBhFfQREFUtQb5KjOlYF2M61kVevhH7Lybjx9+TsPNMElKy9dhy/Aa2HL8BlUKGPk3DMbhNLQxsVQvh/p47vxURERERERFVrjJNjLNp0ybMnTsX8+bNQ2pqKlq2bIm4uDhMmDDBWkcIAaPRCJPJ5PK2AHDvvfdi165dWLRoEZ577jlkZGSgfv36eOqpp/DKK69ApVKVpelEHkWjlGNAy1oY0LIW3h4rcOxqGn48k4Qffv8bV1JysOvcLew6dwuSdAptIgLRONwfkeF+aBTui8gwPzQK90OwL39XiIiIiIiIqrsyBUD+/v5YunQpli5dWmyd/v37QwhRpm0tYmJiEBMTU5YmEnkduUxCl8hQdIkMxSvDWuLPJC1+/P1v/HgmCaf+ysDpvzJx+q9Mh+2CfZXWMCgyzA+R4b7m5+F+CNQo3fBOiIiIiIiI6G7jrbGIPJAkSWhROwAtagfgHwOb4UZ6Lk79lYHE5GwkpmTjcnI2EpNz8HdmHtJz8nE8Jx3Hr6U77CfMT4XIgmCoUbiv9XlkuB/81Tw9EBEREREReQt+wyPyAhHBPogI9nEoz9EbcCUlB4nJ2bickm0OiJJzcDklG7ezdEjJ1iMlW4+jV9Ictq0RoEajMD80DPMtuKyscASRr4qnDiIiIiIiIk/Cb3FEXsxXpUCrOoFoVSfQYZ1WZ7COGEpMzsbl5Bzr85RsPW5n6XA7S4dDiakO29YKVBdeVmYdQWQOi4iIiIiIiKjqYQDkKT7pD4SOA5K/AsKaAeFNgfDmQGA9QCZzd+vIA/mrFWhbNwht6wY5rMvMyy8IhcwjhqyXlaVkIz0nH0mZOiRl6pBw2T4ckiRgRmuBfx3+FbUCNagVqEHtIE3BczVqB2pQM1CDQI0CkiTdrbdKRERERERU7TEA8gR5mcCNY4CmH3D6U/t1Ch8grAkQVhAIhTcreN4MUAe4p73k8QI1SrSvF4z29YId1qXn6K1hUKLNqKHLydnIzDNAZzDhxPUMABnF7t9HKUftIA1qBqhRO0hjDYZqB2pQO0iNmgHm0EilYLhJRERERERUERgAeQKlDzDlJ+DsVSDieSD5PJByAUi5CBhygaTT5qWogDpFgqGCkUNB9QGZ/O6/D1cJAeiygOzbgDYJ0N4qeH4LyL5lXh9YFwiqCwRGmEdDBdUFVH7ubrlXC/ZVoWMDFTo2CLErF0IgLScfRw8fQnR0M/ydmYekjDwkZeaZn2fmISlTh4zcfOTmG3G5IDQqSZifqiAYUhcERpYRRWrz6KJADUJ8VZDJOJqIiIiIiIioJAyAPIFcCdTvCtwwAdH3FZYbDUD6FXMYlPxnYTCU/Kc5KMm6aV4Sf7Hfn0IDhDYpvIzMEgyFNQM0jnPFVCghgLyMwiBHm2Qf6mhv2z8a8lw/hiaoMAwKrGsTElmWCEDFuWoqmiRJCPVTIdBHiejWtYqtl6s3FoRB9sGQNTDKykNShg56o8k6SfXZm8UfVymXnAZDYf5q+Kvl8FUp4KdWwF+tgK9KDn+1+TVHFxERERERUXXCAMiTyRUFl381AZoPsV+Xm14QBp03B0Ip54HkC0DqRXOocut381KUf22by8hsLikLblD8qCEhgNw0xyBHm+Q81DHqXXufqgDAvwbgV9PmsaZ5XeZfQMZfhY/6LHPAlJfh/P1Z+IQWCYYigKB6hWUBEYBS41o7qVR8VHLzxNHhxY/UsowmsoZEGTYhkc2SrNUj3yjwV3ou/krPdakdSrkEP7UCfioF/NRyu5CosFxhDZEswZGvuiBEKrKdWiHjvEZERERERFRlMQDyVj7BQL0u5sWWyWgeNZR8oSAU+rPwuTYJ0P5tXoqOGpKrC+caUgcUGbFzGzDlu9Y+dSDgV8Mc5PjXLAx1LGW2YY8ro3XyMu1DIWfP87OB3FTzknSq+H35hjsZRVSvICwqCIkUKtfeN5WKZTRRqJ/K6R3MLPQGE25rdfg7I89uVNGtTPMt7nN0Bmh1BmTrDcjRGaHVmecoAoB8o0B6Tj7Sc1zsu8WQyyTrCCPbkUa+KgV8VHL4KGXwUcrho1IUPDq+1ijl8FGaAycfpRwaSx2lHAo5RywREREREVHZMQCqbmRyILSxecFg+3V5GTbBkGXkUMFcQ0YdcOuMeSmOJqiYIKdmkVCnhnleo8qgCTQvNVs5Xy8EkJcOZN4oCIau2z+3hEWGPCAn2bzcPFH88fxqAn7hgEJtDskUqoJHNSBXFfNoW09lviSvaFmx+7BZL1eab7tVjakUMtQN9kHd4NL3p3yjCTl6I7J1BvNS8FyrMyBHb4BWZ35tDo8K6ukL6uqM1udanRE5egNy9EYAgNEkkJVnQFaeoXLeq1wGjVIGH5U5IDKHRbKCcMk+ZNKo5PBVKqwhk0Yph49KDrVCDrXCHDSpFTKolTLHMoWMYRMRERERkRdiAESFNEFAvc7mxZbJCKRfLbykLD/HPtDxr2UOdRRq97TbFZIE+ISYl1ptnNexXNKWURAO2QZDmTcKy4068yio7Ft39z1YSQXBkMYcCmmCgOCGQEgkEGJ5jDSX+QS7qY1Vj1IuQ5CPDEE+ygrZn9EkkJtvEyIVjDQqDI6MyMs3IjffiFx9wWO+EXkFz3MKHvMK1ufobernGyGE+Th6owl6owmZeQYAugppe3HkMskhFFIr5NBYAiNlYZltiGR5rrEtU8igVsqhKXhUyWVQyCUoZBKU1ucyKOUSFHIZlDLzo1wmmcsK1vHyOiIiIiKi8mEARHcmkwOhjcxLs0Hubk3lkyTAN9S81GnvvI4QQE6KOQzKTTPPa2TIAwx6czBk0BWU2T7qbNYXVy+v5DrCaNsI83qjzpwHZN82h3TOaIIdQyHL86D6vJStHOQyCf4F8wAVP/V12QghoDOY7IIj63Ob4ChHb/86V29Ejk3IZAma9AYTdAYTdAYjdPm2j+ZwycJoEsgpCKOqCrmsaGhkDocU8oKyguBIWWRdYahkHzC1VWbip21n7OZ78lPL4acqnOfJMveTb0G5nHebIyIiIiIPxgCIqCwkyXzpl1/43T2uyVh8mJSdbJ7fKe0KkJZoXtKvmIOhvHTgZrrzy9kkmXmOI9tQyBIWBTc0j/bi6Au3kCQJmoJLuEIq+VgmkygMhwwma0CUl29TVhAY5dkER8WWGUzIy7ffl64ggDIYTTCYBAxGAYPJhHyjgMFoQr7J/GgSju0zmgSMBW2sCMEtjPj0j8subaNRyqzBkDk4ktsFR5bn1uDIMidUkcnELRONc+JwIiIiIrqbGAAReRKZvGBSbBcmxtZpzZfw2YZCaYmFQZEhF8i4Zl6u/Oq4vdK3IBxyNoKoIaAq/m5e5DlkMsk8j5CqmLv93UUmk0C+yQSjSVjDIYNJIN9osguNjJYyJ+ssz4uGTIaCfQZpE/FkrQiHuaAsj4UTiBthLEik8vJNyMvXA3DxTobFUBRcaidJEiQAkAAJ5uBPsn0OSwZrWw5IsK8HS7mTdZb9Wx5s92upWxHKG2gpCkbUBWgU8NcoEKA2P/qrlQjQFJQXjLjz1ygQqFFanys5dxURERFRiRgAEXk7tT9Qq7V5KUoI8x3dioZClqAo47p5zqfbZ82LM3417EMhfTNg/yHAZChYjDbPS/u6LNsUeS1XAgG1C5Y6hY/+tQpf+9fi5W9VkEwmQS2r3CAqISEZD0UXM1m8DctleNaJw20nBLcLjuzLc/RGu4nFc2zW5+abL60zmAQMVegyO0+nVsgKQiKlXUgUYBMo+auV9mXWOsqC9QqoFAySqBiWUbgQ5v8c4Qg+IiLyMAyAiKozSQICapmX+t0c1xv05pFBdiOHEguDorx08yVm2beB64fN2zSeBSQsuUtv4A5yU0u+cx0A+IYXBEK1HMOigNqAf23zZXDyipk02quZTOYRZfm55uDQ8qi3eS6MBROXqwGFj/lRWfCo0NgvMvd/Ebe9DC/Ur2LCQvMcS+YwSGcwT/QtYA6bzI8AIGzKAWF5bfMcRdfdYR8osl7AvNJ2fWmY91DKui7sFzDfpU9bMPoqK88AbZ7Nc11+waO5PKvg0RKo6Qwm6LR6JGvLN0JLrTDfXc9uZBVsv+sXjpiyFFlGUhU+t5QXBgSW0VkFe3CyDwk2h4AEYGSdbMzdt7fY0Vp2I7mctcFuBJnz0V/2I8NsRo8V2Z/tMW3bbbu+6Pt2VgdF9ul8P/Z1ZDBBJfKgNuVBiXwohR5KYYDK8hz5UAhLeb65jkkPhU25+TEfCqGHQuihNOkhF4Xr5UIPhcm8Xl7wqDCZy+WmfChMeshQGNgaJBVylCHIVYYgVxmMHGUIchQhBY/ByFEEI1tpfp0tD4ZO7gtAKvw9tfxOwtnvtXC6zv53VUAmSQj0USJQo0SQjxKBPgoE+RQ8LygL0Ch4Z0ciIrJiAERExVOogLAm5sWZ3HTH0UP5dYD2DwIyhfmSNZnCZrnT6wrcJj8P0P4NZP0NZN0s8phkfm7KB3KSzUvSqRI+CMk80skuIHIyusivhvnYVZHJWBDCFAln7J47CWxcqWfIrdg2y1U2IVGRcKjoa7sgqWiw5OR1ThaQuM/JKLL8kkeZGYtbX/p9yI35CDAZEGAymgMxYSr4ViecPEfB84LXd3wuylZfkgHqQEAdYLP4FzwWKVf5O6+rCgDkd//PCkNBaGQNh2wCoqy8/CIhUsG6PD30eTnQ6XJhyMuBUZ8DU74OGuihMeqhyDPCJGQwQAYTZDBADhNkMKKwzAgZjJDDKArLjTb1C5/LIFC2L+DZoUZcuKWt4E+sMglooIcvdPCV8uAH8+Ir6eCLPPhCBz8pD77Ig5+UBx/orOv9YC631LUt95Eq5rLLiqQQegTqkxCoTypVfZ1QIBWBSBUBSBGBSEEgUkUgUkQAUhFoLhOBSEUAUkUgMmEOjCqCv1qBQI3CHBYVBERBToKjQI0SQb725T5KOecqIyLyIgyAiKjsfILNS50OhWUJCUD00+5qkT1nl71ZmEzmO7jZhkPWwMg2LPrb/CU9+5Z5+ftk8fuUZIBfTftgSBNk3t720jZhCQZMRV4X1CnxtaF0dYSpyOu7fKmRQmMOXJS+No++5s/IqDMHdAbbRWcOkWzbadSbl8q4633jWcDeJZWwYw+Wl17+fSh9SxEWFbOoAsyhs6UvGHQFI8ps+omTcoUhD8H5uQgutr7leZ55vUFnfm1LBkBd/rdfEiHJIWRyCEkOSAoISWZ+XlAmJBmETAEUlAtJjmOaBzErYhMAyVrH/Fxmfi6ZwyUhmddb19mUWZ4DMpgkGSBJBWUym0fJ2h4BqfA15OZthQkKQw7kxlwoDDnmxZgDuSEHCkuZ0VwuuTBCzOXPEBKMMhWMMiWMMnXho6QsKFfBIBWuNxSUGyQVDJISBpkKRkkFg0xpV5YvKc3lkhL5UCFfpoRBUiNfUsAA83qDTAnzWCMVAAF/Qzr8jOnwN2TAz5gGP0M6/A1p8DNkwNdgfu1nSIOvIR0qUx7UkgF1kIo6Umqp3qtRUiBXGYIcZTByFQUjjVSWEUfmJUcegMx8GTL0knnRCaTpgHQdkJIHZOkBPRTI1Rmh1eXjRkbenQ9chFIuWcMh2wApUFMQGvkoUSMnB5cPXYVcJtkvkuRYVlCukEuQFVmvkJnLFDIZZDJYH+VOyhQyGWRS+ecds4zGEgBM1uelGGVpKmn0pZmzkXeWcrvRd0XncgOcjha03bbEfVfRwM46ss3mMwPsR7pZ1ts9FvmcbbeBkzqWY4ki+9AZjEjW6iCTJGvfkUkoeG3+TGU2ZVX5sywrIQRMovDzsf0sqPpgAERE1ZNMBviFmZfabYuvZzIBOSlORhHdBLRJha+1Sea/CLV/m5ebx+/aW3GZwsccyqj8CsIZJ0FN0TKVkzJn9Sz7K+tIKKOh8Eu67Zf9Yl87CZGcbmMbAuSZ2xrWzHEUmVxZipFnyjuPTJPfaZRawVLwJd78l6b5y739cxQ8FtQp9XPJtfomA6DXArpM88TxuiybJbNgXZEy23rGgpTOMlpMW7pREVWCJCv4nbAZUSZXFglWTcWErjYBqyj5DnWSMEIyuhbEKkKz4J/6e3nenfsofc3nGJWfOQgs+lrlZz6vWJ8XredYR1JooJAkz/vjVZ9jHmmaXbDkFHm0K0sB9FrIhQH++tvw198u2zFlADSFLwUkCJkSJpkCRkkJg6SAAUoYIIceCuiFHDqhgM4kR55JhlyTHHohRz4UMOjk0OsUyE9XmF8XbJMPBXKFAnnNopH061rIJRNkMBWMlzM/FvdcJhUtEwVj5pyUO9S1eW6zDpCQD3lBUKcwt1EooIPS5rkCeiihFwrre9BDCZ0ofG7ZVi/M2+kt5UJh99qyT8tzS7kJMsAcrxa8p5IeC9+3DAIyqchrm3pymCAVs51cMkEqUi4B1hpG66NUOHrRbjRjwXNR+Lro+qLbmI9w5+DA1UuBK8vUFkY89t1PLm1zp4BIVhBalhQoyWX220qQIGATxBQJIK1BZEFIU1gOwGa7ErdH4TqTMEGJfGiEHj7QwUcyP6pggB7m3w8dlDBKSuRLKuglc1AOSWZus8z2PUuQywrfo0xm/37lts9tPpuin5PcZltL+CSKdBTb8M7udZHQz/wcdk8ctrHuUxR5bb9+Us9IjOlY16U+4ok87t9QIqK7SiYD/GuYlzrti69nMprnQsq6WXiJWdbf5i/HMnlhACBZggBZkdcFdexeFwQEdq/lZdufXGn+YlVF5tYpllwByAtGhFSmhARg4pzKPUZ1YtDbBEhZJS/6omVFgiTLZXq2gYzt5XtOLwe0vcyvuPJi9itT2E9yU1ZCOA+GnI30sx2hV1zIJIxAohbosdHmUkGbxbIfy2V9wlRMPZNjmV09UWR/tnUslwxKNiFN0cVJaKP0rdrnmbtN5QuoGgDBDUpXPz/XJhRKKXi87ViWm14wUjLffOmp5blRb+5HNiQISCY9ZCY9FCjFgDcXfnwJsvp4WPl16Te4mziwoVIZxR0CpYKlKCEKfzBF8yFR5Ifm+Lr4+kXr2kqUPY5xqjhr6KG3C+8KX5sDQ6U5LBRF1psKX+uKbGfZl04o7ddbQ0GHTwEqGKCBDhrkw0fSwRfmcEZTENL4QA8fyXx5suW1ZZ2mYJ2P7TaSOeCxqwM9ZJLrKVy+MAe9OlPB+7G+r4LPSCit4ar1MxM266GATqhs6qigFzZ1C/aRD7k17JVLRigKeo8CJshhhNzmuUIyQgYBRUGPs4S/dttI9tvILdsWxKEKGG2CY6NdPeOlR4GOU13+rDwNAyAiooogkxfOC0RU3ShUgCIU8A11d0vcR5IK5kCqwD+tbicAzaIrbn/kGZQ+QHB981JWQhSGQZZAyPLcaBsWlVSnaKhUpL7RUr8+0GWy+T8cJFnBfz7IbP4TwvaxFOV2dQrG/EgF41okc6AgLCGDzYgVkxCQmfIhmfIhM+rNgZcxH5JJB8mYD8mkh2TUQzIWfV2wmPIBo67gdeFzFKyH7XODruCzKKhTDubLOeU2I0Jl5gBVkgGQQVg+H0myqyds61suKbWMIC24tBMmIyRL4FsQLEuWYFoYIVkuGS8Irh3rljyyUS4JyGEEbCZILxU3BHPJUh5ay67c/QPDfAmwSa6CSaaCkOSQG3WQGXPv+PlWeDtkSgilj/k/Q+Sqgj6tg2TQmR9t2qOUjFDCCD/LdfjVIExNDbzX3U24KxgAERERERF5E0kqCGYr5u6FJUpIAKIr73/NZXBpQNLdZw3bdIXBWdEgx2nIYw5s7vS92q3fu4uObLQ+muAw/6BNsOSwjd0+i33h5HoxF9bfaduL6UDPTebQw6ArfDTkOZZZH3XmEa52j7oS1tnUsTm+JIyQG3IhRzE3y5BkgLK4y/J9iiy+JdQreFQ4ry/JlSX3J6PB5vPIK/JebZc8xzJj0fWWfeidv7ZsY9TbjF63GdHubFS77eXzRS+vl2y2k9ls58K+QyM6lfTpeA0GQERERERERGVxN8O2u60yRja6y98JQNO7NKJSCHMwVjQcsYy0swQzCo35Ua6smEuRy0uuAOT+7m4FVTIv+G0mIiIiIiIiqgIkyRzqyJWAmoEKVS1VekQlERERERERERGVHwMgIiIiIiIiIiIvxwCIiIiIiIiIiMjLMQAiIiIiIiIiIvJyDICIiIiIiIiIiLwcAyAiIiIiIiIiIi/HAIiIiIiIiIiIyMsp3N2Au0EIAQDQ6XRubkn5GI1Gj38P5P3YT8kTsJ+SJ2A/JU/AfkqegP2UPEF5+qllO0v2UZxqEQDp9XoAwAcffODmlpSdwWDAL7/8gj59+kChqBY/NvJA7KfkCdhPyROwn5InYD8lT8B+Sp6govqpXq+HRqMpdr0k7hQReQGTyQStVguVSgVJktzdnDLJzMxEzZo1cevWLQQGBrq7OUROsZ+SJ2A/JU/AfkqegP2UPAH7KXmC8vZTIQT0ej38/f0hkxU/00+1iEBlMpnH/7Kr1Wrro+U5UVXDfkqegP2UPAH7KXkC9lPyBOyn5Akqop+WNPLHgpNAExERERERERF5OQZARERERERERERejgGQh1Cr1Zg/fz6HLVKVxn5KnoD9lDwB+yl5AvZT8gTsp+QJ7lY/rRaTQBMRERERERERVWccAURERERERERE5OUYABEREREREREReTkGQEREREREREREXo4BEBERERERERGRl2MAVMVptVrMmjULERER0Gg0iIqKwpdffunuZhHZ2bNnDyRJcrocPHjQ3c2jaiYrKwsvvfQSBg8ejBo1akCSJMTGxjqty3MsuUtp+ynPr+ROu3fvxuTJk9GyZUv4+fmhbt26GD16NI4ePepQl+dTcqfS9lWeU8ldjh8/jhEjRqBBgwbw8fFBaGgoevTogbVr1zrUrczzqaJC9kKVZty4cTh8+DAWLVqE5s2bY926dZg4cSJMJhMeeughdzePyM6//vUvxMTE2JW1bdvWTa2h6iolJQWffPIJOnTogDFjxuDTTz8tti7PseQurvRTgOdXco8VK1YgJSUFzz33HFq3bo3bt2/j/fffR/fu3fHDDz9gwIAB1ro8n5I7udJXAZ5T6e5LT09H/fr1MXHiRNStWxfZ2dn43//+h0cffRSJiYl47bXXrHUr9XwqqMratm2bACDWrVtnVz5o0CAREREhDAaDm1pGZC8+Pl4AEBs2bHB3U4iEyWQSJpNJCCHE7du3BQAxf/58h3o8x5I7lbaf8vxK7pSUlORQlpWVJWrVqiUGDhxoLeP5lNyttH2V51SqaqKjo0X9+vWtryv7fMpLwKqwzZs3w9/fH/fff79d+aRJk3Djxg0kJCS4qWVERFWXZSj3nfAcS+5U2n5K5E41a9Z0KPP390fr1q1x7do1axnPp+Rupe2rRFVNeHg4FIrCC7Mq+3zKAKgKO336NFq1amXXIQCgffv21vVEVckzzzwDhUKBwMBADBkyBL/++qu7m0RULJ5jyZPw/EpVRUZGBn777Te0adPGWsbzKVVFzvqqBc+p5C4mkwkGgwG3b9/G8uXL8cMPP2DOnDnW9ZV9PmUAVIWlpKQgNDTUodxSlpKScrebRORUUFAQnnvuOXz88ceIj4/H0qVLce3aNfTv3x8//PCDu5tH5BTPseQJeH6lquaZZ55BdnY25s6day3j+ZSqImd9ledUcrcZM2ZAqVSiZs2aeP7557Fs2TI89dRT1vWVfT7lJNBVXEnDwzl0nKqKjh07omPHjtbXffr0wdixY9GuXTu89NJLGDJkiBtbR1Q8nmOpquP5laqS119/Hf/73//wf//3f+jcubPdOp5PqSoprq/ynEru9uqrr2Lq1Km4desWvvvuO8ycORPZ2dl44YUXrHUq83zKEUBVWFhYmNOELzU1FQCcJoNEVUVwcDBGjhyJkydPIjc3193NIXLAcyx5Kp5fyR0WLFiAt956C2+//TZmzpxpt47nU6pKSuqrzvCcSndTgwYN0KVLFwwfPhwrVqzAk08+iVdeeQW3b98GUPnnUwZAVVi7du1w9uxZGAwGu/JTp04B4K0KqeoTQgDg//xR1cRzLHkynl/pblqwYAFiY2MRGxuLV1991WE9z6dUVdyprxaH51Ryl27dusFgMODSpUsAKv98ygCoChs7diy0Wi02btxoV75mzRpEREQgOjraTS0jurO0tDRs3boVUVFR0Gg07m4OkQOeY8lT8fxKd9Obb76J2NhYvPbaa5g/f77TOjyfUlVQmr7qDM+p5E7x8fGQyWRo3LgxgMo/n3IOoCps2LBhGDRoEKZPn47MzEw0bdoUcXFx2LFjB9auXQu5XO7uJhIBAB566CHrcMbw8HCcP38e77//PpKSkrB69Wp3N4+qoe+//x7Z2dnIysoCAJw5cwZff/01AGD48OHw9fXlOZbcrjT9lOdXcqf3338f8+bNw9ChQzFixAgcPHjQbn337t0B8G9Wcr/S9lWeU8ldnnzySQQGBqJbt26oVasWkpOTsWHDBqxfvx4vvvgiatSoAeAunE8FVWlZWVni2WefFbVr1xYqlUq0b99exMXFubtZRHYWLlwooqKiRFBQkJDL5aJGjRpi7Nix4tChQ+5uGlVTDRs2FACcLpcvX7bW4zmW3Kk0/ZTnV3Knfv36FdtHi36N4PmU3Km0fZXnVHKXVatWiT59+ojw8HChUChEcHCw6Nevn/jiiy8c6lbm+VQSouCCRyIiIiIiIiIi8kqcA4iIiIiIiIiIyMsxACIiIiIiIiIi8nIMgIiIiIiIiIiIvBwDICIiIiIiIiIiL8cAiIiIiIiIiIjIyzEAIiIiIiIiIiLycgyAiIiIiIiIiIi8HAMgIiIiIiIiIiIvxwCIiIiIiIiIiMjLMQAiIiIiIiIiIvJyDICIiIiIiIiIiLwcAyAiIiIiIiIiIi/3/6PkPrmMQmbqAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1400x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Move the model to the device\n",
    "model = model.to(device)\n",
    "\n",
    "# Define optimizer and scheduler\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2.5e-4, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5, verbose=True)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_pearson_correlations = []\n",
    "\n",
    "patience = 5\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "num_epochs = 30\n",
    "\n",
    " # Initialize the Log-Cosh Loss\n",
    "#log_cosh_loss = LogCoshLoss()\n",
    "NN_name='Transformer'\n",
    "\n",
    "\n",
    "model_name='model_'+str(NN_name)+'_'+str(loss_name)\n",
    "\n",
    "checkpoint_path=os.getcwd()+'/'+model_name+'.pth'\n",
    "\n",
    "alternate_freq = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start_time=time.time()\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "    running_train_stft_loss = 0.0\n",
    "    running_train_log_loss = 0.0\n",
    "    running_val_pearson = 0.0\n",
    "    num_val_samples = 0\n",
    "\n",
    "    for inputs, targets in train_loader:\n",
    "        time0=time.time()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Move inputs and targets to the device\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "      \n",
    "        \n",
    "        if (epoch // alternate_freq) % 2 == 0:\n",
    "            huber_loss = nn.SmoothL1Loss()\n",
    "            loss = huber_loss(outputs, targets)\n",
    "            # Use L2 loss \n",
    "#            loss = torch.nn.functional.l1_loss(outputs, targets)\n",
    "#            l2_loss=nn.MSELoss()\n",
    "#            loss=l2_loss(outputs, targets)\n",
    "#            loss=compute_loss_stft(outputs, targets, qtransform)\n",
    "#            loss= compute_loss(outputs,targets)\n",
    "        else:\n",
    "            huber_loss = nn.SmoothL1Loss()\n",
    "            loss = huber_loss(outputs, targets)\n",
    "#            loss= compute_loss(outputs,targets)\n",
    "#            loss = torch.nn.functional.l1_loss(outputs, targets)\n",
    "#            l2_loss=nn.MSELoss()\n",
    "#            loss=l2_loss(outputs, targets)\n",
    "            # Use STFT loss \n",
    "#            loss = compute_loss_stft(outputs, targets,qtransform)\n",
    "#            loss=compute_loss_stft(outputs, targets, qtransform)\n",
    "        #loss = compute_loss(outputs, targets)\n",
    "        #loss=oneD_L1_loss(outputs, targets)\n",
    "        #loss, stft_loss, log_loss=compute_loss(outputs, targets)\n",
    "        \n",
    "\n",
    "        loss.backward()\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_train_loss += loss.item()\n",
    "        #running_train_stft_loss+= stft_loss.item()\n",
    "        #running_train_log_loss+= log_loss.item()\n",
    "    \n",
    "    \n",
    "    avg_train_loss = running_train_loss / len(train_loader)\n",
    "    #avg_train_stft_loss = running_train_stft_loss / len(train_loader)\n",
    "    #avg_train_log_loss = running_train_log_loss / len(train_loader)\n",
    "\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    running_val_stft_loss = 0.0\n",
    "    running_val_log_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            # Move inputs and targets to the device\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "\n",
    "            \n",
    "            if (epoch // alternate_freq) % 2 == 0:\n",
    "                huber_loss = nn.SmoothL1Loss()\n",
    "                loss = huber_loss(outputs, targets)\n",
    "#                l2_loss=nn.MSELoss()\n",
    "#                loss=l2_loss(outputs, targets)\n",
    "#                loss=compute_loss_stft(outputs, targets, qtransform)\n",
    "                # Use L1 loss \n",
    "#                loss = torch.nn.functional.l1_loss(outputs, targets)\n",
    "#                loss= compute_loss(outputs,targets)\n",
    "            else:\n",
    "                huber_loss = nn.SmoothL1Loss()\n",
    "                loss = huber_loss(outputs, targets)\n",
    "#                loss= compute_loss(outputs,targets)\n",
    "#                l2_loss=nn.MSELoss()\n",
    "#                loss=l2_loss(outputs, targets)\n",
    "#                loss = torch.nn.functional.l1_loss(outputs, targets)\n",
    "#                loss=compute_loss_stft(outputs, targets, qtransform)\n",
    "                # Use STFT loss \n",
    "#                loss = compute_loss_stft(outputs, targets,qtransform)\n",
    "            \n",
    "            #loss = compute_loss(outputs, targets)\n",
    "            #loss=oneD_L1_loss(outputs, targets)\n",
    "            #oss, stft_loss, log_loss=compute_loss(outputs, targets)\n",
    "\n",
    "            \n",
    "            running_val_loss += loss.item()\n",
    "            #running_val_stft_loss += stft_loss.item()\n",
    "            #running_val_log_loss += log_loss.item()\n",
    "\n",
    "\n",
    "    avg_val_loss = running_val_loss / len(val_loader)\n",
    "    #avg_val_stft_loss= running_val_stft_loss / len(val_loader)\n",
    "    #avg_val_log_loss= running_val_log_loss / len(val_loader)\n",
    "    \n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    # Convert outputs and targets to numpy arrays for Pearson correlation calculation\n",
    "#     outputs_np = outputs.detach().cpu().numpy()\n",
    "#     targets_np = targets.detach().cpu().numpy()\n",
    "#     for out, target in zip(outputs_np, targets_np):\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         pearson_corr, _ = pearsonr(out, target)\n",
    "#         time8=time.time()\n",
    "#         print('Elapsed7 '+str(time8-time7))\n",
    "#         running_val_pearson += pearson_corr\n",
    "#         num_val_samples += 1\n",
    "\n",
    "#     val_pearson_correlations.append(abs(running_val_pearson) / num_val_samples)\n",
    "\n",
    "\n",
    "\n",
    "    #pint(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {avg_train_loss:.4f},STFT Train Loss: {avg_train_stft_loss:.4f},LOG Train Loss: {avg_train_log_loss:.4f}, Validation Loss: {avg_val_loss:.4f},STFT Validation Loss: {avg_val_stft_loss:.4f},LOG Validation Loss: {avg_val_log_loss:.4f}, Time: {time.time()-start_time:.4f}')\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f} , Time: {time.time()-start_time:.4f}')\n",
    "\n",
    "#    if avg_val_loss < best_val_loss:\n",
    "#        best_val_loss = avg_val_loss\n",
    "#        patience_counter = 0\n",
    "#    else:\n",
    "#        patience_counter += 1\n",
    "#        if patience_counter >= patience:\n",
    "#            print(\"Early stopping\")\n",
    "#            break\n",
    "            \n",
    "    \n",
    "    \n",
    "    # Save checkpoint if validation loss improves\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss_name,\n",
    "            'val_loss': avg_val_loss,\n",
    "        }\n",
    "        \n",
    "        best_checkpoint_filename = checkpoint_path.format('best')\n",
    "        torch.save(checkpoint, best_checkpoint_filename)\n",
    "        best_val_loss = avg_val_loss\n",
    "\n",
    "    fig=plt.figure(figsize=(14, 7))\n",
    "    #print('Total time: '+str(time.time()-start_time))\n",
    "\n",
    "#model_name='model_'+str(NN_name)+'_'+str(loss_name)\n",
    "#torch.save(model.state_dict(), model_name+'.pth')\n",
    "    \n",
    "    \n",
    "gs=gridspec.GridSpec(2,1)\n",
    "ax1 = plt.subplot(gs[0,0])\n",
    "\n",
    "# Plot the training and validation metrics\n",
    "ax1.plot(train_losses, label='Train Loss')\n",
    "ax1.plot(val_losses, label='Validation Loss')\n",
    "ax1.legend()\n",
    "\n",
    "#ax2 = plt.subplot(gs[1,0])\n",
    "#ax2.plot(val_pearson_correlations, label='Validation Pearson')\n",
    "plt.title(model_name)\n",
    "plt.savefig(model_name+'.png', format='png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79edc396-91e9-46f0-b121-dda9ab6cb166",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_autoencoder(model, train_loader, val_loader, loss_name='MSE&STFT',num_epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15b8425-d675-43c1-8aef-9ded387eb71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_CNNLSTM(model, train_loader, val_loader, loss_name='SL_L2&STFT', num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41ac069-2c4a-4114-851a-759539030f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transformer(model, train_loader, val_loader, loss_name='SL_L2&STFT', num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0007f3e4-ceb8-48b9-8856-d77b689b198c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1600, 24, 513])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_stft(y_test_tensor).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67453198-b86a-4f60-b1da-956c46519e5a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# MCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246a2110-f381-42ed-becd-2e3bbf2f9f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pylab as plt\n",
    "import IPython.display as ipd\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "#from utils import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19742c71-e5cd-4a6c-bb67-1ada4e57db11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de9f299-53e0-4d18-a231-0803d22d3d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Prepare DataLoader for training and validation data\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b26e9c-289a-4e40-a1a3-ac3803890d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this choice of layers' parameters is consistent with the sampling rate of 613, need to change it if the sampling rate is changed\n",
    "DEFAULT_LAYERS_PARAMS = [\n",
    " (2, 16, 64, 0),\n",
    " (2, 16, 32, 0),\n",
    " (2, 16, 16, 0),\n",
    " (2, 16, 8, 0),\n",
    " (2, 16, 4, 0),\n",
    " (2, 16, 2, 0),\n",
    " (2, 16, 1, -14)]\n",
    "\n",
    "for i, p in enumerate(DEFAULT_LAYERS_PARAMS):\n",
    "            stride, kernel_size, out_channels, add = p\n",
    "            if i == 0:\n",
    "                l_out = 48\n",
    "            #else:\n",
    "            #    in_channels = DEFAULT_LAYERS_PARAMS[i-1][2]\n",
    "            padding = (kernel_size - stride)//2\n",
    "            l_out=l_out*stride+add\n",
    "            print(l_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9208b269-1985-466b-8569-640f8408a103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this choice of layers' parameters is consistent with the sampling rate of 613, need to change it if the sampling rate is changed\n",
    "DEFAULT_LAYERS_PARAMS = [\n",
    " (2, 16, 64, 0),\n",
    " (2, 16, 32, 0),\n",
    " (2, 16, 16, 0),\n",
    " (2, 16, 8, 0),\n",
    " (2, 16, 4, 0),\n",
    " (2, 16, 2, 0),\n",
    " (1, 16, 1,-11)]\n",
    "\n",
    "for i, p in enumerate(DEFAULT_LAYERS_PARAMS):\n",
    "            stride, kernel_size, out_channels, add = p\n",
    "            if i == 0:\n",
    "                l_out = 90\n",
    "            #else:\n",
    "            #    in_channels = DEFAULT_LAYERS_PARAMS[i-1][2]\n",
    "            padding = (kernel_size - stride)//2\n",
    "            l_out=l_out*stride+add\n",
    "            print(l_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89c6381-3d0b-483e-ac0e-0bdc52fc2dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCNN(nn.Module):\n",
    "    def __init__(self, n_heads=3, layer_params=DEFAULT_LAYERS_PARAMS):\n",
    "        super(MCNN, self).__init__()\n",
    "        self._heads = nn.ModuleList()\n",
    "        for i in range(n_heads):\n",
    "            h = self._create_head(layer_params)\n",
    "            self._heads.append(h)\n",
    "        self.linear = nn.Linear(n_heads, 1)\n",
    "        # Adjust linear layer output dimensions\n",
    "        #self.linear = nn.Linear(4, 1)  \n",
    "        self.act_fn = nn.Softsign()\n",
    "    \n",
    "    def forward(self, x):\n",
    "#         import pdb; pdb.set_trace();\n",
    "        b = x.shape[0]\n",
    "        out = [head(x).reshape(-1, 1) for head in self._heads]\n",
    "        out = torch.cat(out, dim=1)\n",
    "        out = self.linear(out)\n",
    "        out = self.act_fn(out).reshape(b, -1)\n",
    "        return out\n",
    "        \n",
    "    def _create_head(self, layer_params):\n",
    "        layers = []\n",
    "        for i, p in enumerate(layer_params):\n",
    "            stride, kernel_size, out_channels, add = p\n",
    "            if i == 0:\n",
    "                in_channels = 250\n",
    "            else:\n",
    "                in_channels = layer_params[i-1][2]\n",
    "            padding = (kernel_size - stride)//2\n",
    "            m = torch.nn.ConvTranspose1d(in_channels, out_channels, kernel_size + add, stride=stride, padding=padding)\n",
    "            layers.append(m)\n",
    "            layers.append(nn.ELU())\n",
    "            \n",
    "        return nn.Sequential(*layers).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1437e962-3353-4cb2-90dd-e53d7ebbb58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_magnitude(stft_res):\n",
    "    real = stft_res[:, :, :, 0]\n",
    "    im = stft_res[:, :, :, 1]\n",
    "    return torch.sqrt(torch.pow(real, 2) + torch.pow(im, 2))\n",
    "def compute_stft(x):\n",
    "    _window = torch.hann_window(1024).cuda()\n",
    "    stft = torch.stft(x, 1024, win_length=1024, hop_length=64, window=_window, center=False, normalized=True,return_complex=False).cuda().transpose(1,2)\n",
    "    stft /= _window.pow(2).sum().sqrt()\n",
    "    return get_magnitude(stft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e651865-5e7b-499f-81ff-b4caacfe8c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_stft(y, target):\n",
    "    b = y.shape[0]\n",
    "    \n",
    "    stft_y = compute_stft(y).reshape(b, -1)\n",
    "        \n",
    "    stft_target = compute_stft(target).reshape(b, -1)\n",
    "\n",
    "    p1 = torch.norm(stft_target - stft_y, p=2, dim=1)\n",
    "\n",
    "    p2 = torch.norm(stft_target, p=2, dim=1)\n",
    "\n",
    "    sc_loss = torch.mean(p1/(p2+1e-19))\n",
    "    \n",
    "    #log_loss=torch.mean(torch.abs(torch.log(p1+1e-19)-torch.log(p2+1e-19)))\n",
    "    \n",
    "    #tot_loss= 0.5*sc_loss+0.5*log_loss\n",
    "\n",
    "    return sc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a628655-21fb-4a34-bede-67ac8042597e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_qt(y, target):\n",
    "    b = y.shape[0]\n",
    "    \n",
    "    qt_y=qtransform(y).reshape(b, -1).to(torch.float64)\n",
    "        \n",
    "    qt_target = qtransform(target).reshape(b, -1).to(torch.float64)\n",
    "\n",
    "    p1 = torch.norm(qt_target - qt_y, p=2, dim=1)\n",
    "\n",
    "    p2 = torch.norm(qt_target, p=2, dim=1)\n",
    "\n",
    "    sc_loss = torch.mean(p1/(p2+1e-19))\n",
    "\n",
    "    return sc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739913f4-3f5c-4ff4-9fa8-0aa386c03186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grad_histograms(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            plt.figure()\n",
    "            plt.hist(param.grad.cpu().numpy().flatten(), bins=100)\n",
    "            plt.title(f\"Gradient Histogram for {name}\")\n",
    "            plt.xlabel(\"Gradient value\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.show()\n",
    "\n",
    "def print_grad_stats(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            grad = param.grad\n",
    "            print(f\"Layer: {name}\")\n",
    "            print(f\"  Mean: {grad.mean().item()}\")\n",
    "            print(f\"  Max: {grad.max().item()}\")\n",
    "            print(f\"  Min: {grad.min().item()}\")\n",
    "            print(f\"  Std: {grad.std().item()}\\n\")\n",
    "        else:\n",
    "            print(f\"Layer: {name} has no gradients\")\n",
    "            \n",
    "\n",
    "def pearson_correlation(x, y):\n",
    "    # Function to compute Pearson correlation coefficient\n",
    "    return np.corrcoef(x, y)[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1875a8aa-81f1-4871-a46d-0e42e098b15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, lr, steps, batch_size=48,patience=5, min_delta=0.001) :\n",
    "    \n",
    "    # Early stopping parameters\n",
    "    best_val_loss = float('inf')\n",
    "    current_patience = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5, verbose=True)\n",
    "\n",
    "    \n",
    "    train_loss_list=[]\n",
    "    val_loss_list=[]\n",
    "    pearson_valid_list = []\n",
    "    \n",
    "    for p in optimizer.param_groups : p['lr'] = lr\n",
    "    start = time.time()\n",
    "    nan_encountered=False\n",
    "    \n",
    "    \n",
    "    for i in range(steps) :\n",
    "        \n",
    "        start_time=time.time()\n",
    "        \n",
    "        model.train()\n",
    "        t_l = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            \n",
    "            inputs=inputs.cuda()\n",
    "            targets=targets.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            predict = model(inputs)\n",
    "            train_loss = compute_loss_stft(predict, targets)\n",
    "\n",
    "            train_loss.backward()\n",
    "            \n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "            optimizer.step()\n",
    "            speed = (i + 1) / (time.time() - start)\n",
    "            now=time.time()\n",
    "            t=now-start\n",
    "            #t = time_since(start)\n",
    "            t_l += train_loss.item()\n",
    "        avg_train_loss=t_l/len(train_loader)\n",
    "        train_loss_list.append(avg_train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        v_l = 0\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs=inputs.cuda()\n",
    "            targets=targets.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            validate = model(inputs)\n",
    "            val_loss = compute_loss_stft(validate, targets)\n",
    "                \n",
    "            v_l += val_loss.item()\n",
    "            \n",
    "        avg_val_loss=v_l/len(val_loader)\n",
    "        val_loss_list.append(avg_val_loss)\n",
    "        \n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        \n",
    "        #real_vals = targets.detach().cpu().numpy()\n",
    "        #generated_vals = validate.detach().cpu().numpy()\n",
    "        #pearson_corr_valid = pearson_correlation(real_vals.flatten(), generated_vals.flatten())\n",
    "        #pearson_valid_list.append(pearson_corr_valid)\n",
    "        \n",
    "        print(f'Step [{i + 1}/{steps}], Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}, Time: {time.time()-start_time:.4f}')\n",
    "            \n",
    "        \n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'model_MCNN_stft_1000x90_200.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "    # Restore the best model state\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        \n",
    "    # Plotting results\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    #plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_loss_list, label='Train Loss')\n",
    "    plt.plot(val_loss_list, label='Valid Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    #plt.subplot(1, 2, 2)\n",
    "    #plt.plot(pearson_valid_list, label='Pearson Valid')\n",
    "    #plt.xlabel('Epoch')\n",
    "    #plt.ylabel('Pearson Correlation')\n",
    "    #plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return model, train_loss_list, val_loss_list, pearson_valid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0469497a-2a0b-4266-a50f-517eb54448bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = MCNN(4)\n",
    "model = model.cuda()\n",
    "inputs = torch.randn(1, 1000, 90).cuda()\n",
    "target= torch.randn(1, 5000).cuda()\n",
    "output = model(inputs)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb56ebc-cf06-42b9-aea6-9c3db49757bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "model = MCNN(4)\n",
    "model = model.cuda()\n",
    "model = model.double() \n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "for inputs, targets in train_loader:\n",
    "            inputs=inputs.cuda()\n",
    "outputs=model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac86369-0436-411e-bc9b-83b3ad25ebdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Conv1d) or isinstance(m, nn.ConvTranspose1d) or isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "            \n",
    "from torch import optim\n",
    "model = MCNN(4)\n",
    "model = model.cuda()\n",
    "model=model.double()\n",
    "            \n",
    "model.apply(initialize_weights)\n",
    "#model(X_train_tensor.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6561fc8f-82e7-42f2-9c73-3284e5b387c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "train(model, optimizer, lr=1e-4, steps=200, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c449d9f9-915f-4c41-a195-47c599bad545",
   "metadata": {},
   "source": [
    "Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b4836b-00cc-4b00-b326-4fe7d41266ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(correlation_list, bins=30)\n",
    "plt.title(f'Pearson Coefficients-MCNN-STFT 150 epochs. Mean={np.mean(correlation_list):.2f}, STD={np.std(correlation_list):.2f}')\n",
    "plt.savefig('Pearson_Coefficients-MCNN-STFT_150_epochs.png', format='png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fdef40-1856-4b39-9f6f-f9ef022acc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_path='model_MCNN_stft_250x48_300.pth'\n",
    "model.load_state_dict(torch.load(saved_model_path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e09029-f890-4728-8650-cc92382de32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out=model(X_test_tensor.cuda())\n",
    "pearson_tot=0.0\n",
    "num_tests=500\n",
    "sampling_rate=613\n",
    "for i in range(num_tests):\n",
    "    generated=out[i].data.cpu().numpy()\n",
    "    real=y_test_tensor[i].data.cpu().numpy()\n",
    "    \n",
    "    real=real[sampling_rate*4:6*sampling_rate]\n",
    "    generated=generated[sampling_rate*4:6*sampling_rate]\n",
    "    \n",
    "    real, generated= maximise_pearson(real, generated)\n",
    "    #fig=plt.figure(figsize=(14, 7))\n",
    "    #gs=gridspec.GridSpec(2,1)\n",
    "    \n",
    "    #ax1 = plt.subplot(gs[0,0])\n",
    "    #ax1.plot(np.array(real), label='Original TimeSeries')\n",
    "    #ax1.plot(np.array(generated), label='Generated Output')\n",
    "    \n",
    "    #plt.show()\n",
    "    \n",
    "    pearson_corr, _ = pearsonr(generated, real)\n",
    "    pearson_tot+=pearson_corr\n",
    "avg_pearson=pearson_tot/num_tests\n",
    "print(avg_pearson)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b2512e-c4b2-44c5-97a4-6ab5d64c394c",
   "metadata": {},
   "source": [
    "Print sample graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17c89b1-2384-49ff-aa12-7d4ffc662e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "correlation_list=[]\n",
    "sampling_rate=613\n",
    "for i in range(5):\n",
    "    tms_real=TimeSeries(y_test_tensor[i].detach().cpu().numpy(), dt=1/sampling_rate)\n",
    "    q_real=X_test_tensor[i]\n",
    "            \n",
    "    output=model(q_real.reshape(1,250,48))\n",
    "    #output=model(X_test_tensor[i])\n",
    "\n",
    "    tms_generated=TimeSeries(output[0].detach().cpu().numpy(), dt=1/sampling_rate)\n",
    "    q_generated=qtransform(output[0])\n",
    "    \n",
    "    tms_real=tms_real[sampling_rate*4:6*sampling_rate]\n",
    "    tms_generated=tms_generated[sampling_rate*4:6*sampling_rate]\n",
    "    \n",
    "    tms_real, tms_generated= maximise_pearson(tms_real, tms_generated)\n",
    "\n",
    "    correlation, _ = scipy.stats.pearsonr(tms_real, tms_generated)\n",
    "    correlation_list.append(abs(correlation)) \n",
    "\n",
    "    print('Correlation= '+str(correlation))    \n",
    "    fig=plt.figure(figsize=(14, 7))\n",
    "    gs=gridspec.GridSpec(1,1)\n",
    "    \n",
    "    ax1 = plt.subplot(gs[0,0])\n",
    "    ax1.plot(np.array(tms_real), label='Original TimeSeries')\n",
    "    ax1.plot(np.array(tms_generated), label='Generated Output')\n",
    "    ax1.set_title(f'Graph {i+1} - Correlation= {correlation:.2f}')\n",
    "    ax1.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8afc40-b96e-4884-be6e-8b86acef241c",
   "metadata": {},
   "source": [
    "Save sample graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199b381b-2ec4-4f5b-99b8-fd2813852cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_list = []\n",
    "sampling_rate = 613\n",
    "num_graphs = 8  # Number of graphs\n",
    "ncols = 4  # Number of columns (two subplots per row: one for the graph, one for the image)\n",
    "nrows = int(np.ceil(num_graphs))  # Number of rows, as we will use two subplots per graph\n",
    "\n",
    "# Adjust figure size to accommodate both time-series and 2D images\n",
    "fig, axs = plt.subplots(nrows, ncols * 2, figsize=(20, 6 * nrows))  # Wider figure to fit both plots and images\n",
    "\n",
    "# Create dummy lines for the single shared legend\n",
    "line_real, = plt.plot([], [], label='Original TimeSeries', color='blue')  # Empty line for the real time series\n",
    "line_generated, = plt.plot([], [], label='Generated Output', color='orange')  # Empty line for the generated time series\n",
    "\n",
    "for i in range(num_graphs):\n",
    "    tms_real = TimeSeries(y_test_tensor[i].detach().cpu().numpy(), dt=1/sampling_rate)\n",
    "    q_real = X_test_tensor[i]\n",
    "\n",
    "    output = model(X_test_tensor[i].reshape(1, 250, 48))\n",
    "    tms_generated = TimeSeries(output[0].detach().cpu().numpy(), dt=1/sampling_rate)\n",
    "    q_generated = qtransform(output[0])\n",
    "\n",
    "    # Truncate time series\n",
    "    tms_real = tms_real[sampling_rate*4:5*sampling_rate]\n",
    "    tms_generated = tms_generated[sampling_rate*4:5*sampling_rate]\n",
    "\n",
    "    # Maximise Pearson correlation\n",
    "    tms_real, tms_generated = maximise_pearson(tms_real, tms_generated)\n",
    "\n",
    "    # Calculate Pearson correlation\n",
    "    correlation, _ = scipy.stats.pearsonr(tms_real, tms_generated)\n",
    "    correlation_list.append(abs(correlation))\n",
    "\n",
    "    # Plot the time-series in the left subplot\n",
    "    row = i  # We only need to calculate the row since `ncols` is 2x the number of graphs\n",
    "    ax_graph = axs[row, 0]  # Left subplot for the time-series graph\n",
    "\n",
    "    ax_graph.plot(np.array(tms_real), color='blue')  # Real time-series\n",
    "    ax_graph.plot(np.array(tms_generated), color='orange')  # Generated time-series\n",
    "    ax_graph.set_title(f'Graph {i+1} - Correlation= {correlation:.2f}')\n",
    "\n",
    "    # Plot the 2D images in the right subplot\n",
    "    ax_image = axs[row, 1]  # Right subplot for the 2D image\n",
    "\n",
    "    ax_image.imshow(np.abs(q_real - q_generated), cmap='inferno')  # Difference between q_real and q_generated\n",
    "    ax_image.set_title(f'2D Image Difference {i+1}')\n",
    "\n",
    "# Remove any extra subplots if num_graphs < nrows * ncols * 2\n",
    "for j in range(num_graphs * 2, nrows * ncols * 2):\n",
    "    fig.delaxes(axs.flatten()[j])\n",
    "\n",
    "# Add a single legend for the entire figure\n",
    "fig.legend(handles=[line_real, line_generated], loc='upper center', ncol=2, bbox_to_anchor=(0.5, 1.05), frameon=False)\n",
    "\n",
    "# Adjust layout to minimize overlap and improve spacing\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust to make space for the legend\n",
    "\n",
    "# Save the entire figure with all subplots (both time-series and 2D images)\n",
    "plt.savefig('MCNN_STFT+log_250x48_300e.png', dpi=300, bbox_inches='tight')  # Save figure\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ccba4c-4100-41cc-8d57-41e69e2d0123",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_list = []\n",
    "sampling_rate = 613\n",
    "num_graphs = 8  # Set number of graphs\n",
    "ncols = 4  # Set 4 columns for better width utilization\n",
    "nrows = int(np.ceil(num_graphs / ncols))  # Automatically adjust number of rows\n",
    "\n",
    "# Adjust the figure size to be wider and less tall\n",
    "fig, axs = plt.subplots(nrows, ncols, figsize=(16, 6))  # Set a more balanced figsize\n",
    "\n",
    "# Create dummy lines for the single shared legend\n",
    "line_real, = plt.plot([], [], label='Original TimeSeries', color='blue')  # Empty line for the real time series\n",
    "line_generated, = plt.plot([], [], label='Generated Output', color='orange')  # Empty line for the generated time series\n",
    "\n",
    "for i in range(num_graphs):\n",
    "    tms_real = TimeSeries(y_test_tensor[i].detach().cpu().numpy(), dt=1/sampling_rate)\n",
    "    q_real = X_test_tensor[i]\n",
    "\n",
    "    output = model(X_test_tensor[i].reshape(1, 250, 48))\n",
    "    tms_generated = TimeSeries(output[0].detach().cpu().numpy(), dt=1/sampling_rate)\n",
    "    q_generated = qtransform(output[0])\n",
    "\n",
    "    # Truncate time series\n",
    "    tms_real = tms_real[sampling_rate*4:6*sampling_rate]\n",
    "    tms_generated = tms_generated[sampling_rate*4:6*sampling_rate]\n",
    "\n",
    "    # Maximise Pearson correlation\n",
    "    tms_real, tms_generated = maximise_pearson(tms_real, tms_generated)\n",
    "\n",
    "    # Calculate Pearson correlation\n",
    "    correlation, _ = scipy.stats.pearsonr(tms_real, tms_generated)\n",
    "    correlation_list.append(abs(correlation))\n",
    "\n",
    "    # Plot in the appropriate subplot\n",
    "    row, col = divmod(i, ncols)  # Calculate row and column index\n",
    "    ax = axs[row, col]  # Select subplot\n",
    "\n",
    "    ax.plot(np.array(tms_real), color='blue')  # Use the same color as in the dummy legend\n",
    "    ax.plot(np.array(tms_generated), color='orange')  # Use the same color as in the dummy legend\n",
    "    ax.set_title(f'Graph {i+1} - Correlation= {correlation:.2f}')\n",
    "\n",
    "# Remove any empty axes if num_graphs is less than nrows * ncols\n",
    "for j in range(num_graphs, nrows * ncols):\n",
    "    fig.delaxes(axs.flatten()[j])\n",
    "\n",
    "# Add a single legend for the entire figure\n",
    "fig.legend(handles=[line_real, line_generated], loc='upper center', ncol=2, bbox_to_anchor=(0.5, 1.05), frameon=False)\n",
    "\n",
    "# Adjust layout to minimize overlap and improve spacing\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust to make space for the legend\n",
    "\n",
    "# Save the entire figure with all subplots\n",
    "plt.savefig('MCNN_STFT+log_250x48_300e.png', dpi=300, bbox_inches='tight')  # Save figure\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d8fb49-ef54-45dd-82e1-a2fb01a24983",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505780b5-a868-48ba-96f2-4c08ea2d6429",
   "metadata": {},
   "outputs": [],
   "source": [
    "tms_generated_torch=torch.tensor(tms_generated, dtype=torch.float64).to(device)\n",
    "#qplot_test=tms_real.q_transform(qrange=(5,5),frange=(5,15))\n",
    "spectrogram_generated=qtransform(tms_generated_torch)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(spectrogram_generated.detach().cpu().numpy().T, aspect='auto', origin='lower', cmap='viridis')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Spectrogram')\n",
    "plt.xlabel('Time bins')\n",
    "plt.ylabel('Frequency bins')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacdf73b-e7b6-4212-a730-e4f627662028",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_checkpoint_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692931c1-89f7-4483-b6c0-890006e853d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_path='model_Transformer_SL_QT_TorchNorm.pth'\n",
    "#checkpoint=torch.load(best_checkpoint_filename)\n",
    "checkpoint=torch.load(saved_model_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5126af-302d-4813-a14b-2f8aa5462d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saved_model_path='model_CNNLSTM_STFT_best.pth'\n",
    "#model.load_state_dict(torch.load(saved_model_path))\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce674866-705e-42ae-9609-b37cbdc6b78d",
   "metadata": {},
   "source": [
    "Computing the average Pearson correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7e3238b5-d879-44ff-9614-47a297a166d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49142014892558183"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy\n",
    "correlation_list=[]\n",
    "sampling_rate=125\n",
    "for i in range(300):\n",
    "    tms_real=TimeSeries(y_test_tensor[i].detach().cpu().numpy(), dt=1/sampling_rate)\n",
    "    q_real=X_test_tensor[i]\n",
    "            \n",
    "    output=model(X_test_tensor[i].reshape(1,11,512))\n",
    "\n",
    "    tms_generated=TimeSeries(output[0].detach().cpu().numpy(), dt=1/sampling_rate)\n",
    "    #q_generated=qtransform(output[0])\n",
    "    \n",
    "    tms_real=tms_real[int(sampling_rate*1.5):int(3.5*sampling_rate)]\n",
    "    tms_generated=tms_generated[int(sampling_rate*1.5):int(3.5*sampling_rate)]\n",
    "    \n",
    "    tms_real, tms_generated= maximise_pearson(tms_real, tms_generated)\n",
    "\n",
    "    correlation, _ = scipy.stats.pearsonr(tms_real, tms_generated)\n",
    "    correlation_list.append(abs(correlation)) \n",
    "np.mean(correlation_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88a67e1-037d-4e2c-b5ec-b562ca986575",
   "metadata": {},
   "source": [
    "Plotting the best and worst generated signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f1ab52c1-1241-4959-b637-5ca6c0a8f38e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1401273879902128\n",
      "0.7734907297457483\n",
      "0.12415951405429991\n",
      "Correlation= 0.12415951405429991\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(np.std(correlation_list))\n",
    "print(np.max(correlation_list))\n",
    "print(np.min(correlation_list))\n",
    "max_idx=correlation_list.index(np.max(correlation_list))\n",
    "min_idx=correlation_list.index(np.min(correlation_list))\n",
    "\n",
    "sampling_rate=125\n",
    "\n",
    "p=min_idx\n",
    "\n",
    "tms_real=TimeSeries(y_test_tensor[p].detach().cpu().numpy(), dt=1/sampling_rate)\n",
    "q_real=X_test_tensor[p]\n",
    "\n",
    "output=model(X_test_tensor[p].reshape(1,11,512))\n",
    "np_generated=output[0].detach().cpu().numpy()\n",
    "\n",
    "tms_generated=TimeSeries(np_generated, dt=1/sampling_rate)\n",
    "q_generated=qtransform(output[0])\n",
    "\n",
    "tms_real=tms_real[int(sampling_rate*1.5):int(3.5*sampling_rate)]\n",
    "tms_generated=tms_generated[int(sampling_rate*1.5):int(3.5*sampling_rate)]\n",
    "\n",
    "tms_real, tms_generated= maximise_pearson(tms_real, tms_generated)\n",
    "\n",
    "correlation, _ = scipy.stats.pearsonr(tms_real, tms_generated)\n",
    "correlation_list.append(abs(correlation)) \n",
    "\n",
    "print('Correlation= '+str(correlation))    \n",
    "fig=plt.figure(figsize=(14, 7))\n",
    "gs=gridspec.GridSpec(2,1)\n",
    "\n",
    "ax1 = plt.subplot(gs[0,0])\n",
    "ax1.plot(np.array(tms_real), label='Original TimeSeries')\n",
    "ax1.plot(np.array(tms_generated), label='Generated Output')\n",
    "ax1.set_title(f'Correlation= {correlation:.2f}')\n",
    "\n",
    "ax2 = plt.subplot(gs[1,0])\n",
    "ax2.imshow(X_test_tensor[p].detach().cpu().numpy(), vmin=0, vmax=15)\n",
    "\n",
    "ax1.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40741983-78fa-4c02-9078-dc572cc15b8b",
   "metadata": {},
   "source": [
    "Save plots of real and generated timeseries together with real and generated CQTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca24c1c-33f7-42c2-9285-7f28b13fcd2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "correlation_list = []\n",
    "sampling_rate = 500\n",
    "num_graphs = 10  # Number of graphs to generate\n",
    "\n",
    "# Adjust figure size to accommodate both the time-series and the spectrograms\n",
    "fig = plt.figure(figsize=(16, 8 * num_graphs))  # Larger figure size to fit everything\n",
    "\n",
    "# Create the outer GridSpec with as many rows as num_graphs, each row will have 2 columns\n",
    "outer_gs = gridspec.GridSpec(num_graphs, 2, width_ratios=[2, 1])\n",
    "\n",
    "for i in range(num_graphs):\n",
    "    tms_real = TimeSeries(y_test_tensor[i].detach().cpu().numpy(), dt=1/sampling_rate)\n",
    "    q_real = X_test_tensor[i].squeeze(0)\n",
    "    #q_real=compute_stft(y_test_tensor[i].reshape(1,1500))\n",
    "    #[:,75:175]\n",
    "    #q_real=qtransform(y_test_tensor[i].unsqueeze(0))[:,:,delta_cut*50:250-delta_cut*50]\n",
    "\n",
    "    output = model(X_test_tensor[i].reshape(1,70, 250))\n",
    "    \n",
    "    np_generated=output[0].detach().cpu().numpy()\n",
    "    \n",
    "    \n",
    "    tms_generated = TimeSeries(np_generated, dt=1/sampling_rate)\n",
    "    q_generated = qtransform(output[0]).squeeze(0)\n",
    "    #[:,:,delta_cut*50:250-delta_cut*50]\n",
    "    #[:,75:175]\n",
    "    #q_generated=compute_stft(output[0].reshape(1,1500))\n",
    "\n",
    "    # Truncate time series\n",
    "    tms_real = tms_real[int(sampling_rate*1.5):int(3.5*sampling_rate)]\n",
    "    tms_generated = tms_generated[int(sampling_rate*1.5):int(3.5*sampling_rate)]\n",
    "\n",
    "    # Maximise Pearson correlation\n",
    "    tms_real, tms_generated = maximise_pearson(tms_real, tms_generated)\n",
    "\n",
    "    # Calculate Pearson correlation\n",
    "    correlation, _ = scipy.stats.pearsonr(tms_real, tms_generated)\n",
    "    correlation_list.append(abs(correlation))\n",
    "\n",
    "    # Create subplots for this row\n",
    "    # Left: Time-series plot\n",
    "    ax1 = fig.add_subplot(outer_gs[i, 0])\n",
    "    ax1.plot(np.array(tms_real), color='blue', label='Original TimeSeries')\n",
    "    ax1.plot(np.array(tms_generated), color='orange', label='Generated Output')\n",
    "    \n",
    "   # Set the custom tick positions and labels on the x-axis\n",
    "    tick_positions = np.linspace(0, len(tms_real)-1, 5)  # 5 evenly spaced positions\n",
    "    tick_labels = ['3', '4', '5', '6', '7']  # Desired labels\n",
    "    \n",
    "    # Set the ticks and labels\n",
    "    ax1.set_xticks(tick_positions)\n",
    "    ax1.set_xticklabels(tick_labels)\n",
    "\n",
    "    ax1.set_title(f'Graph {i+1} - Correlation= {correlation:.2f}')\n",
    "    ax1.legend(loc='upper right')\n",
    "\n",
    "    # Right: Create a nested GridSpec for stacked spectrograms\n",
    "    gs_right = gridspec.GridSpecFromSubplotSpec(2, 1, subplot_spec=outer_gs[i, 1], hspace=0.4)\n",
    "\n",
    "    # Right (top): Spectrogram of q_real\n",
    "    ax2 = fig.add_subplot(gs_right[0])\n",
    "    q_real_numpy=q_real.detach().cpu().numpy().reshape(70,250)\n",
    "    #q_real_numpy=q_real.detach().cpu().numpy().reshape(8,513)\n",
    "    im1 = ax2.imshow(q_real_numpy, aspect='auto', vmin=0, vmax=15)\n",
    "    \n",
    "    # Set the custom tick positions and labels on the x-axis\n",
    "    tick_positions = np.linspace(0, q_real_numpy.shape[1]-1, 5)  # 5 evenly spaced positions\n",
    "\n",
    "    # Set the ticks and labels\n",
    "    ax2.set_xticks(tick_positions)\n",
    "    ax2.set_xticklabels(tick_labels)\n",
    "    ax2.set_title(f'CQT Real {i+1}')\n",
    "\n",
    "    plt.colorbar(im1, ax=ax2)\n",
    "\n",
    "    # Right (bottom): Spectrogram of q_generated\n",
    "    ax3 = fig.add_subplot(gs_right[1])\n",
    "    q_generated_numpy=q_generated.detach().cpu().numpy()\n",
    "    q_generated_numpy=q_generated_numpy.reshape(70,250)\n",
    "    im2 = ax3.imshow(q_generated_numpy, aspect='auto', vmin=0, vmax=15)\n",
    "    \n",
    "     # Set the custom tick positions and labels on the x-axis\n",
    "    tick_positions = np.linspace(0, q_generated_numpy.shape[1]-1, 5)  # 5 evenly spaced positions\n",
    "\n",
    "    # Set the ticks and labels\n",
    "    ax3.set_xticks(tick_positions)\n",
    "    ax3.set_xticklabels(tick_labels)\n",
    "    \n",
    "    ax3.set_title(f'CQT Generated {i+1}')\n",
    "    plt.colorbar(im2, ax=ax3)\n",
    "\n",
    "# Adjust layout to minimize overlap and improve spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the entire figure with all subplots\n",
    "plt.savefig('Transformer_QTiles_SL_STFT.png', dpi=300, bbox_inches='tight')  # Save figure\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dedc99-dfb6-4069-b555-8548f8707bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_stft(y_test_tensor[0].reshape(1,1500)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ed59ea-e00f-4a56-96fa-b4f4cd6338be",
   "metadata": {},
   "outputs": [],
   "source": [
    "qtransform = SingleQTransform(sample_rate=500, duration=5, q=10, frange=[5,40],spectrogram_shape=(70,250)).to(device)\n",
    "qplt=qtransform(y_test_tensor[1]).detach().cpu().numpy().reshape(70,250)\n",
    "qplt=qplt[:,50:200]\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(qplt, aspect='auto', vmin=0,vmax=15)\n",
    "plt.title('Real')\n",
    "plt.xlabel('Time [pixel]')\n",
    "plt.ylabel('Frequency [pixel]')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcfe14e-95da-4748-91a0-a281ec3d41e9",
   "metadata": {},
   "source": [
    "Compare the real CQTs with the ones of the generated timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e02e39-0625-429d-a9fb-6d601fa1147a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = 500\n",
    "num_graphs = 10  \n",
    "\n",
    "\n",
    "for i in range(num_graphs): \n",
    "    \n",
    "    q_real = X_test_tensor[i].reshape(1000,90).T\n",
    "    output = model(X_test_tensor[i].reshape(1,1, 1000, 90))\n",
    "        \n",
    "    q_generated = qtransform(output[0]).reshape(1000,90).T\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 7))\n",
    "    im0 = axes[0].imshow(q_real.detach().cpu().numpy(), aspect=\"auto\", origin=\"lower\", vmin=0, vmax=10)\n",
    "    axes[0].set_title(\"Real CQT\")\n",
    "    fig.colorbar(im0, ax=axes[0])\n",
    "\n",
    "    im1 = axes[1].imshow(q_generated.detach().cpu().numpy(), aspect=\"auto\", origin=\"lower\", vmin=0, vmax=10)\n",
    "    axes[1].set_title(\"Generated CQT\")\n",
    "    fig.colorbar(im1, ax=axes[1])\n",
    "    \n",
    "    abs_diff=q_real.detach().cpu().numpy()-q_generated.detach().cpu().numpy()\n",
    "\n",
    "    im2 = axes[2].imshow(abs_diff, aspect=\"auto\", origin=\"lower\", vmin=0, vmax=10)\n",
    "    axes[2].set_title(\"Abs Difference\")\n",
    "    fig.colorbar(im2, ax=axes[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfaee90-dc5c-45ed-9635-66a98deb3f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(X_test_tensor[0].reshape(1,1, 1000, 90))\n",
    "compute_stft(output).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ce7438-e8d9-43d0-916f-7df6abb01b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164533e5-11b6-4dd5-bca5-3ef0ef74c6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = 500\n",
    "num_graphs = 5\n",
    "\n",
    "\n",
    "for i in range(num_graphs): \n",
    "    \n",
    "    stft_real=compute_stft(y_test_tensor[i].reshape(1,5000)).reshape(63,513)\n",
    "    output = model(X_test_tensor[i].reshape(1,1, 1000, 90))\n",
    "        \n",
    "    stft_generated = compute_stft(output[0].reshape(1,5000)).reshape(63,513)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 7))\n",
    "    im0 = axes[0].imshow(stft_real.detach().cpu().numpy().T, aspect=\"auto\", origin=\"lower\")\n",
    "    axes[0].set_title(\"Real STFT\")\n",
    "    fig.colorbar(im0, ax=axes[0])\n",
    "\n",
    "    im1 = axes[1].imshow(stft_generated.detach().cpu().numpy().T, aspect=\"auto\", origin=\"lower\")\n",
    "    axes[1].set_title(\"Generated STFT\")\n",
    "    fig.colorbar(im1, ax=axes[1])\n",
    "    \n",
    "    abs_diff=np.abs(stft_real.detach().cpu().numpy()-stft_generated.detach().cpu().numpy())\n",
    "\n",
    "    im2 = axes[2].imshow(abs_diff.T, aspect=\"auto\", origin=\"lower\")\n",
    "    axes[2].set_title(\"Abs Difference\")\n",
    "    fig.colorbar(im2, ax=axes[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57aa3b0-ec38-454c-afb7-3371b853cdde",
   "metadata": {},
   "source": [
    "Plotting the STFT and Q transforms to compare them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cc4aba-71ff-4acd-bfb7-a8b02ac02d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_list = []\n",
    "sampling_rate = 500\n",
    "duration=10\n",
    "num_graphs = 10  # Number of graphs to generate\n",
    "\n",
    "# Adjust figure size to accommodate both the time-series and the spectrograms\n",
    "fig = plt.figure(figsize=(16, 8 * num_graphs))  # Larger figure size to fit everything\n",
    "\n",
    "# Create the outer GridSpec with as many rows as num_graphs, each row will have 2 columns\n",
    "outer_gs = gridspec.GridSpec(num_graphs, 2, width_ratios=[2, 1])\n",
    "\n",
    "for i in range(num_graphs):\n",
    "    tms_real = TimeSeries(y_test_tensor[i].detach().cpu().numpy(), dt=1/sampling_rate)\n",
    "    q_real = X_test_tensor[i]\n",
    "    stft_real=compute_stft(y_test_tensor[i].reshape(1,sampling_rate*duration))\n",
    "\n",
    "    output = model(X_test_tensor[i].reshape(1,1, 250, 48))\n",
    "    tms_generated = TimeSeries(output[0].detach().cpu().numpy(), dt=1/sampling_rate)\n",
    "    q_generated = qtransform(output[0])\n",
    "    stft_generated=compute_stft(output[0].reshape(1,sampling_rate*duration))\n",
    "\n",
    "    gs_left = gridspec.GridSpecFromSubplotSpec(2, 1, subplot_spec=outer_gs[i, 0], hspace=0.4)\n",
    "    \n",
    "    ax0 = fig.add_subplot(gs_left[0])\n",
    "    im0=ax1.imshow(stft_real.detach().cpu().numpy().T, aspect='auto')\n",
    "    ax0.set_title(f'STFT real {i+1}')\n",
    "    plt.colorbar(im0, ax=ax0)\n",
    "    \n",
    "    \n",
    "    ax1 = fig.add_subplot(gs_left[1])\n",
    "    im1=ax1.imshow(stft_generated.detach().cpu().numpy().T, aspect='auto')\n",
    "    ax1.set_title(f'STFT generated {i+1}')\n",
    "    plt.colorbar(im1, ax=ax1)\n",
    "    \n",
    "\n",
    "    # Right: Create a nested GridSpec for stacked spectrograms\n",
    "    gs_right = gridspec.GridSpecFromSubplotSpec(2, 1, subplot_spec=outer_gs[i, 1], hspace=0.4)\n",
    "\n",
    "    # Right (top): Spectrogram of q_real\n",
    "    ax2 = fig.add_subplot(gs_right[0])\n",
    "    im2 = ax2.imshow(q_real.detach().cpu().numpy().T, aspect='auto', vmin=0, vmax=20)\n",
    "    ax2.set_title(f'q_real Spectrogram {i+1}')\n",
    "    plt.colorbar(im2, ax=ax2)\n",
    "\n",
    "    # Right (bottom): Spectrogram of q_generated\n",
    "    ax3 = fig.add_subplot(gs_right[1])\n",
    "    im3 = ax3.imshow(q_generated.detach().cpu().numpy().T, aspect='auto', vmin=0, vmax=20)\n",
    "    ax3.set_title(f'q_generated Spectrogram {i+1}')\n",
    "    plt.colorbar(im3, ax=ax3)\n",
    "\n",
    "# Adjust layout to minimize overlap and improve spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the entire figure with all subplots\n",
    "plt.savefig('STFT_VS_QT_STFTloss_250x48_29e.png', dpi=300, bbox_inches='tight')  # Save figure\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66845a27-ccb9-4c87-bb84-4edfaba8f464",
   "metadata": {},
   "source": [
    "Plotting the real and generated timeseries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161788f0-f927-4870-8690-fb2cd067c533",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sampling_rate=500\n",
    "for i in range(10):\n",
    "    tms_real=TimeSeries(y_test_tensor[i].detach().cpu().numpy(), dt=1/613)\n",
    "    q_real=X_test_tensor[i]\n",
    "            \n",
    "    output=model(X_test_tensor[i].reshape(1,70,250))\n",
    "    tms_generated=TimeSeries(output[0].detach().cpu().numpy(), dt=1/613)\n",
    "    q_generated=qtransform(output[0])\n",
    "    \n",
    "    tms_real=tms_real[sampling_rate*1:4*sampling_rate]\n",
    "    tms_generated=tms_generated[sampling_rate*1:4*sampling_rate]\n",
    "    \n",
    "    tms_real, tms_generated= maximise_pearson(tms_real, tms_generated)\n",
    "\n",
    "    correlation, _ = scipy.stats.pearsonr(tms_real, tms_generated)\n",
    "    correlation_list.append(abs(correlation)) \n",
    "\n",
    "    print('Correlation= '+str(correlation))    \n",
    "    fig=plt.figure(figsize=(14, 7))\n",
    "    gs=gridspec.GridSpec(1,1)\n",
    "    \n",
    "    ax1 = plt.subplot(gs[0,0])\n",
    "    ax1.plot(np.array(tms_real), label='Original TimeSeries')\n",
    "    ax1.plot(np.array(tms_generated), label='Generated Output')\n",
    "    ax1.set_title(f'Graph {i+1} - Correlation= {correlation:.2f}')\n",
    "    ax1.legend()\n",
    "    \n",
    "    #vmin = min(np.min(qplot_test), np.min(q_generated))\n",
    "    #vmax = max(np.max(qplot_test), np.max(q_generated))\n",
    "    \n",
    "    #ax2=plt.subplot(gs[1,0],sharex=ax1)\n",
    "    #im2=ax2.imshow(q_real[0].detach().cpu().numpy(), aspect='auto', origin='lower', extent=[tms_real.times.value.min(), tms_real.times.value.max(),low_freq,high_freq], cmap='viridis', vmin=0, vmax=15)\n",
    "\n",
    "    #im2=ax2.imshow(np.transpose(q_real[0].detach().cpu().numpy()), aspect='auto', origin='lower', cmap='viridis')\n",
    "\n",
    "    #ax3 = plt.subplot(gs[2,0], sharex=ax1)\n",
    "    #im3=ax3.imshow(q_generated.detach().cpu().numpy(), aspect='auto', origin='lower', extent=[tms_generated.times.value.min(), tms_generated.times.value.max(),low_freq,high_freq], cmap='viridis', vmin=0, vmax=15)\n",
    "    #im3=ax3.imshow(abs(np.transpose(q_real[0].detach().cpu().numpy())-np.transpose(q_generated.detach().cpu().numpy())), aspect='auto', origin='lower', cmap='viridis')\n",
    "\n",
    "    #ax3.set_xlabel('Time (s)')\n",
    "    \n",
    "    #fig.savefig(f'subfigure_{i+1}.png')\n",
    "    #plt.close(fig)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423565f1-7994-4169-a4fd-bd5442536902",
   "metadata": {},
   "outputs": [],
   "source": [
    "gw_spectrogram=qtransform(torch.tensor(event)).to(torch.float64)\n",
    "gw_spectrogram=gw_spectrogram.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c112dd-3817-48b5-a3d9-d16db9ec6be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sl_path='/data/datasets/intertwin-dati-aux/gspy_O3a_Scattered_Light_V1'\n",
    "filename=listdir(sl_path)[2]\n",
    "fout=h5.File(sl_path+'/'+filename)\n",
    "event_id=list(fout.keys())[0]\n",
    "signal=fout[event_id]['V1:Hrec_hoft_16384Hz']\n",
    "\n",
    "gps=signal.attrs['t0']\n",
    "tms=TimeSeries(signal, dt=1/signal.attrs['sample_rate'],t0=gps).whiten()\n",
    "event= tms.crop(gps+3, gps+13).resample(500)\n",
    "\n",
    "\n",
    "\n",
    "gw_generated=model(gw_spectrogram.reshape(1,1,70,250)).detach().cpu().numpy().reshape(5000)\n",
    "gw_generated, event=maximise_pearson(gw_generated, event)\n",
    "\n",
    "np_event=np.array(event)\n",
    "np_event=np_event/max(np_event)\n",
    "\n",
    "correlation, _ = scipy.stats.pearsonr(np_event, gw_generated)\n",
    "correlation=np.abs(correlation)\n",
    "fig=plt.figure(figsize=(14, 7))\n",
    "gs=gridspec.GridSpec(1,1)\n",
    "\n",
    "ax1 = plt.subplot(gs[0,0])\n",
    "ax1.plot(np_event, label='Original TimeSeries')\n",
    "ax1.plot(gw_generated, label='Generated Output')\n",
    "ax1.set_title(f'Correlation= {correlation:.2f}')\n",
    "ax1.legend()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108c17c5-005b-41f9-99ba-f3fac1d2f026",
   "metadata": {},
   "outputs": [],
   "source": [
    "qscan=QScan(sample_rate=500, duration=5, qrange=[5,30], frange=[5,40],spectrogram_shape=(70,250)).to(device)\n",
    "qs=qscan.get_qs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5564d6-1beb-4fe6-a4e6-40008136ff85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#qplt=X_test_tensor[0].detach().cpu().numpy()\n",
    "\n",
    "qtransform2 = SingleQTransform(sample_rate=500, duration=5, q=22, frange=[5,40],spectrogram_shape=(70,250)).to(device)\n",
    "glitch=torch.tensor(df_tms.iloc[0]['V1:Hrec_hoft_16384Hz'])\n",
    "qplt_1=qtransform(glitch).reshape(70,250)\n",
    "qplt_2=qtransform2(glitch).reshape(70,250)\n",
    "\n",
    "fig=plt.figure(figsize=(14, 7))\n",
    "gs=gridspec.GridSpec(1,1)\n",
    "\n",
    "#plt.figure(figsize=(6, 6))\n",
    "ax1 = plt.subplot(gs[0,0])\n",
    "ax1.imshow(qplt_1, aspect='auto', vmin=0, vmax=10)\n",
    "#plt.title('Real')\n",
    "#plt.xlabel('Time [pixel]')\n",
    "#plt.ylabel('Frequency [pixel]')\n",
    "ax1.colorbar()\n",
    "\n",
    "ax2 = plt.subplot(gs[1,0])\n",
    "ax2 = plt.subplot(gs[0,0])\n",
    "ax2.imshow(qplt_2, aspect='auto', vmin=0, vmax=10)\n",
    "ax2.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452c9141-e1a3-4e30-a76f-3fca321769b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Comparing STFT and Q Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d77075-9daf-4415-a982-9ca731f7e247",
   "metadata": {},
   "outputs": [],
   "source": [
    "qt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab502cb-6135-400e-8fca-3a45926816f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "qt=padded_tensor[0].detach().cpu().numpy().reshape(11,512)\n",
    "fig, axes=plt.subplots(1,1, figsize=(20,7))\n",
    "im0=axes.imshow(qt, aspect='auto', vmin=0, vmax=5)\n",
    "fig.colorbar(im0, ax=axes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1da4dca-2bec-42fc-b171-3128b404e92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qt=X_test_tensor[0].detach().cpu().numpy()\n",
    "#stft=compute_stft(y_test_tensor[0].reshape(1,2500)).reshape(24,513).detach().cpu().numpy()\n",
    "qt_g=qtransform(model(X_test_tensor[0].reshape(1,11,512))).reshape(11,512).detach().cpu().numpy()\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 7))\n",
    "\n",
    "#plt.figure(figsize=(6, 6))\n",
    "#axes[0] = plt.subplot(gs[0,0])\n",
    "im0=axes[0].imshow(qt, aspect='auto', vmin=0, vmax=5)\n",
    "fig.colorbar(im0, ax=axes[0])\n",
    "#plt.title('Real')\n",
    "#plt.xlabel('Time [pixel]')\n",
    "#plt.ylabel('Frequency [pixel]')\n",
    "#axes[0].colorbar()\n",
    "\n",
    "#ax2 = plt.subplot(gs[1,1])\n",
    "#axes[1] = plt.subplot(gs[0,0])\n",
    "im1=axes[1].imshow(qt_g, aspect='auto', vmin=0, vmax=5)\n",
    "fig.colorbar(im1, ax=axes[1])\n",
    "\n",
    "im2=axes[2].imshow(qt-qt_g, aspect='auto', vmin=0, vmax=5)\n",
    "fig.colorbar(im2, ax=axes[2])\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321d7963-01b2-417b-a280-2112ad575a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "qt1=X_test_tensor[0].reshape(1,11,512)\n",
    "qt2=qtransform(model(X_test_tensor[0].reshape(1,11,512)))\n",
    "((qt1-qt2)/(qt1+1e-19)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fca25f-4d4a-40ad-977d-8e8575faefd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.norm(a1, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1536e03f-29b7-4f9d-9cc7-1cae338f7dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.norm(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ebec355c-4044-497c-899b-f5356cd2c76e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2.0000, 0.0000, 0.0000],\n",
       "         [1.0000, 1.5000, 0.0000],\n",
       "         [3.0000, 3.0000, 1.6667]]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted/(a1+1e-19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "91d5950d-1085-4685-b52c-1c1d68a5c60d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 1., 1., 0., 1., 1., 1.]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9ed8ed12-6173-493f-801d-5a0057bd0f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[2.0000, 0.0000, 0.0000],\n",
      "         [1.0000, 1.5000, 0.0000],\n",
      "         [3.0000, 3.0000, 1.6667]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(4.0556)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1=torch.tensor([[[1,0,0],[1,1,0],[1,1,1]]], dtype=torch.float32)\n",
    "a2=torch.tensor([[[3,0,0],[2,4,0],[4,7,6]]], dtype=torch.float32)\n",
    "weights=torch.tensor([1,2,3])\n",
    "weighted=torch.abs(a1-a2)/weights\n",
    "print(weighted)\n",
    "torch.mean(torch.sum(weighted/(a1+1e-19), dim=2))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#p1=torch.norm(a1,p=2, dim=1)\n",
    "\n",
    "#p2=torch.norm(a2,p=2, dim=1)\n",
    "\n",
    "#torch.mean(p1/(p2+1e-19))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc5cf1c-f505-4d7a-9959-d5fe31f96d4b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Autoencoder Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ea7a2b-a593-42b5-91be-ea995ac58ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_path='model_CNN_AE_MSE&STFT.pth'\n",
    "checkpoint=torch.load(saved_model_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1df26de-d28e-471f-b286-f5aa1588f1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_path='model_CNN_AE_MSE&STFT.pth'\n",
    "model.load_state_dict(torch.load(saved_model_path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e147e70-16e0-406a-acef-cad927df88ef",
   "metadata": {},
   "source": [
    "Define the encoder to generate timeseries from spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc353c57-6a4a-4368-a6b2-694596b4f23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, autoencoder):\n",
    "        super(Encoder, self).__init__()\n",
    "        # Copy the encoder layers from the autoencoder\n",
    "        self.encoder_conv = autoencoder.encoder_conv\n",
    "        self.encoder_linear = autoencoder.encoder_linear\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the encoder convolutional layers\n",
    "        x = x.unsqueeze(1)  # Add the channel dimension (batch, 1, height, width)\n",
    "        conv_out = self.encoder_conv(x)\n",
    "        flattened = torch.flatten(conv_out, start_dim=1)\n",
    "        # Get the 1D latent space (the time series)\n",
    "        latent = self.encoder_linear(flattened)\n",
    "        \n",
    "        # Apply tanh to restrict the latent space between -1 and 1\n",
    "        latent = torch.tanh(latent)\n",
    "        return latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f7adb5-45d8-423b-8994-3b12649c9f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(model).to(device)\n",
    "\n",
    "# Set the encoder in evaluation mode\n",
    "encoder.eval()\n",
    "\n",
    "# Use a 2D spectrogram input (shape: [batch_size, height, width])\n",
    "spectrogram_input = torch.randn(1, 70, 250).double().to(device)  # Example input\n",
    "\n",
    "# Get the 1D latent time series from the encoder\n",
    "with torch.no_grad():  # Disable gradients for inference\n",
    "    time_series_output = encoder(spectrogram_input)\n",
    "\n",
    "print(time_series_output.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33196bc1-f6d9-40d2-abd5-08e946c1bc88",
   "metadata": {},
   "source": [
    "Average pearson correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c2274d-3f77-49ba-9c9a-917b0ed080d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "correlation_list=[]\n",
    "sampling_rate=500\n",
    "for i in range(200):\n",
    "    tms_real=TimeSeries(y_test_tensor[i].detach().cpu().numpy(), dt=1/sampling_rate)\n",
    "    q_real=X_test_tensor[i]\n",
    "            \n",
    "\n",
    "    output=encoder(X_test_tensor[i].reshape(1,70,250))\n",
    "\n",
    "    tms_generated=TimeSeries(output[0].detach().cpu().numpy(), dt=1/sampling_rate)\n",
    "    #q_generated=qtransform(output[0])\n",
    "    \n",
    "    tms_real=tms_real[sampling_rate:4*sampling_rate]\n",
    "    tms_generated=tms_generated[sampling_rate:4*sampling_rate]\n",
    "    \n",
    "    tms_real, tms_generated= maximise_pearson(tms_real, tms_generated)\n",
    "\n",
    "    correlation, _ = scipy.stats.pearsonr(tms_real, tms_generated)\n",
    "    correlation_list.append(abs(correlation)) \n",
    "np.mean(correlation_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b213d5b-c7e6-4dfe-a065-b66b18c28727",
   "metadata": {},
   "source": [
    "Print the best/worst generated signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f572b38-a8c8-47e9-b303-9551b6792738",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.std(correlation_list))\n",
    "print(np.max(correlation_list))\n",
    "print(np.min(correlation_list))\n",
    "max_idx=correlation_list.index(np.max(correlation_list))\n",
    "min_idx=correlation_list.index(np.min(correlation_list))\n",
    "\n",
    "sampling_rate=500\n",
    "\n",
    "p=min_idx\n",
    "\n",
    "tms_real=TimeSeries(y_test_tensor[p].detach().cpu().numpy(), dt=1/sampling_rate)\n",
    "q_real=X_test_tensor[p]\n",
    "\n",
    "output=encoder(X_test_tensor[p].reshape(1,250,70))\n",
    "np_generated=output[0].detach().cpu().numpy()\n",
    "\n",
    "tms_generated=TimeSeries(np_generated, dt=1/sampling_rate)\n",
    "q_generated=qtransform(output[0])\n",
    "\n",
    "tms_real=tms_real[sampling_rate*1:4*sampling_rate]\n",
    "tms_generated=tms_generated[sampling_rate*1:4*sampling_rate]\n",
    "\n",
    "tms_real, tms_generated= maximise_pearson(tms_real, tms_generated)\n",
    "\n",
    "correlation, _ = scipy.stats.pearsonr(tms_real, tms_generated)\n",
    "correlation_list.append(abs(correlation)) \n",
    "\n",
    "print('Correlation= '+str(correlation))    \n",
    "fig=plt.figure(figsize=(14, 7))\n",
    "gs=gridspec.GridSpec(1,1)\n",
    "\n",
    "ax1 = plt.subplot(gs[0,0])\n",
    "ax1.plot(np.array(tms_real), label='Original TimeSeries')\n",
    "ax1.plot(np.array(tms_generated), label='Generated Output')\n",
    "ax1.set_title(f'Correlation= {correlation:.2f}')\n",
    "ax1.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9fcc90-ddf8-4436-8ce0-09c6b99ef140",
   "metadata": {},
   "outputs": [],
   "source": [
    "for spectrograms, target_timeseries in train_loader:\n",
    "    model.train()\n",
    "    spectrograms = spectrograms.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    target_timeseries = target_timeseries.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            \n",
    "    # Forward pass\n",
    "    reconstructed_spectrograms, latent_rep = model(spectrograms)\n",
    "    # Compute loss\n",
    "    recon_loss, latent_loss = loss_function_autoencoder(reconstructed_spectrograms, spectrograms, latent_rep, target_timeseries)\n",
    "\n",
    "\n",
    "    # Compute gradients for each loss component independently\n",
    "    recon_loss.backward(retain_graph=True)\n",
    "    recon_grad_norm = sum(p.grad.norm() for p in model.parameters() if p.grad is not None)\n",
    "    model.zero_grad()\n",
    "\n",
    "    latent_loss.backward(retain_graph=True)\n",
    "    latent_grad_norm = sum(p.grad.norm() for p in model.parameters() if p.grad is not None)\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Dynamically adjust alpha based on gradient norms\n",
    "    alpha = recon_grad_norm / (latent_grad_norm + 1e-8)\n",
    "\n",
    "    # Compute the total loss and update the model\n",
    "    loss = recon_loss + alpha * latent_loss\n",
    "    \n",
    "    # Backpropagation and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a65e77-7edc-436c-8603-5be77b31bc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, conv_output_shape, flattened_size, latent_dim, output_shape=(70, 250)):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # Linear layer to map latent space back to flattened convolution output\n",
    "        self.decoder_linear = nn.Linear(in_features=latent_dim, out_features=flattened_size)\n",
    "        \n",
    "        # Decoder from flattened 2D shape back to the full spectrogram\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Unflatten(1, conv_output_shape),  # Unflatten to match encoder output shape\n",
    "            nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=3, stride=2, padding=1, output_padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=16, out_channels=1, kernel_size=3, stride=2, padding=1, output_padding=(1, 1)),\n",
    "            nn.Sigmoid()  # Assuming input spectrogram is normalized between [0, 1]\n",
    "        )\n",
    "        \n",
    "        self.output_shape = output_shape  # Store the target output shape for cropping\n",
    "    \n",
    "    def forward(self, latent):\n",
    "        # Map latent space back to the flattened 2D shape\n",
    "        decoded = self.decoder_linear(latent)\n",
    "        \n",
    "        # Unflatten and decode the feature map into a spectrogram\n",
    "        reconstructed = self.decoder(decoded)\n",
    "        \n",
    "        # Optionally crop the output to ensure exact shape (70, 250)\n",
    "        reconstructed = reconstructed[:, :, :self.output_shape[0], :self.output_shape[1]]\n",
    "        \n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8cae71-c811-4b5c-babb-3f97275b1d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (70, 250)  # The height and width of your input spectrograms\n",
    "latent_dim = 2500  # Or whatever latent dimension you used\n",
    "\n",
    "conv_output_shape, flattened_size = model.conv_output_shape, model.flattened_size\n",
    "\n",
    "# Instantiate the decoder with the necessary parameters\n",
    "decoder = Decoder(conv_output_shape=conv_output_shape, flattened_size=flattened_size, latent_dim=latent_dim).to(device).double()\n",
    "\n",
    "\n",
    "qplt_g=decoder(y_test_tensor[0].reshape(1,2500))\n",
    "qplt_g.shape\n",
    "\n",
    "# Test the decoder by passing a 1D latent tensor\n",
    "#latent_tensor = torch.randn(1, latent_dim)  # A random latent vector\n",
    "#output_spectrogram = decoder(latent_tensor)\n",
    "\n",
    "# Check the shape of the output (should be 2D spectrogram shape)\n",
    "#print(output_spectrogram.shape)  # Expected shape: (1, 1, 70, 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae3a76e-feb6-40da-af15-d59a1b80a9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "qplt_g.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c4230c-8ffd-4a39-8561-0f52588e0d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=37\n",
    "qplt=X_test_tensor[i].detach().cpu().numpy()\n",
    "\n",
    "output=encoder(X_test_tensor[i].reshape(1,70,250))\n",
    "\n",
    "tms_generated=output[0]\n",
    "\n",
    "\n",
    "qplt_g=qtransform(tms_generated).detach().cpu().numpy()[0]\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 7))\n",
    "im0 = axes[0].imshow(qplt, aspect=\"auto\", origin=\"lower\", vmin=0, vmax=0.001)\n",
    "axes[0].set_title(\"Real CQT\")\n",
    "fig.colorbar(im0, ax=axes[0])\n",
    "\n",
    "im1 = axes[1].imshow(qplt_g, aspect=\"auto\", origin=\"lower\", vmin=0, vmax=25)\n",
    "axes[1].set_title(\"Generated CQT\")\n",
    "fig.colorbar(im1, ax=axes[1])\n",
    "\n",
    "abs_diff=qplt-qplt_g\n",
    "\n",
    "im2 = axes[2].imshow(abs_diff, aspect=\"auto\", origin=\"lower\", vmin=0, vmax=25)\n",
    "axes[2].set_title(\"Abs Difference\")\n",
    "fig.colorbar(im2, ax=axes[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401f0a04-e0a6-4b5b-866d-c9ecf80ef8cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tms_generated=TimeSeries(output[0].detach().cpu().numpy(), dt=1/sampling_rate)\n",
    "\n",
    "for q in range(1,30):\n",
    "    print(q)\n",
    "    qplt_g=tms_generated.q_transform(qrange=[q,q], logf=True, whiten=False, frange=[5,40])\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(qplt_g, aspect='auto', vmin=0,vmax=25)\n",
    "    plt.title('Real')\n",
    "    plt.xlabel('Time [pixel]')\n",
    "    plt.ylabel('Frequency [pixel]')\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfe4bee-1184-483c-b96d-354235e78f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor[5].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cd8030-da54-4238-8fa1-287b168d19e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "qplt=X_train_tensor[0].detach().cpu().numpy()\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(qplt, aspect='auto', vmin=0,vmax=15)\n",
    "plt.title('Real')\n",
    "plt.xlabel('Time [pixel]')\n",
    "plt.ylabel('Frequency [pixel]')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fd7dc9-d774-4943-aaae-4185df62f58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(qplt.max())\n",
    "qplt_g.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b214bd-0f28-4336-9327-3d6c8023d3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#qplt_g=qtransform(y_train_tensor[12])\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(qplt_g.detach().cpu().numpy()[0][0], aspect='auto', vmin=0,vmax=0.58)\n",
    "plt.title('Generated')\n",
    "plt.xlabel('Time [pixel]')\n",
    "plt.ylabel('Frequency [pixel]')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c207716a-5fa6-4c72-915a-e4c0370e4696",
   "metadata": {},
   "outputs": [],
   "source": [
    "qplt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f540d1c-ec2a-4a05-87a5-7f605dd33d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tms=TimeSeries(y_train_tensor[12].detach().cpu().numpy(), dt=1/500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c724e65-76da-40b9-93a7-88e9c05fd876",
   "metadata": {},
   "outputs": [],
   "source": [
    "qplt_gwpy=tms.q_transform(qrange=[30,30])\n",
    "qplt_gwpy.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3462ad2-0776-4756-993b-4fc21573b4c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-intertwin",
   "language": "python",
   "name": "pytorch-intertwin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
