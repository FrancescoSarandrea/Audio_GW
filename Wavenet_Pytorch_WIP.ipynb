{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["EHIraMCNHk-a","FswdlesRkFeX","IZV8s3zE4SFZ"],"authorship_tag":"ABX9TyN4557tkv9mV5lby4OQpkJH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mqWIda7CK1Qy","executionInfo":{"status":"ok","timestamp":1698405674591,"user_tz":-120,"elapsed":1779,"user":{"displayName":"Francesco Sarandrea","userId":"01970008328388858960"}},"outputId":"2235b397-9145-4919-99d3-0cc6171b55e7"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n","Cloning into 'WaveNet'...\n","remote: Enumerating objects: 64, done.\u001b[K\n","remote: Counting objects: 100% (28/28), done.\u001b[K\n","remote: Compressing objects: 100% (14/14), done.\u001b[K\n","remote: Total 64 (delta 15), reused 14 (delta 14), pack-reused 36\u001b[K\n","Receiving objects: 100% (64/64), 4.28 MiB | 15.94 MiB/s, done.\n","Resolving deltas: 100% (24/24), done.\n","/content/WaveNet\n","Already up to date.\n"]}],"source":["%cd /content\n","!git clone https://github.com/golbin/WaveNet.git\n","%cd /content/WaveNet\n","!git pull"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B7ko9k1MR9RP","executionInfo":{"status":"ok","timestamp":1698405699164,"user_tz":-120,"elapsed":18956,"user":{"displayName":"Francesco Sarandrea","userId":"01970008328388858960"}},"outputId":"bbebc76c-06ec-4060-ead6-f6be50078762"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# Model"],"metadata":{"id":"EHIraMCNHk-a"}},{"cell_type":"markdown","source":["We changed 'loss.data[0]' with 'loss.item()' in the train function"],"metadata":{"id":"7TVelMwRQQUu"}},{"cell_type":"code","source":["import os\n","\n","import torch\n","import torch.optim\n","\n","from wavenet.networks import WaveNet as WaveNetModule\n","\n","\n","class WaveNet:\n","    def __init__(self, layer_size, stack_size, in_channels, res_channels, lr=0.002):\n","\n","        self.net = WaveNetModule(layer_size, stack_size, in_channels, res_channels)\n","\n","        self.in_channels = in_channels\n","        self.receptive_fields = self.net.receptive_fields\n","\n","        self.lr = lr\n","        self.loss = self._loss()\n","        self.optimizer = self._optimizer()\n","\n","        self._prepare_for_gpu()\n","\n","    @staticmethod\n","    def _loss():\n","        loss = torch.nn.CrossEntropyLoss()\n","\n","        if torch.cuda.is_available():\n","            loss = loss.cuda()\n","\n","        return loss\n","\n","    def _optimizer(self):\n","        return torch.optim.Adam(self.net.parameters(), lr=self.lr)\n","\n","    def _prepare_for_gpu(self):\n","        if torch.cuda.device_count() > 1:\n","            print(\"{0} GPUs are detected.\".format(torch.cuda.device_count()))\n","            self.net = torch.nn.DataParallel(self.net)\n","\n","        if torch.cuda.is_available():\n","            self.net.cuda()\n","\n","    def train(self, inputs, targets):\n","        \"\"\"\n","        Train 1 time\n","        :param inputs: Tensor[batch, timestep, channels]\n","        :param targets: Torch tensor [batch, timestep, channels]\n","        :return: float loss\n","        \"\"\"\n","        outputs = self.net(inputs)\n","\n","        loss = self.loss(outputs.view(-1, self.in_channels),\n","                         targets.long().view(-1))\n","\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        self.optimizer.step()\n","\n","        #return loss.data[0]\n","        return loss.item()\n","\n","\n","    def generate(self, inputs):\n","        \"\"\"\n","        Generate 1 time\n","        :param inputs: Tensor[batch, timestep, channels]\n","        :return: Tensor[batch, timestep, channels]\n","        \"\"\"\n","        outputs = self.net(inputs)\n","\n","        return outputs\n","\n","    @staticmethod\n","    def get_model_path(model_dir, step=0):\n","        basename = 'wavenet'\n","\n","        if step:\n","            return os.path.join(model_dir, '{0}_{1}.pkl'.format(basename, step))\n","        else:\n","            return os.path.join(model_dir, '{0}.pkl'.format(basename))\n","\n","    def load(self, model_dir, step=0):\n","        \"\"\"\n","        Load pre-trained model\n","        :param model_dir:\n","        :param step:\n","        :return:\n","        \"\"\"\n","        print(\"Loading model from {0}\".format(model_dir))\n","\n","        model_path = self.get_model_path(model_dir, step)\n","\n","        self.net.load_state_dict(torch.load(model_path))\n","\n","    def save(self, model_dir, step=0):\n","        print(\"Saving model into {0}\".format(model_dir))\n","\n","        model_path = self.get_model_path(model_dir, step)\n","\n","        torch.save(self.net.state_dict(), model_path)"],"metadata":{"id":"dPsNqj7VHjG8","executionInfo":{"status":"ok","timestamp":1698405716253,"user_tz":-120,"elapsed":4211,"user":{"displayName":"Francesco Sarandrea","userId":"01970008328388858960"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# Trainer"],"metadata":{"id":"Cy6TkCaX2hwO"}},{"cell_type":"markdown","source":["We set the parameters and we create the directories for saving the model and the output."],"metadata":{"id":"DHHqXzZvSYBR"}},{"cell_type":"code","source":["# these are the values for the parameters as set in the git repo, at the moment they suck up all the available RAM\n","#layer_size=5\n","#stack_size=2\n","#in_channels=2\n","#res_channels=512\n","#lr=0.0002\n","#data_dir='/content/WaveNet/test/data/'\n","#receptive_fields=62\n","#sample_size=2000\n","#sample_rate=8000\n","#num_steps=100000\n","#model_dir='/content/WaveNet/model/'\n","#output_dir='/content/WaveNet/output/'"],"metadata":{"id":"LwsVmR434qSN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# diminishing res_channels, sample_size, sample_rate and num_steps, makes the process go through, but the generated output is bad\n","layer_size=5\n","stack_size=2\n","in_channels=2\n","res_channels=2\n","lr=0.0002\n","data_dir='/content/WaveNet/test/data/'\n","#receptive_fields=5\n","receptive_fields=62\n","sample_size=2000\n","sample_rate=8000\n","num_steps=10000\n","model_dir='/content/WaveNet/model/'\n","output_dir='/content/WaveNet/output/'"],"metadata":{"id":"wxEH8doc29Co","executionInfo":{"status":"ok","timestamp":1698405722912,"user_tz":-120,"elapsed":521,"user":{"displayName":"Francesco Sarandrea","userId":"01970008328388858960"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","import wavenet.config as config\n","#from wavenet.model import WaveNet\n","from wavenet.utils.data import DataLoader\n","\n","\n","class Trainer:\n","    def __init__(self, layer_size, stack_size,in_channels, res_channels, lr, data_dir, receptive_fields, sample_size,sample_rate, num_steps, model_dir, output_dir):\n","\n","\n","        self.wavenet = WaveNet(layer_size, stack_size,\n","                               in_channels, res_channels,\n","                               lr=lr)\n","\n","        self.data_loader = DataLoader(data_dir, receptive_fields,\n","                                      sample_size, sample_rate, in_channels)\n","\n","    def infinite_batch(self):\n","        while True:\n","            for dataset in self.data_loader:\n","                for inputs, targets in dataset:\n","                    yield inputs, targets\n","\n","    def run(self):\n","        total_steps = 0\n","\n","        for inputs, targets in self.infinite_batch():\n","            loss = self.wavenet.train(inputs, targets)\n","            print('LOSS= '+str(loss))\n","\n","            total_steps += 1\n","\n","            print('[{0}/{1}] loss: {2}'.format(total_steps, num_steps, loss))\n","\n","            if total_steps > num_steps:\n","                break\n","\n","        self.wavenet.save(model_dir)\n","\n","\n","def prepare_output_dir(args):\n","    log_dir = os.path.join(output_dir, 'log')\n","    model_dir = os.path.join(output_dir, 'model')\n","    test_output_dir = os.path.join(output_dir, 'test')\n","\n","    os.makedirs(log_dir, exist_ok=True)\n","    os.makedirs(model_dir, exist_ok=True)\n","    os.makedirs(test_output_dir, exist_ok=True)"],"metadata":{"id":"h71-zF5iwR7g","executionInfo":{"status":"ok","timestamp":1698405726385,"user_tz":-120,"elapsed":6,"user":{"displayName":"Francesco Sarandrea","userId":"01970008328388858960"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["trainer=Trainer(layer_size, stack_size,in_channels, res_channels, lr, data_dir, receptive_fields, sample_size,sample_rate, num_steps, model_dir, output_dir)"],"metadata":{"id":"BEaewjHN2Hx6","executionInfo":{"status":"ok","timestamp":1698405729156,"user_tz":-120,"elapsed":959,"user":{"displayName":"Francesco Sarandrea","userId":"01970008328388858960"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["trainer.run()"],"metadata":{"id":"u8wj3GKq9Dkm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Generation"],"metadata":{"id":"FswdlesRkFeX"}},{"cell_type":"markdown","source":["We need to upload the audiocheck.net_whitenoisegaussian.wav file to have a seed, and to create the output directory for the generated audio."],"metadata":{"id":"uO0r74s2SLFg"}},{"cell_type":"code","source":["#seed_file='/audiocheck.net_whitenoisegaussian.wav'\n","seed_file='/content/WaveNet/test/data/helloworld.wav'\n","out='/content/WaveNet/output/generated.wav'"],"metadata":{"id":"oLEmxY2b5ko0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#seed_file='/content/drive/MyDrive/audiocheck.net_whitenoisegaussian.wav'\n"],"metadata":{"id":"LTiq-R5mSR6-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["step=0"],"metadata":{"id":"XeCdAB_Eo2Qz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import librosa\n","import datetime\n","import numpy as np\n","import soundfile as sf\n","\n","import wavenet.config as config\n","from wavenet.model import WaveNet\n","import wavenet.utils.data as utils\n","\n","\n","\n","class Generator:\n","    def __init__(self, layer_size,stack_size,in_channels,res_channels,\n","                 model_dir,step,sample_size,sample_rate, seed_file, out):\n","\n","        self.wavenet = WaveNet(layer_size, stack_size,\n","                               in_channels, res_channels)\n","\n","        self.wavenet.load(model_dir, step)\n","\n","    @staticmethod\n","    def _variable(data):\n","        tensor = torch.from_numpy(data).float()\n","\n","        if torch.cuda.is_available():\n","            return torch.autograd.Variable(tensor.cuda())\n","        else:\n","            return torch.autograd.Variable(tensor)\n","\n","    def _make_seed(self, audio):\n","        audio = np.pad([audio], [[0, 0], [self.wavenet.receptive_fields, 0], [0, 0]], 'constant')\n","\n","        if sample_size:\n","            seed = audio[:, :sample_size, :]\n","        else:\n","            seed = audio[:, :self.wavenet.receptive_fields*2, :]\n","\n","        return seed\n","\n","    def _get_seed_from_audio(self, filepath):\n","        audio = utils.load_audio(filepath, sample_rate)\n","        audio_length = len(audio)\n","\n","        audio = utils.mu_law_encode(audio, in_channels)\n","        audio = utils.one_hot_encode(audio, in_channels)\n","\n","        seed = self._make_seed(audio)\n","\n","        return self._variable(seed), audio_length\n","\n","    def _save_to_audio_file(self, data):\n","        data = data[0].cpu().data.numpy()\n","        data = utils.one_hot_decode(data, axis=1)\n","        audio = utils.mu_law_decode(data, in_channels)\n","        print('OUT=',out)\n","        sf.write(out, audio, sample_rate, 'PCM_24')\n","        #librosa.output.write_wav(out, audio, sample_rate)\n","        print('Saved wav file at {}'.format(out))\n","\n","        return librosa.get_duration(y=audio, sr=sample_rate)\n","\n","    def generate(self):\n","        outputs = []\n","        #inputs, audio_length = self._get_seed_from_audio(self.seed)\n","        inputs, audio_length = self._get_seed_from_audio(seed_file)\n","\n","        while True:\n","            new = self.wavenet.generate(inputs)\n","\n","            outputs = torch.cat((outputs, new), dim=1) if len(outputs) else new\n","\n","            print('{0}/{1} samples are generated.'.format(len(outputs[0]), audio_length))\n","\n","            if len(outputs[0]) >= audio_length:\n","                break\n","\n","            inputs = torch.cat((inputs[:, :-len(new[0]), :], new), dim=1)\n","\n","        outputs = outputs[:, :audio_length, :]\n","\n","        return self._save_to_audio_file(outputs)"],"metadata":{"id":"teyeeWAcTvUz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generator = Generator(layer_size,stack_size,in_channels,res_channels,model_dir,step,sample_size,sample_rate, seed_file,out)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ko8g52gwpgRg","executionInfo":{"status":"ok","timestamp":1698310693451,"user_tz":-120,"elapsed":14,"user":{"displayName":"Francesco Sarandrea","userId":"01970008328388858960"}},"outputId":"f982a2af-0e42-4ddb-fdf2-119a91b08615"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading model from /content/WaveNet/model/\n"]}]},{"cell_type":"code","source":["generator.generate()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EtiZurSRp7Ko","executionInfo":{"status":"ok","timestamp":1698310711368,"user_tz":-120,"elapsed":1238,"user":{"displayName":"Francesco Sarandrea","userId":"01970008328388858960"}},"outputId":"56b21afe-5ab0-41b7-a8c3-ce02244a246c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["138/10429 samples are generated.\n","276/10429 samples are generated.\n","414/10429 samples are generated.\n","552/10429 samples are generated.\n","690/10429 samples are generated.\n","828/10429 samples are generated.\n","966/10429 samples are generated.\n","1104/10429 samples are generated.\n","1242/10429 samples are generated.\n","1380/10429 samples are generated.\n","1518/10429 samples are generated.\n","1656/10429 samples are generated.\n","1794/10429 samples are generated.\n","1932/10429 samples are generated.\n","2070/10429 samples are generated.\n","2208/10429 samples are generated.\n","2346/10429 samples are generated.\n","2484/10429 samples are generated.\n","2622/10429 samples are generated.\n","2760/10429 samples are generated.\n","2898/10429 samples are generated.\n","3036/10429 samples are generated.\n","3174/10429 samples are generated.\n","3312/10429 samples are generated.\n","3450/10429 samples are generated.\n","3588/10429 samples are generated.\n","3726/10429 samples are generated.\n","3864/10429 samples are generated.\n","4002/10429 samples are generated.\n","4140/10429 samples are generated.\n","4278/10429 samples are generated.\n","4416/10429 samples are generated.\n","4554/10429 samples are generated.\n","4692/10429 samples are generated.\n","4830/10429 samples are generated.\n","4968/10429 samples are generated.\n","5106/10429 samples are generated.\n","5244/10429 samples are generated.\n","5382/10429 samples are generated.\n","5520/10429 samples are generated.\n","5658/10429 samples are generated.\n","5796/10429 samples are generated.\n","5934/10429 samples are generated.\n","6072/10429 samples are generated.\n","6210/10429 samples are generated.\n","6348/10429 samples are generated.\n","6486/10429 samples are generated.\n","6624/10429 samples are generated.\n","6762/10429 samples are generated.\n","6900/10429 samples are generated.\n","7038/10429 samples are generated.\n","7176/10429 samples are generated.\n","7314/10429 samples are generated.\n","7452/10429 samples are generated.\n","7590/10429 samples are generated.\n","7728/10429 samples are generated.\n","7866/10429 samples are generated.\n","8004/10429 samples are generated.\n","8142/10429 samples are generated.\n","8280/10429 samples are generated.\n","8418/10429 samples are generated.\n","8556/10429 samples are generated.\n","8694/10429 samples are generated.\n","8832/10429 samples are generated.\n","8970/10429 samples are generated.\n","9108/10429 samples are generated.\n","9246/10429 samples are generated.\n","9384/10429 samples are generated.\n","9522/10429 samples are generated.\n","9660/10429 samples are generated.\n","9798/10429 samples are generated.\n","9936/10429 samples are generated.\n","10074/10429 samples are generated.\n","10212/10429 samples are generated.\n","10350/10429 samples are generated.\n","10488/10429 samples are generated.\n","OUT= /content/WaveNet/output/generated.wav\n","Saved wav file at /content/WaveNet/output/generated.wav\n"]},{"output_type":"execute_result","data":{"text/plain":["104.29"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["np.zeros((5, 14), dtype=float)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MjNEkqa7xHDJ","executionInfo":{"status":"ok","timestamp":1698152526734,"user_tz":-120,"elapsed":309,"user":{"displayName":"Francesco Sarandrea","userId":"01970008328388858960"}},"outputId":"7fe33178-9247-4076-b1c6-a71df5bf4e00"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"]},"metadata":{},"execution_count":24}]},{"cell_type":"markdown","source":["# Bits And Pieces\n","Parts of code which may be useful when working to generate Timeseries"],"metadata":{"id":"IZV8s3zE4SFZ"}},{"cell_type":"code","source":["data_loader=DataLoader(TEST_AUDIO_DIR,RECEPTIVE_FIELDS, SAMPLE_SIZE, SAMPLE_RATE, IN_CHANNELS,\n","                             shuffle=False)"],"metadata":{"id":"h4N8hc5A4FjX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_size = []\n","\n","for dataset in data_loader:\n","  input_size = []\n","  target_size = []\n","\n","  for i, t in dataset:\n","    #input_size.append(i.shape)\n","    #target_size.append(t.shape)\n","    print(i.shape)\n","    print(t.shape)\n","\n","  #dataset_size.append([input_size, target_size])"],"metadata":{"id":"cm6pGt3L4Jat"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#inputs=torch.tensor([[[0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [0.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.],\n","#         [1.]]])\n","#targets=torch.tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","#         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","#         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","#         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","#         0., 0., 0., 0.]])"],"metadata":{"id":"URgXpFMv4QOx"},"execution_count":null,"outputs":[]}]}